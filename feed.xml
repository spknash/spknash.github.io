<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://spknash.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://spknash.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-01-28T18:05:12+00:00</updated><id>https://spknash.github.io/feed.xml</id><title type="html">Suhaas</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Weak to Strong Generalization</title><link href="https://spknash.github.io/blog/2024/weak-to-strong/" rel="alternate" type="text/html" title="Weak to Strong Generalization" /><published>2024-01-19T00:00:00+00:00</published><updated>2024-01-19T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2024/weak-to-strong</id><content type="html" xml:base="https://spknash.github.io/blog/2024/weak-to-strong/"><![CDATA[<h1 id="weak-to-strong-generalization-openai">Weak-to-Strong Generalization (OpenAI)</h1>

<p>OpenAI recently came out with a paper titled <em>Weak-to-Strong Generalization: Eliciting Strong Capabilities using Weak Supervision</em>. The paper tackles the broad problem of super human alignment – how will humans supervise superhuman artificial intelligence. Since superintelligent AI does not exist yet, this alignment problem can not be tackled directly. The authors choose to tackle an analogous problem – can a weak model supervise a strong model(both of whichare sub-human intelligence)? I found this paper to be very interesting. It shows that weak-to-strong generalisation is tractable based on the experiments they ran, however a significant percentage of strong capabilities are not elicitied using weak supervision.</p>

<p>In this post I’ll summarize methods used, results, and conclusions of the paper. Then I’ll look ahead to experiments that I am doing which were inspired by this paper.</p>

<h2 id="aligning-superintelligence">Aligning Superintelligence</h2>

<p>Aligning Superintelligence is one of the grand challenges of AI. Modern day sota models use human supervision for alignment. This is fine because humans are still stronger than the current models. However when superintelligent AI exists, they will be stronger than humans and humans can only weakly supervise these models. Superintelligent AI does not exist yet, so a close analogous situation is weak-to-strong generalization – a weak model supervising a strong model. The authors aim to show that weak-to-strong generalazation and take the first steps towards super alignment.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/weak_strong-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/weak_strong-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/weak_strong-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/weak_strong.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
credit: OpenAI
    
</div>

<h2 id="methodology">Methodology</h2>

<p>GPT-2 is used as the weak supervisor, and GPT-4 is used as the strong student in the experiments</p>

<p>The experiment the authors conducted is structured in the following way. A weak supervisor is finetuned on the ground truth, and this model makes predictions on test data(Call the score of the weak model WM). A strong student is trained with weak supervision, ie the strong model is finetuned on labels produced by the weak model. This model then makes predictions on the test data(Call the score of this weakly trained strong model WTS). Finally, as a benchmark, the strong model is trained on the ground truth and labels the test data(Call the score of this model SM). The effectiveness of the weak-to-strong generalization is measured by the Performance Gap Recovered(PGR) which is the percent of the performance gap between the weak model and strong model covered by the weak to strong model. So the PGR = (WTS-WM)/(SM-WM). In other words if the PGR is 0% the weak to strong generalization was completely innefective and the weakly supervised strong model performed the same as the weak model. If PGR is 100% then it was very effective and the weak to strong generalization is just as good as the strong model.</p>

<p>Some advantages of this setup include:</p>

<ul>
  <li>Any weak and strong models acn be used in this experiment, as long as there are task and measurement methods to assess performance on tasks.</li>
  <li>Similarly, any set of tasks can be used</li>
  <li>The authors say success in weak-to-strong generalization will help with alignment of current models.</li>
</ul>

<p>There are also some limits of this setup. Specifically, there are disanalogies to the problem of humans supervising super intelligent AI.</p>

<ul>
  <li>The first is Imitation Saliency: The errors weak models make may be differenct than the errors humans make.</li>
  <li>The second is Pretraining Leakage: The pretraining data in the strong models used in these experiments contains supervision from humans. So in the experiment, the weakly trained strong models may be artificially producing the correct answer in some cases because of their human supervised pretraining. Howerver, super-intelligence may not be created using human supervision, eg self-supervised training. This would make it harder to elicit from super human models.</li>
</ul>

<h2 id="results">Results</h2>

<p>The tasks used in the experiment are the following:</p>

<ul>
  <li>Popular NLP benchmarks: all datasets converted to binary classification, these tasks include sentiment analysis, natural language inference and more</li>
  <li>Chess Puzzles: data set of puzzles which is a position on the chess board and the label is the best next move. This is the only generative task used in this experiment</li>
  <li>GPT reward modeling: a binary dataset of pairs of gpt responses and the label is the better completion result</li>
</ul>

<p>First the authors did plain weak-to-strong generalization with no additional mathods used and tested to get a benchmark. On the NLP benchmarks test, the PGR was between 20% and 50% with the PGR increasing as the model size of the strong model increases. On the chess puzzles task, the PGR is close to 0% for small weak model size and it increases to 40% for the largest weak model size. The PGR does not increase with an increase of strong model size. On the GPT reward modeling task, the PGR is always quite low never crossing 25% PGR. On all these tasks the PGR was greater than 0% but still very far from the ceiling set by the strong model trained on the ground truth. The highest PGR in any task was around 50%. These results show that weak-to-strong generalization is tractable but many improvements need to be made before it is a viable alignment technique.</p>

<p>The authors then show results from experiments using weak-to-strong generalization but with different improvements. The first is bootstrapping.</p>

<p>Bootstrapping is a method that has been talked about for super alignment before. The idea is to train a model slightly stronger than the weak model, then use this model to train a slightly stronger model than itself, and so on until you train the strong model you are trying to align. In this experiment, the authors used two intermediary models(I1 and I2) between GPT2 and GPT4. So GPT-2 trained I1 which trained I2 which trained GPT-4. Bootstrapping showed a significant improvement in the chess puzzles task, the largest improvement being around a 25% increase in the PGR. The other tasks, however, did not show much improvement.</p>

<p>The second method which caused gains for weak-to-strong supervision was the auxillary confidence loss term. One of the main things causing errors in the weak-to-strong training is the strong model will sometimes imitate the mistakes of the weak supervisor. Giving the strong model the ability to disagree with the weak models label using its pretraining knowledge would help improve the weak-to-strong generalization. That is what adding the auxillary confidence loss term to standard cross entropy does. During training, if the strong model disagrees with the weak models labels, this label will not affec the model weights very much. This method dramatically improves PGR expecially in large gaps between the size of weak and strong models.</p>

<h2 id="understanding-weak-to-strong-generalization">Understanding Weak-to-Strong Generalization</h2>

<p>The authors identify two major phenomena related to weak-to-strong generalization: Imitation of weak supervisor mistakes, and saliency of tasks in the strong model.</p>

<h3 id="weak-supervisor-imitation">Weak Supervisor Imitation</h3>

<p>The strong model repeating the weak models mistakes reduces the effectiveness of weak to strong generalization. One cause of mistake imitation is the strong model overfitting with the weak model. This can be reduced by using techniques commonly used to fix over-fitting like regularization or early stopping of training. Another reason for imitation is high supervisor-student agreement. From the experiment they saw that introducing confidence loss significantly decreased model agreement and the PGR increased as well.</p>

<h3 id="saliency-of-strong-model-representations">Saliency of Strong model representations</h3>

<p>Some of the PGR acheived by the strong model could be due to the inherent saliency of the tasks in the strong model. For example, if GPT-4 was trained using data similar to the popular NLP tasks it may be able to acheive better results than the weak model without any finetuning. The authors tested this by seeing how accurate the strong models were with zero-shot or few shot prompting. For smaller strong models the PGR acheived was minimal but for larger students the PGR was significantly higher and comparable to results of the weakly supervised model. However, weakly supervised models with confidence loss generally outperforms zero-shot and few-shot prompting even for large models.</p>

<p>If salient representations of the task are ineherent in the pretrained strong model, then weak-to-strong generalization could be improved by using unsupervised finetuning in order to bring out the salient abilities of the strong model. Unsupervised finetuning improves the PGR of the GPT reward modeling task from  less than 20% to 30-40%.</p>

<p>Finally, the authors discuss the possibility of using linear probing to improve weak-to-strong. They found that weak supervision increases the linearity of the labels so finetuning using weak labels and then linear probing increases the PGR of the weak-to-strong generalization.</p>

<h2 id="discussion">Discussion</h2>

<p>The first potential avenue for future work discussed is changes to the analogous setup. As mentioned before, there are two main disanalogies with this setup:</p>

<ul>
  <li>Imitation Saliency</li>
  <li>Pretraining Leakage</li>
</ul>

<p>The authors mention the following ways to improve the setup:</p>
<ul>
  <li>fixing disanalogies</li>
  <li>showing that the disanalogies are not severe</li>
  <li>generalizing tasks to more complicated tasks</li>
</ul>

<p>The next avenue future work is scalable methods for weak-to-strong generalization. In a desired weak-to-strong generalization:</p>

<ul>
  <li>the strong model should disagree with weak models errors</li>
  <li>the generalization should be salient to the strong model</li>
  <li>The gneralization should be consistent.</li>
</ul>

<p>The authors also believe there is a lot of potential for gains to be made in weak-to-strong generalization by using more ML techniques like confidence loss which dramatically improved PGR.</p>

<p>Finally, the authors say that super human alignment is a critical problem, and if we are to use super human AI for important things there should be a clear scientific understanding of when and why the alignment works. They mention the following questions among others:</p>

<ul>
  <li>Why is there a significant difference between PGR obtained for different tasks?</li>
  <li>What makes a task or concept easy or hard to elicit?</li>
  <li>How much to errors in weak labels affect the strong models behavior?</li>
</ul>

<h2 id="what-i-am-working-on">What I am working on</h2>

<p>The authors mention that the saliency of the strong model representation for the tasks used is not clearly known and could have effected the results. Specifically, there may be overlap between the GPT-4 pretraining data and the popular NLP tasks data which was used in the experiment. I plan to recreate the experiment but instead of using GPT-2 and GPT-4 as the weak and strong models I will use 2 opensource models llama v1 and llama v2. And I will test on data that is disjoint from the pretraining data of these models. The idea behind this is to choose a task which the strong model does not have pretraining data on, to show I will share all details and code in my next post.</p>]]></content><author><name></name></author><category term="AI," /><category term="Alignment" /><summary type="html"><![CDATA[Review of new paper on super aligment and weak to strong generalization]]></summary></entry><entry><title type="html">Echo Server cpp</title><link href="https://spknash.github.io/blog/2023/echo-server-cpp/" rel="alternate" type="text/html" title="Echo Server cpp" /><published>2023-09-09T00:00:00+00:00</published><updated>2023-09-09T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2023/echo-server-cpp</id><content type="html" xml:base="https://spknash.github.io/blog/2023/echo-server-cpp/"><![CDATA[<h1 id="intro-socket-programming-in-c">Intro Socket Programming in C</h1>

<p>Hello! It has been a while since my last post. I have been busy with job applications and switching jobs the last few weeks and haven’t set aside enough time for this. From now on I’ll make sure to post every week I’ll say Monday even if what I am working on is not in any kind of finished state I’ll just put my progress or talk about something completely different. Like a filler episode in anime.</p>

<p>This week I am getting started with C++ and neworking/socket programming. I want to learn C++ because I said on my resume and some job applications that I know it and I want to also improve my networking/socket programming skills. The end goal is to create a something more interesting like a packet sniffer (like libpcap) or a blockchain network but since I am just getting started this week I am getting used to socket programming in C++ and implementing an echo server and client</p>

<h2 id="echo-server">Echo Server</h2>

<p>One guide that was very helpful with getting familiar with socket programming in C was <a href="https://beej.us/guide/bgnet/html/split/slightly-advanced-techniques.html#blocking">Beej’s guide</a>. I knew the overall structure for a echo server. I would need:</p>
<ol>
  <li>Accept connection from client socket</li>
  <li>Have multi-threading capability to allow the server to handle multiple clients simultaneously</li>
  <li>Ability to read message from client and send message back</li>
  <li>Client side should have some kind of command line program to allow client to type message and receive echo</li>
</ol>

<p>Some parts of this are in the server side and some parts are on the client side. I will start by going throught the server side.</p>

<h2 id="server">Server</h2>
<p>First some variables I initialized at the beginning of the program that I will use throughout server.cpp:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">status</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">sockfd</span><span class="p">,</span> <span class="n">new_fd</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">port_num</span><span class="p">;</span>
<span class="k">struct</span> <span class="n">sigaction</span> <span class="n">sa</span><span class="p">;</span>
<span class="n">socklen_t</span> <span class="n">addr_size</span><span class="p">;</span>
<span class="k">struct</span> <span class="n">sockaddr_storage</span> <span class="n">their_addr</span><span class="p">;</span>
<span class="k">struct</span> <span class="n">addrinfo</span> <span class="n">hints</span><span class="p">;</span>
<span class="k">struct</span> <span class="n">addrinfo</span> <span class="o">*</span><span class="n">res</span><span class="p">,</span> <span class="o">*</span><span class="n">p</span><span class="p">;</span>
<span class="n">socklen_t</span> <span class="n">sin_size</span><span class="p">;</span>
<span class="kt">char</span> <span class="n">s</span><span class="p">[</span><span class="n">INET6_ADDRSTRLEN</span><span class="p">];</span>
<span class="kt">char</span> <span class="n">ipstr</span><span class="p">[</span><span class="n">INET6_ADDRSTRLEN</span><span class="p">];</span>
<span class="n">memset</span><span class="p">(</span><span class="o">&amp;</span><span class="n">hints</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="k">sizeof</span> <span class="n">hints</span><span class="p">);</span>
<span class="n">hints</span><span class="p">.</span><span class="n">ai_family</span> <span class="o">=</span> <span class="n">AF_UNSPEC</span><span class="p">;</span> <span class="c1">// AF_INET or AF_INET6 to force version</span>
<span class="n">hints</span><span class="p">.</span><span class="n">ai_socktype</span> <span class="o">=</span> <span class="n">SOCK_STREAM</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">yes</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</code></pre></div></div>

<p>The first step is to get the address info of the server and that is done with the function ‘getaddrinfo’:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">((</span><span class="n">status</span> <span class="o">=</span> <span class="n">getaddrinfo</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span> <span class="s">"3490"</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">hints</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">res</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">){</span>
        <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span> <span class="s">"getaddrinfo: %s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">gai_strerror</span><span class="p">(</span><span class="n">status</span><span class="p">));</span>
        <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
    <span class="p">};</span>
</code></pre></div></div>

<p>This populates <code class="language-plaintext highlighter-rouge">res</code> with a linked list of potential addresses that a client can bind to in the server. The next step is to iterate through the possible addresses and initialize a socket file descriptor using one of the valid addresses.</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span> <span class="n">p</span><span class="o">!=</span> <span class="nb">NULL</span><span class="p">;</span> <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_next</span><span class="p">){</span>
        <span class="k">if</span><span class="p">((</span><span class="n">sockfd</span> <span class="o">=</span> <span class="n">socket</span><span class="p">(</span><span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_family</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_socktype</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_protocol</span><span class="p">))</span><span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">){</span>
            <span class="n">perror</span><span class="p">(</span><span class="s">"server: socket"</span><span class="p">);</span>
            <span class="k">continue</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">setsockopt</span><span class="p">(</span><span class="n">sockfd</span><span class="p">,</span> <span class="n">SOL_SOCKET</span><span class="p">,</span> <span class="n">SO_REUSEADDR</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">yes</span><span class="p">,</span>
                <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">))</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">perror</span><span class="p">(</span><span class="s">"setsockopt"</span><span class="p">);</span>
            <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">bind</span><span class="p">(</span><span class="n">sockfd</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_addr</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_addrlen</span><span class="p">)</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">close</span><span class="p">(</span><span class="n">sockfd</span><span class="p">);</span>
            <span class="n">perror</span><span class="p">(</span><span class="s">"server: bind"</span><span class="p">);</span>
            <span class="k">continue</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="k">break</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div>

<p>The socket fild descriptor(<code class="language-plaintext highlighter-rouge">sockfd</code>) will be set to the first address that is valid and if there are no valid addresses the program will exit:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">freeaddrinfo</span><span class="p">(</span><span class="n">res</span><span class="p">);</span>
    <span class="k">if</span><span class="p">(</span><span class="n">p</span><span class="o">==</span><span class="nb">NULL</span><span class="p">){</span>
        <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span> <span class="s">"Failed to bind</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
</code></pre></div></div>

<p>The next step is for the socket to listen to incoming clients trying to connec to the server which is done through the <code class="language-plaintext highlighter-rouge">listen()</code> function:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span><span class="p">(</span><span class="n">listen</span><span class="p">(</span><span class="n">sockfd</span><span class="p">,</span> <span class="n">BACKLOG</span><span class="p">)</span><span class="o">==-</span><span class="mi">1</span><span class="p">){</span>
        <span class="n">perror</span><span class="p">(</span><span class="s">"listen"</span><span class="p">);</span>
        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">};</span>
    <span class="c1">// waiting for connection now</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"server waiting for connections ... </span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
</code></pre></div></div>

<p>This code concludes the setup of the server; it now is able to listen to incoming clients. Now the server must accept clients and send echos to each client simultaneously as long as the server is active. Since the following functionality happens for the duration of the lifetime of the server, the remaining code is within a while loop: <code class="language-plaintext highlighter-rouge">while(1) </code></p>

<p>The first step is to accept clients that connec to the server:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sin_size</span> <span class="o">=</span> <span class="k">sizeof</span> <span class="n">their_addr</span><span class="p">;</span>
<span class="n">new_fd</span> <span class="o">=</span> <span class="n">accept</span><span class="p">(</span><span class="n">sockfd</span><span class="p">,</span> <span class="p">(</span><span class="k">struct</span> <span class="n">sockaddr</span> <span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">their_addr</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">sin_size</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">new_fd</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">perror</span><span class="p">(</span><span class="s">"accept"</span><span class="p">);</span>
    <span class="k">continue</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The return value of the <code class="language-plaintext highlighter-rouge">accept</code> function is the filedescriptor of the client socket and can be used as a parameter in <code class="language-plaintext highlighter-rouge">send</code> and <code class="language-plaintext highlighter-rouge">recv</code> to send and receive data from the client socket. After the client is connected to the server, the server must spawn a child process to handle this client simultaneously with the parent process as well as handling requests from other clients. This is where we must use <code class="language-plaintext highlighter-rouge">fork()</code> to do this. I also send a message to the client explaining what this particular server does:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">!</span><span class="n">fork</span><span class="p">()</span> <span class="c1">// this is the child process</span>
<span class="n">close</span><span class="p">(</span><span class="n">sockfd</span><span class="p">);</span> <span class="c1">// child doesn't need the listener</span>
<span class="k">if</span> <span class="p">(</span><span class="n">send</span><span class="p">(</span><span class="n">new_fd</span><span class="p">,</span> <span class="s">"Hello! welcome to echo server. Type something and I'll type the exact same thing back. How exciting!!"</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">perror</span><span class="p">(</span><span class="s">"send"</span><span class="p">);</span>
</code></pre></div></div>

<p>Now that the client is connected and running in a child process, the server must do what it is intended to do: echo messages from the client. So whenever the server socket receives data from this client it will send a message with the same data back to the client.</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">while</span> <span class="p">(</span><span class="mi">1</span><span class="p">){</span>
    <span class="c1">//printf("trying to recv from client");</span>
    <span class="kt">int</span> <span class="n">numbytes</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="kt">char</span> <span class="o">*</span><span class="n">buf</span><span class="p">[</span><span class="mi">256</span><span class="p">];</span>
    <span class="n">numbytes</span> <span class="o">=</span> <span class="n">recv</span><span class="p">(</span><span class="n">new_fd</span><span class="p">,</span> <span class="n">buf</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">numbytes</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">){</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"sever: received '%s'</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">buf</span><span class="p">);</span>
    <span class="p">}</span>
    
    <span class="k">if</span> <span class="p">(</span><span class="n">numbytes</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">){</span>
        <span class="kt">int</span> <span class="n">bytes_sent</span><span class="p">;</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">bytes_sent</span> <span class="o">=</span> <span class="n">send</span><span class="p">(</span><span class="n">new_fd</span><span class="p">,</span> <span class="n">buf</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">perror</span><span class="p">(</span><span class="s">"send"</span><span class="p">);</span>
            <span class="n">printf</span><span class="p">(</span><span class="s">"exiting"</span><span class="p">);</span>
            <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This concludes the logic needed on the server side. In summary, we get the address info of the server, initialize a socket file descriptor using the address, bind the socket to a specific port, listen to incoming connection requests form clients, accept connection from client, start a child process for client and send echo messages. Now we can move on to the client side logic which is very similar.</p>
<h2 id="client">Client</h2>

<p>The client side will be similar to the server side but instead of listening for requests, now we are making a connection request and we need to have functionality allowing the user to connect to a particular server and make echo requests to the server. The plan is to have the client executable run like this: <code class="language-plaintext highlighter-rouge">./client ***server name***</code> and once connected to the server the user should be able to type a message and receive the echo.</p>

<p>First some variable intialized at the beginning of the script that will be used throughout client.cpp:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">status</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">sockfd</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">numbytes</span><span class="p">;</span>

<span class="kt">char</span> <span class="o">*</span><span class="n">buf</span><span class="p">[</span><span class="mi">100</span><span class="p">];</span>

<span class="k">struct</span> <span class="n">addrinfo</span> <span class="n">hints</span><span class="p">;</span>
<span class="k">struct</span> <span class="n">addrinfo</span> <span class="o">*</span><span class="n">res</span><span class="p">,</span> <span class="o">*</span><span class="n">p</span><span class="p">;</span>
<span class="n">socklen_t</span> <span class="n">sin_size</span><span class="p">;</span>

<span class="kt">char</span> <span class="n">s</span><span class="p">[</span><span class="n">INET6_ADDRSTRLEN</span><span class="p">];</span>
</code></pre></div></div>

<p>Make sure that the command line arguments are coming in correctly:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"usage: client hostname</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Now we get the address info of the server using the command line argument:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// getting addrinfo and binding to specific port</span>
<span class="k">if</span> <span class="p">((</span><span class="n">status</span> <span class="o">=</span> <span class="n">getaddrinfo</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s">"3490"</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">hints</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">res</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">){</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span> <span class="s">"getaddrinfo: %s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">gai_strerror</span><span class="p">(</span><span class="n">status</span><span class="p">));</span>
    <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">};</span>
</code></pre></div></div>

<p>The result is stored a linked list in res just as before. Just as before we iterate through the possible addresses and create a sock file descriptor using the first valid one and make a connection request:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span> <span class="n">p</span><span class="o">!=</span> <span class="nb">NULL</span><span class="p">;</span> <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_next</span><span class="p">){</span>
    <span class="k">if</span><span class="p">((</span><span class="n">sockfd</span> <span class="o">=</span> <span class="n">socket</span><span class="p">(</span><span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_family</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_socktype</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_protocol</span><span class="p">))</span><span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">){</span>
        <span class="n">perror</span><span class="p">(</span><span class="s">"client: socket"</span><span class="p">);</span>
        <span class="k">continue</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">connect</span><span class="p">(</span><span class="n">sockfd</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_addr</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_addrlen</span><span class="p">)</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">close</span><span class="p">(</span><span class="n">sockfd</span><span class="p">);</span>
        <span class="n">perror</span><span class="p">(</span><span class="s">"client: connect"</span><span class="p">);</span>
        <span class="k">continue</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">break</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>After the client is connected to the socket there will be a loop of the client receiving the message from the server then getting a prompt to make another echo request. If another echo request is made then another interation fo the loop happens. We will have a way for the client to break out of the loop and end the program if they type <code class="language-plaintext highlighter-rouge">exit</code> when they are prompted with another echo request.</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">while</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>

    <span class="k">if</span> <span class="p">((</span><span class="n">numbytes</span> <span class="o">=</span> <span class="n">recv</span><span class="p">(</span><span class="n">sockfd</span><span class="p">,</span> <span class="n">buf</span><span class="p">,</span> <span class="n">MAXDATASIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">perror</span><span class="p">(</span><span class="s">"recv"</span><span class="p">);</span>
        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">buf</span><span class="p">[</span><span class="n">numbytes</span><span class="p">]</span> <span class="o">=</span> <span class="sc">'\0'</span><span class="p">;</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">"client: received '%s' from server</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">buf</span><span class="p">);</span>
    
    <span class="kt">char</span> <span class="n">buffer</span><span class="p">[</span><span class="mi">256</span><span class="p">];</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"Enter a message: "</span><span class="p">);</span>
    
    <span class="c1">// Read a line of input from the user</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">fgets</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">buffer</span><span class="p">),</span> <span class="n">stdin</span><span class="p">)</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"Error reading input.</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
        <span class="k">continue</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Remove the trailing newline character, if any</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">buffer</span><span class="p">[</span><span class="n">strlen</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="sc">'\n'</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">buffer</span><span class="p">[</span><span class="n">strlen</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="sc">'\0'</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Exit the loop if the user types "exit"</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">strcmp</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="s">"exit"</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">break</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="c1">//printf("%s\n", buffer);</span>

    <span class="c1">// send the message</span>
    <span class="kt">int</span> <span class="n">bytes_sent</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">bytes_sent</span> <span class="o">=</span> <span class="n">send</span><span class="p">(</span><span class="n">sockfd</span><span class="p">,</span> <span class="n">buffer</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">perror</span><span class="p">(</span><span class="s">"send"</span><span class="p">);</span>
        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This concludes the client side code.</p>

<p>Here is a quick demo of how it works</p>

<p>After <code class="language-plaintext highlighter-rouge">./server</code> is run we get this:</p>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/echo1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/echo1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/echo1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/echo1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>After the client connects to the server we see this on the server side:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/echo2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/echo2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/echo2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/echo2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>This is what the client sees when they connect to the client and enters a message to the server:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/echo3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/echo3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/echo3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/echo3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>And this is what is seen on the server side after the client sends a message to the server:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/echo4-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/echo4-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/echo4-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/echo4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>To conclude – this was just a very short introduction into socket programming in C/C++ and I plan to go deeper into network programming. I feel I got a good understanding for setting up a server client connection and plan to do a more complicated network program next time like a network packet sniffer, or simple blockchain network. I’ll be back here next week!</p>]]></content><author><name></name></author><category term="networking," /><category term="C++" /><summary type="html"><![CDATA[Intro to socket programming in C++]]></summary></entry><entry><title type="html">Webex llama bot</title><link href="https://spknash.github.io/blog/2023/Webex-llama-bot/" rel="alternate" type="text/html" title="Webex llama bot" /><published>2023-08-11T00:00:00+00:00</published><updated>2023-08-11T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2023/Webex-llama-bot</id><content type="html" xml:base="https://spknash.github.io/blog/2023/Webex-llama-bot/"><![CDATA[<h1 id="webex-chatbot-to-increase-office-interaction">Webex chatbot to increase office interaction</h1>

<p>This project started off as the project my team did for the intern hackathon at my summer internship. The topic of the hackathon was build something that improves remote and hybrid work. Our team decided that one of the biggest things lost during remote work is in person interaction and social interaction. It is very hard to become friends with someone when you don’t see them in person. Even harder to become friends when the only topic of conversation between you and them is a specific work task within the context of a work meeting. In person there are a lot more casual interactions in the hallway and especially during lunch that allow co-workers to become much closer to each other.</p>

<p>Our idea was to create a application which pairs two random employees in the company for a 30 minute chat each week. I won’t talk about the implementation of the full application but only about what I did. I worked on all parts related to Webex. We needed to use the Webex API to automatically schedule meetings between two people in the org. We also decided to incorporate a webex chatbot which could do things like provide background information about each participant in the meeting and also do things like offer a potential fun meeting topic if the participants don’t know what to talk about. As I was making the chatbot I realized its not a very good chatbot if it only responds to certain commands. Like below:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bot-help-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bot-help-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bot-help-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/bot-help.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>I wanted to use a Large language model to allow the bot to respond to messages more dynamically. The only issue is I don’t want to spend any money on this because this a purely for fun project and many llms require payment to either use their apis or require the use of cloud credits to host on a cloud platform. I arrived at an idea based on something I used to build the webex chatbot.</p>

<h2 id="webex-chatbot">Webex chatbot</h2>

<p>The webex chatbot was made using this template: <a href="https://github.com/WebexSamples/webex-bot-starter">github link</a>. I cloned this repo to my local and began to make changes. In order for the bot to work it needs to be able to communicate over http, so I used ngrok to expose the bot application to the internet.
###What is ngrok and how does it work?
When developing a web application, you typically run a local server that is only accessible on your personal computer. However, there might be instances where you need to share this local server with someone else on the internet, perhaps for testing, demonstrations, or external integrations like webhooks.</p>

<p>This is where ngrok comes in. By running a simple command in your command line interface, ngrok provides you with a public URL (HTTP/HTTPS) that forwards incoming requests to your local server. This URL can be shared with anyone, and they’ll be able to access your locally running application as if it were hosted online.</p>

<h3 id="general-use-cases">General Use Cases:</h3>
<p>Testing on Different Devices: If you want to test how your application appears on different devices, ngrok makes it easy by allowing those devices to access your local server through the public URL.</p>

<p>Collaborating with Team Members: If you’re working with team members who are not on the same local network, you can use ngrok to give them access to your development environment.</p>

<p>Webhook Development and Testing: Many third-party services use webhooks to communicate with your application. Ngrok allows these services to connect with your local server, simplifying the process of developing and testing webhooks.</p>

<p>Sharing a Demo with Clients: If you want to share a live demo of an application that’s still in development, ngrok enables you to do so without having to deploy it to a public server.</p>

<p>Temporary Hosting for Hackathons or Prototyping: Quick prototyping or participation in hackathons often requires temporary public access to your local development. Ngrok provides a swift solution for these scenarios.</p>

<p>In conclusion, ngrok is an essential tool for modern web development, offering a convenient way to share your local development environment with others. Its ease of use and wide range of applications make it an indispensable resource for developers seeking to streamline their workflow and collaboration efforts.</p>

<h2 id="trying-to-use-llamacpp">Trying to use llama.cpp</h2>
<p>So my webex bot at this point works well with limited structured commands but I wanted the catch-all case to give a best response using a large language model instead of not being able to handle a novel response. The first LLM I thought of using was llama because it seems to be the best performing open source langauge model at the moment but also because of llama.cpp which is a implementation of llama in C which is compact enough thata it can be run on Mac M1 or M2 chips. I forked llama.cpp and tried using it on local but the speed at which the response was very slow and it is also probably not great to run something so intensive on my Mac even if it was just for a hackathon/demo.</p>

<p>The next idea was to use llama.cpp on colab – and use ngrok to expose llama to the internet so it can be used essentially as an inference api for any application I want to build.</p>

<p>##Using llama.cpp and ngrok on Colab</p>

<p>Now I can use llama.cpp to create an app on colab and expose it to the internet to my webex bot can use it as a inference API to generate responses. To use llama.cpp on colab I need to use llama-cpp-python which is python bindings for llama.cpp. I also use langchains to do some minor prompt templating.</p>

<p>The imports:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.llms</span> <span class="kn">import</span> <span class="n">LlamaCpp</span>
<span class="kn">from</span> <span class="n">langchain</span> <span class="kn">import</span> <span class="n">PromptTemplate</span><span class="p">,</span> <span class="n">LLMChain</span>
<span class="kn">from</span> <span class="n">langchain.callbacks.manager</span> <span class="kn">import</span> <span class="n">CallbackManager</span>
<span class="kn">from</span> <span class="n">langchain.callbacks.streaming_stdout</span> <span class="kn">import</span> <span class="n">StreamingStdOutCallbackHandler</span>
</code></pre></div></div>

<p>The simple prompt template using langchains:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">{question}

Answer: Let</span><span class="sh">'</span><span class="s">s work this out in a step by step way to be sure we have the right answer.</span><span class="sh">"""</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">template</span><span class="p">,</span> <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div>

<p>This command declares a callback manager which allows the llm to produce tokens one by one chat-gpt style.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">callback_manager</span> <span class="o">=</span> <span class="nc">CallbackManager</span><span class="p">([</span><span class="nc">StreamingStdOutCallbackHandler</span><span class="p">()])</span>
</code></pre></div></div>

<p>This command gets the ggml file needed to use llama.cpp</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="p">.</span><span class="n">co</span><span class="o">/</span><span class="n">TheBloke</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">13</span><span class="n">B</span><span class="o">-</span><span class="n">chat</span><span class="o">-</span><span class="n">GGML</span><span class="o">/</span><span class="n">resolve</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">13</span><span class="n">b</span><span class="o">-</span><span class="n">chat</span><span class="p">.</span><span class="n">ggmlv3</span><span class="p">.</span><span class="n">q4_0</span><span class="p">.</span><span class="nb">bin</span>
</code></pre></div></div>

<p>This next block of code sets the model parameters and creates the inference function: llm</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_gpu_layers</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Change this value based on your model and your GPU VRAM pool.
</span><span class="n">n_batch</span> <span class="o">=</span> <span class="mi">512</span>  <span class="c1"># Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
</span>
<span class="c1"># Make sure the model path is correct for your system!
</span><span class="n">llm</span> <span class="o">=</span> <span class="nc">LlamaCpp</span><span class="p">(</span>
    <span class="n">model_path</span><span class="o">=</span><span class="sh">"</span><span class="s">llama-2-13b-chat.ggmlv3.q4_0.bin</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">n_gpu_layers</span><span class="o">=</span><span class="n">n_gpu_layers</span><span class="p">,</span>
    <span class="n">n_batch</span><span class="o">=</span><span class="n">n_batch</span><span class="p">,</span>
    <span class="n">callback_manager</span><span class="o">=</span><span class="n">callback_manager</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Now that we have the inference function we can create a app and expose to the internet using ngrok. First I test llm to see how fast it is.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prompt</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
Are there hills in Peru?
</span><span class="sh">"""</span>
<span class="nf">llm</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Yes, Peru is home to a variety of landscapes including hills, mountains,
 plains, and coastal areas. The Andes Mountains run through Peru, giving rise
 to many high peaks and rolling hills. These geographical features create
 diverse ecosystems throughout the country, from the high-altitude grasslands
 of the Andean Plateau to the arid coastal plains.
</code></pre></div></div>

<p>It runs much faster than on local and it is a necesary improvement because it was way too slow on local. Now onto the app.  Initially I had trouble using ngrok, this was because I hadn’t authenticated my ngrok token and to do that I needed to run these commands:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="nb">bin</span><span class="p">.</span><span class="n">equinox</span><span class="p">.</span><span class="n">io</span><span class="o">/</span><span class="n">c</span><span class="o">/</span><span class="mi">4</span><span class="n">VmDzA7iaHb</span><span class="o">/</span><span class="n">ngrok</span><span class="o">-</span><span class="n">stable</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">amd64</span><span class="p">.</span><span class="nb">zip</span>
<span class="err">!</span><span class="n">unzip</span> <span class="n">ngrok</span><span class="o">-</span><span class="n">stable</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">amd64</span><span class="p">.</span><span class="nb">zip</span>
<span class="err">!</span><span class="p">.</span><span class="o">/</span><span class="n">ngrok</span> <span class="n">authtoken</span> <span class="o">*</span><span class="n">my</span><span class="o">-</span><span class="n">token</span><span class="o">*</span>
</code></pre></div></div>

<p>I made a flask app and which on a POST request to /generate, retrieves the prompt and generates the response, and packages the response in json and sends it back.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">flask</span> <span class="kn">import</span> <span class="n">Flask</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">jsonify</span>
<span class="kn">from</span> <span class="n">flask_ngrok</span> <span class="kn">import</span> <span class="n">run_with_ngrok</span>
<span class="kn">from</span> <span class="n">flask_cors</span> <span class="kn">import</span> <span class="n">CORS</span>

<span class="n">app</span> <span class="o">=</span> <span class="nc">Flask</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>
<span class="nc">CORS</span><span class="p">(</span><span class="n">app</span><span class="p">)</span>  <span class="c1"># Enable CORS for all routes
</span><span class="nf">run_with_ngrok</span><span class="p">(</span><span class="n">app</span><span class="p">)</span>  <span class="c1"># Start ngrok when app is run
</span>
<span class="c1"># Assuming llm is already defined and loaded elsewhere in your code
# and you can get a response by calling llm(prompt)
</span><span class="nd">@app.route</span><span class="p">(</span><span class="sh">'</span><span class="s">/</span><span class="sh">'</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">home</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">home page</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="sh">'</span><span class="s">Hello, World!</span><span class="sh">'</span>
<span class="nd">@app.route</span><span class="p">(</span><span class="sh">'</span><span class="s">/generate</span><span class="sh">'</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">POST</span><span class="sh">'</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">generate</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">generating response ...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="n">json</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">prompt</span><span class="sh">'</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Received prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># Print the received prompt
</span>    <span class="n">response</span> <span class="o">=</span> <span class="nf">llm</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Generated response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># Print the generated response
</span>
    <span class="k">return</span> <span class="nf">jsonify</span><span class="p">({</span><span class="sh">'</span><span class="s">response</span><span class="sh">'</span><span class="p">:</span> <span class="n">response</span><span class="p">})</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">app</span><span class="p">.</span><span class="nf">run</span><span class="p">()</span>
</code></pre></div></div>

<p>Lets say this app runs on abc.ngrok-free.app. Now since this app is exposed to the internet I can make a POST request to this app in the app hosting the Webex bot.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">import</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">sys</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Retrieve the prompt from command-line arguments
</span><span class="n">url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">abc.ngrok-free.app/generate</span><span class="sh">"</span>
<span class="n">payload</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">Content-Type</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">application/json</span><span class="sh">'</span><span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">payload</span><span class="p">),</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">)</span>

<span class="k">if</span> <span class="n">response</span><span class="p">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Failed to make request. Status code: </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">status_code</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>This script communicates with the main javascript file which produces the bot responses</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">const</span> <span class="p">{</span> <span class="k">exec</span> <span class="p">}</span> <span class="o">=</span> <span class="nf">require</span><span class="p">(</span><span class="sh">'</span><span class="s">child_process</span><span class="sh">'</span><span class="p">);</span>

    <span class="n">const</span> <span class="n">prompt</span> <span class="o">=</span> <span class="n">trigger</span><span class="p">.</span><span class="n">text</span><span class="p">;</span>
    <span class="n">const</span> <span class="n">scriptPath</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./make_post.py</span><span class="sh">'</span><span class="p">;</span> <span class="o">//</span> <span class="n">Make</span> <span class="n">sure</span> <span class="n">to</span> <span class="n">provide</span> <span class="n">the</span> <span class="n">correct</span> <span class="n">path</span> <span class="n">to</span> <span class="n">the</span> <span class="n">script</span>

    <span class="nf">exec</span><span class="p">(</span><span class="sb">`python ${scriptPath} "${prompt}"`</span><span class="p">,</span> <span class="p">(</span><span class="n">error</span><span class="p">,</span> <span class="n">stdout</span><span class="p">,</span> <span class="n">stderr</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
      <span class="nf">if </span><span class="p">(</span><span class="n">error</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">console</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span><span class="sb">`An error occurred: ${error}`</span><span class="p">);</span>
        <span class="k">return</span><span class="p">;</span>
      <span class="p">}</span>
      <span class="n">const</span> <span class="n">res</span> <span class="o">=</span> <span class="n">stdout</span><span class="p">.</span><span class="nf">trim</span><span class="p">();</span>
      <span class="n">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="sb">`Response from Python: ${res.response}`</span><span class="p">);</span>

      <span class="n">bot</span><span class="p">.</span><span class="nf">say</span><span class="p">(</span><span class="sb">`Sorry, I don't know how to respond to "${trigger.text}" but llama might`</span><span class="p">)</span>
        <span class="p">.</span><span class="nf">then</span><span class="p">(()</span> <span class="o">=&gt;</span> <span class="n">bot</span><span class="p">.</span><span class="nf">say</span><span class="p">(</span><span class="sh">"</span><span class="s">markdown</span><span class="sh">"</span><span class="p">,</span> <span class="n">res</span><span class="p">))</span>
        <span class="o">//</span> <span class="p">.</span><span class="nf">then</span><span class="p">(()</span> <span class="o">=&gt;</span> <span class="nf">sendHelp</span><span class="p">(</span><span class="n">bot</span><span class="p">))</span>
        <span class="p">.</span><span class="nf">catch</span><span class="p">((</span><span class="n">e</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="n">console</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span><span class="sb">`Problem in the unexepected command hander: ${e.message}`</span><span class="p">));</span>
    <span class="p">});</span>
</code></pre></div></div>

<p>And lets see if that works:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bot-llama-response-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bot-llama-response-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bot-llama-response-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/bot-llama-response.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>Yes it does!</p>
<h2 id="final-thoughts">Final thoughts</h2>
<p>So this project started off as a fun way to get more interaction in a remote workplace, but I think the most important thing I found here is how to create my own inference api using a very high quality LLM in llama v2 using ngrok and colab. I think soon this may not even be an issue, people are finding ways to compact these opensource language models so that they can be run on local computers and even micro controllers like Raspberry Pis. I am excited to see where that goes.</p>

<p>This exposed me to some interesting concepts like ngrok for application development and the trend towards models being made light weight to run on devices is also very interesting – tiny ML as they call it. And I’ll probably go deeper into these topics later.</p>]]></content><author><name></name></author><category term="chat-bot," /><category term="llama," /><category term="networking," /><category term="colab" /><summary type="html"><![CDATA[Webex chatbot using llama.cpp and ngrok]]></summary></entry><entry><title type="html">Topic Modeling in Reddit</title><link href="https://spknash.github.io/blog/2023/Reddit-Topic-Modeling/" rel="alternate" type="text/html" title="Topic Modeling in Reddit" /><published>2023-07-28T00:00:00+00:00</published><updated>2023-07-28T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2023/Reddit-Topic-Modeling</id><content type="html" xml:base="https://spknash.github.io/blog/2023/Reddit-Topic-Modeling/"><![CDATA[<h1 id="most-common-topic-in-rchangemyview">Most Common Topic in r/changemyview</h1>

<p>In this post I am tackling the problem: What is the most talked about issue/topic on r/changemyview right now?</p>

<p>r/changemyview is a a subreddit where users post an opinion they have about a certain issue or topic and people post replies which try to pursuade the original poster to change their view. The original poster can then award delta points to the reply which pursuades them to change their view, or comes close by presenting a very good argument. Due to moderation and community rules, every single post has this same format. A opinion, and then replies which try to pursuade away from that opinion, and the replies which do an exceptional job receive “delta points”. Due to the structure and high quality of posts in this subreddit, r/changemyview is an ideal source of data for machine learning projects. Below is picture of what a r/changemyview post looks like.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/r:changemyview-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/r:changemyview-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/r:changemyview-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/r:changemyview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>For the problem I am trying to solve we only need the post titles and not the full scope of the data provided in the sub. The post title contains an opinion on a topic/issue and this is enough to determine the what topic the post is addressing. The first step to solve this problem no matter the strategy chosen is to fetch post titles over the timeframe you are interested in. Below is code for fetching titles from the top 50 post titles since last year as a sample. <code class="language-plaintext highlighter-rouge">reddit</code> is a read/write reddit instance that has been initialized with my reddit api credentials.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">submission</span> <span class="ow">in</span> <span class="n">reddit</span><span class="p">.</span><span class="nf">subreddit</span><span class="p">(</span><span class="sh">"</span><span class="s">changemyview</span><span class="sh">"</span><span class="p">).</span><span class="nf">top</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
      <span class="nf">print</span><span class="p">(</span><span class="n">submission</span><span class="p">.</span><span class="n">title</span><span class="p">)</span>
</code></pre></div></div>

<p>From here the question becomes how do we use this collection of post titles to determine the most common topic. I try a few different methods.
##Manual Labels &amp; Zero-shot Classification</p>

<p>The idea behind this first method is to manually provide several potential topics which we think are popular and use zero-shot classification to classify each post title. Either zero shot classification or trained classifier could be used but I thought zero shot classification(specifically the hugging face pipeline) might work reasonably well especially when the categories are very diverse. After manually scanning through some of the r/changemyview titles, here are the categories I came up with:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">candidate_labels</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">education</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">taxes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Trump</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">healthcare</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Religion</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">elections</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">race</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">LGBTQ</span><span class="sh">"</span><span class="p">]</span>
</code></pre></div></div>

<p>After that I used zero-shot classification in hugging-face pipelines to classify each title and here were the results of 5 posts to see if it was working as well as I wanted it to:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CMV: Mike Bloomberg's campaign is proof that the ultra wealthy in the US can afford a higher tax rate with no ill effect on them
[0.8761507272720337, 0.039841409772634506, 0.028353260830044746, 0.025303443893790245, 0.010511195287108421, 0.007160380948334932, 0.006719440221786499, 0.005960128735750914]
CMV: Kanye West is a shill for president Trump and running to syphon off young voters from voting for Biden.
[0.3772038221359253, 0.30558013916015625, 0.2326604723930359, 0.038126397877931595, 0.012778099626302719, 0.012653850950300694, 0.011820374056696892, 0.009176867082715034]
CMV: Most Americans who oppose a national healthcare system would quickly change their tune once they benefited from it.
[0.8781680464744568, 0.03057781793177128, 0.028276970610022545, 0.018674472346901894, 0.012455514632165432, 0.012006503529846668, 0.010698405094444752, 0.009142286144196987]
CMV: Donald Trump has not made a single lasting positive impact on the USA during his term as president.
[0.9717963933944702, 0.008201655931770802, 0.006410819478332996, 0.0034016254357993603, 0.0033515936229377985, 0.0030098608694970608, 0.0021523970644921064, 0.0016757362755015492]
CMV: being a conservative is the least Christ-like political view
[0.24276088178157806, 0.15054336190223694, 0.12963169813156128, 0.1104813814163208, 0.10187441110610962, 0.09882443398237228, 0.08833231031894684, 0.07755151391029358]
</code></pre></div></div>

<p>A quick look at how zero-shot classification is doing in these few examples reveals that it is not working that well. The topic “CMV: Donald Trump has not made a single lasting positive impact on the USA during his term as president.” is classified as 0.97 towards the topic <code class="language-plaintext highlighter-rouge">education</code> even though this topic has the word “Trump” in it and <code class="language-plaintext highlighter-rouge">Trump</code> is one of the categories. That is concerning but could be because the model doesn’t have embeddings for the word “Trump” as in president Trump instead of just the dictionary word. The same thing happens for “CMV: Most Americans who oppose a national healthcare system would quickly change their tune once they benefited from it.”. The classifier chooses <code class="language-plaintext highlighter-rouge">education</code> with 0.87 probability even though <code class="language-plaintext highlighter-rouge">healthcare</code> exists as an option.</p>

<p>Clearly, this strategy of manual labels and zero-shot classification is not effective. The next strategy I could explore is still within supervised topic modeling, but instead of zero-shot classification we use a classifier trained specifically for this classification task. The big problem with this method is there is no obvious way to get labels for each post title in the training dataset – manually would take very long. So instead of going in this route I will explore unsupervised topic modeling because there is no clear path forward within supervised modeling, and unsupervised modeling seems more interesting anyways because we can let the model identify distinctions between topics. So I will look at a Bag of Words model first.</p>

<h2 id="bag-of-wordslda">Bag of Words/LDA</h2>

<p>I’ll walk through how I used the bag of words method and LDA to perform unsupervised topic classification. Afterwards I’ll provide a brief overview of how these methods work and the motivation behind it.</p>

<p>The first step is to create a bag of words matrix, which will contain  For each document(each post title in this case) the frequency of each word in the dictionary is recorded in a row vector. The idea behind the bag of words matrix is to capture where words are repeating to provide information about which documents are covering the similar topics. For example if document 1 and document 3 both contain high frequency of the word “cat” they likely cover similar topics. This type of analysis clearly does not depend on many words which appear very frequently in the english language such as “as”, “to”, or “the”. These words are called stop words. Additionally, in this type of analysis there is no real difference between root words and extended words such as “happy” and “happiest”. Therefore, all words should be shortened to their shortest stem. Below is the function used to pre-process every post title. It tokenizes, deletes stop words, and shortens to smallest stem. This implementation uses the nltk library which is common for text preprocessing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">process_document</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
  <span class="n">tokens</span> <span class="o">=</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

  <span class="c1"># Removing Stop words
</span>  <span class="n">stop_words</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">stopwords</span><span class="p">.</span><span class="nf">words</span><span class="p">(</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">))</span>
  <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>

  <span class="c1"># Stemming
</span>  <span class="n">stemmer</span> <span class="o">=</span> <span class="nc">PorterStemmer</span><span class="p">()</span>
  <span class="n">stemmed_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="p">.</span><span class="nf">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">stemmed_tokens</span>
</code></pre></div></div>

<p>Below is an example to show exactly what the preprocessing is doing:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">process_document</span><span class="p">(</span><span class="sh">"</span><span class="s">hello, my name is Suhaas and I really like frisbee and tennis. What sports do you like?</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['hello',
 ',',
 'name',
 'suhaa',
 'i',
 'realli',
 'like',
 'frisbe',
 'tenni',
 '.',
 'what',
 'sport',
 'like',
 '?']
</code></pre></div></div>

<p>After preprocessing of the text is complete the bag of words matrix can be made.</p>

<p>The row vector of every document in the corpus are stacked on top of each other to form the Bag of words matrix. The row vector contains the frequency of each word. Below is a simple example. Say the corpus is</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>corpus = ["the dog is wet, the dog is angry",
  "the cat is upset and very hungry",
  "who let the dogs out? They are making a mess",
  "I can't believe the cat drank the milk"]
</code></pre></div></div>
<p>Then the bag of words matrix will be:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   angri  believ  ca  cat  dog  drank  hungri  let  make  mess  milk  upset  \
0      1       0   0    0    2      0       0    0     0     0     0      0   
1      0       0   0    1    0      0       1    0     0     0     0      1   
2      0       0   0    0    1      0       0    1     1     1     0      0   
3      0       1   1    1    0      1       0    0     0     0     1      0   

   wet  
0    1  
1    0  
2    0  
3    0
</code></pre></div></div>

<p>This matrix is pretty useful but can be made even more meaningful if rare words are given a greater weight and prevalence in the model. For example, in r/changemyview topics the word “abortion” is very rare. But when it does appear it is almost certainly the topic of the entire post. There are many words like this: words which are rare in the english language but determine the topic of the document when they do appear. This is where TF-IDF comes in. TF-IDF is a method to alter the bag of words matrix to increase the weight on these important words which are rare in the corpus. TF-IDF is composed of two parts:</p>

<p>Term Frequency (TF): This is simply the frequency of a word in a document. It’s based on the idea that the importance of a word is proportional to its frequency. However, some words like ‘the’, ‘is’, and ‘and’ appear frequently in all sorts of contexts, so high frequency doesn’t always mean high importance. That’s where the second part of TF-IDF comes in.</p>

<p>Inverse Document Frequency (IDF): This reduces the weight of words that are common in the corpus. IDF is calculated as the logarithm of the total number of documents in the corpus divided by the number of documents containing the term. Thus, it increases for rare words and decreases for common words.</p>

<p>The overall TF-IDF score for a word in a document is the product of its TF and IDF scores.</p>

<p>Here’s the mathematical formula for TF-IDF:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TF-IDF(t, d, D) = TF(t, d) * IDF(t, D)
</code></pre></div></div>

<p>where:</p>

<p><code class="language-plaintext highlighter-rouge">t</code> is the term or word
<code class="language-plaintext highlighter-rouge">d</code> is the document
<code class="language-plaintext highlighter-rouge">D</code> is the corpus
<code class="language-plaintext highlighter-rouge">TF(t, d)</code> is the term frequency of t in d (usually normalized by dividing by the total number of words in d)
<code class="language-plaintext highlighter-rouge">IDF(t, D)</code> is the inverse document frequency of t in D, calculated as <code class="language-plaintext highlighter-rouge">log(N / df(t))</code>, where N is the total number of documents and df(t) is the number of documents that contain t (to prevent division by zero if a word is not in the corpus, it’s common to add 1 to the denominator)</p>

<p>This calculation increases the weight of words which are rare in the corpus but frequent in a particular document. Such a word is probably very relevant to the topic of that document. Below is a implementation of creating a bag of words matrix using TF-IDF(this implementation uses the sci-kit library):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">BAG_matrix</span><span class="p">(</span><span class="n">corpus</span><span class="p">):</span>
  <span class="c1"># Apply preprocessing to each document in the corpus
</span>  <span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="nf">process_document</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>

  <span class="c1"># Initialize CountVectorizer
</span>  <span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

  <span class="c1"># Tokenize and build vocab
</span>  <span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

  <span class="c1"># Now, we initialize the TfidfTransformer and transform our count-matrix to tf-idf representation
</span>  <span class="n">transformer</span> <span class="o">=</span> <span class="nc">TfidfTransformer</span><span class="p">(</span><span class="n">smooth_idf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">use_idf</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="n">tfidf</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

  <span class="c1"># Output the shape of X
</span>  <span class="nf">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

  <span class="c1"># To get feature names
</span>  <span class="n">feature_names</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">()</span>

  <span class="c1"># To view the matrix as a DataFrame
</span>  <span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
  <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">tfidf</span><span class="p">.</span><span class="nf">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_names</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tfidf</span><span class="p">,</span> <span class="n">vectorizer</span>
</code></pre></div></div>

<p>And below is the matrix of the same corpus from earlier but using tf-idf now:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     angri    believ        ca       cat       dog     drank    hungri  \
0  0.47212  0.000000  0.000000  0.000000  0.744450  0.000000  0.000000   
1  0.00000  0.000000  0.000000  0.486934  0.000000  0.000000  0.617614   
2  0.00000  0.000000  0.000000  0.000000  0.414289  0.000000  0.000000   
3  0.00000  0.465162  0.465162  0.366739  0.000000  0.465162  0.000000   

        let      make      mess      milk     upset      wet  
0  0.000000  0.000000  0.000000  0.000000  0.000000  0.47212  
1  0.000000  0.000000  0.000000  0.000000  0.617614  0.00000  
2  0.525473  0.525473  0.525473  0.000000  0.000000  0.00000  
3  0.000000  0.000000  0.000000  0.465162  0.000000  0.00000  
</code></pre></div></div>

<p>The next step is to use this tf-idf matrix to construct topics. This can be done using Latent Dirichlet Allocation.
We will be using the LDA method in sci-kit but here is an overview of how LDA works:</p>

<p>Initialize LDA: First, initialize the LDA model. One key parameter to set here is the number of topics you want the model to identify. This is a hyperparameter that might need to be tuned to get the best results.</p>

<p>Assign topics to words: LDA starts by randomly assigning each word in each document to one of the K topics (where K is the number of topics you decided on). This random assignment already gives you both topic representations of all the documents and word distributions of all the topics (albeit not very good ones because it is random).</p>

<p>Iteratively update topic assignments: Then, LDA iteratively updates the topic assignments for each word in each document, based on two criteria:</p>

<p>a. How prevalent is the topic in the document? The more often the topic occurs in the document, the more likely it is that the word belongs to this topic.</p>

<p>b. How prevalent is the word across topics? If a word is already often assigned to a topic, it’s likely that it will be assigned to this topic again.</p>

<p>Each iteration of this step is done using a method called Gibbs Sampling, which is a type of Markov Chain Monte Carlo (MCMC) algorithm.</p>

<p>The process continues until the model’s estimates of the topics stabilize, or after a set number of iterations. Once finished, you’ll evaluate the topics that the model has learned. After this iterative process, LDA will represent each document in the corpus as a mixture of different topics(learns a distribution of topics for each document), and represents each topic as a set of top words(learns a distribution of words for each topic). More information about LDA can be found in the paper by Blie, Jordan, and Ng <a href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">here.</a></p>

<p>Below is an implementattion using the bag of words matrix from earlier and an LDA method from sci-kit. It produces <code class="language-plaintext highlighter-rouge">num_topics</code> and prints the top 5 words for each topic to allow us to see what each topic is really about.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">LDA_topics</span><span class="p">(</span><span class="n">tfidf</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">,</span> <span class="n">num_topics</span><span class="p">):</span>
  <span class="c1"># Initialize LDA
</span>  <span class="c1"># n_components specifies the number of topics
</span>  <span class="n">lda</span> <span class="o">=</span> <span class="nc">LatentDirichletAllocation</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">num_topics</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

  <span class="c1"># Fit LDA to BoW data
</span>  <span class="n">lda</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">tfidf</span><span class="p">)</span>

  <span class="c1"># For each topic, print the top 10 most representative words
</span>  <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">topic</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">lda</span><span class="p">.</span><span class="n">components_</span><span class="p">):</span>
      <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Top 5 words for Topic #</span><span class="si">{</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
      <span class="nf">print</span><span class="p">(</span>
          <span class="p">[</span><span class="n">vectorizer</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">()[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">topic</span><span class="p">.</span><span class="nf">argsort</span><span class="p">()[</span><span class="o">-</span><span class="mi">5</span><span class="p">:]])</span>
      <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>This function takes the tfidf matrix and vectorizer as inputs from the bag of words function – The function below creates the tf-idf matrix and topics using LDA.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_topics</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">num_topics</span><span class="p">):</span>
  <span class="n">tfidf</span><span class="p">,</span> <span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">BAG_matrix</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
  <span class="nc">LDA_topics</span><span class="p">(</span><span class="n">tfidf</span><span class="p">,</span><span class="n">vectorizer</span><span class="p">,</span><span class="n">num_topics</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we are almost ready to use this function to generate topics from r/changemyview posts! The last thing left to do is to produce the corpus which will be a list post title from the subreddit. The code below creates the corpus:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">reddit_corpus</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">subreddit</span> <span class="o">=</span> <span class="n">reddit</span><span class="p">.</span><span class="nf">subreddit</span><span class="p">(</span><span class="sh">"</span><span class="s">changemyview</span><span class="sh">"</span><span class="p">)</span>
<span class="n">after</span> <span class="o">=</span> <span class="bp">None</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">after</span><span class="p">:</span>
        <span class="n">new_posts</span> <span class="o">=</span> <span class="n">subreddit</span><span class="p">.</span><span class="nf">top</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">after</span><span class="sh">'</span><span class="p">:</span> <span class="n">after</span><span class="p">})</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">new_posts</span> <span class="o">=</span> <span class="n">subreddit</span><span class="p">.</span><span class="nf">top</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

    <span class="n">last_post</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">post</span> <span class="ow">in</span> <span class="n">new_posts</span><span class="p">:</span>
        <span class="n">reddit_corpus</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">post</span><span class="p">.</span><span class="n">title</span><span class="p">[</span><span class="mi">4</span><span class="p">:])</span>
        <span class="n">last_post</span> <span class="o">=</span> <span class="n">post</span>

    <span class="n">after</span> <span class="o">=</span> <span class="n">last_post</span>  <span class="c1"># ID of the last post
</span>
<span class="k">for</span> <span class="n">submission</span> <span class="ow">in</span> <span class="n">reddit</span><span class="p">.</span><span class="nf">subreddit</span><span class="p">(</span><span class="sh">"</span><span class="s">changemyview</span><span class="sh">"</span><span class="p">).</span><span class="nf">top</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
      <span class="c1">#print(submission.title[4:])
</span>      <span class="n">reddit_corpus</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">submission</span><span class="p">.</span><span class="n">title</span><span class="p">[</span><span class="mi">4</span><span class="p">:])</span>
</code></pre></div></div>

<p>For each post I append <code class="language-plaintext highlighter-rouge">post.title[4:]</code> instead of the full post because each post starts with “CMV:” and we don’t want CMV to be one of the words which define a topic since it is present in every title. Additionally, the reddit API only allows 60 queries per minute and on 500 posts per query so I had to use the <code class="language-plaintext highlighter-rouge">params={'after': after})</code> parameter to start the next query where the previous one left off.</p>

<p>Now lets try out how the LDA topic generation did.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">make_topics</span><span class="p">(</span><span class="n">reddit_corpus</span><span class="p">,</span><span class="mi">15</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Top 5 words for Topic #1
['work', 'licens', 'conserv', 'the', 'religion']


Top 5 words for Topic #2
['charact', 'the', 'race', 'us', 'racist']


Top 5 words for Topic #3
['noth', 'there', 'sex', 'peopl', 'gender']


Top 5 words for Topic #4
['tri', 'cover', 'parent', 'world', 'sub']


Top 5 words for Topic #5
['joke', 'eat', 'women', 'citizenship', 'the']


Top 5 words for Topic #6
['offens', 'help', 'place', 'polic', 'the']


Top 5 words for Topic #7
['anim', 'much', 'small', 'commun', 'peopl']


Top 5 words for Topic #8
['child', 'flag', 'the', 'peopl', 'chang']


Top 5 words for Topic #9
['parti', 'make', 'peopl', 'vote', 'we']


Top 5 words for Topic #10
['wealth', 'donald', 'if', 'bodi', 'posit']


Top 5 words for Topic #11
['realiti', 'answer', 'appropri', 'cultur', 'thing']


Top 5 words for Topic #12
['hire', 'incom', 'job', 'includ', 'social']


Top 5 words for Topic #13
['consid', 'there', 'health', 'it', 'peopl']


Top 5 words for Topic #14
['homeless', 'there', 'it', 'see', 'wrong']


Top 5 words for Topic #15
['like', 'need', 'cultur', 'us', 'get']
</code></pre></div></div>

<p>Ok! so definetly an improvement from zero-shot learning, but still room for improvement as well. Some topics have top words that are very connected and make sense with how humans would define topics in this corpus. For example topic 3</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'charact', 'the', 'race', 'us', 'racist'
</code></pre></div></div>
<p>seems to be about race and culture.</p>

<p>Topic 12 is</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['hire', 'incom', 'job', 'includ', 'social']
</code></pre></div></div>
<p>This topic seems to be about employment and jobs.</p>

<p>And this topic:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['noth', 'there', 'sex', 'peopl', 'gender']
</code></pre></div></div>
<p>seems to be about gender and sexuality issues. On that same note there are some topics which don’t have a common theme like these ones:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['homeless', 'there', 'it', 'see', 'wrong']
['joke', 'eat', 'women', 'citizenship', 'the']
</code></pre></div></div>
<p>This is to be expected and highlights the limitations of LDA. Because it is a unsupervised topic model, the topics may not be formed in the same way a human would. The algorithms may form a topic based on some similarities or frequency of a particular word which we don’t see as important. Additionally, LDA does not take into account the order and semantics fo the words. For example, LDA would think “this post is about police” and “this post is not about police” are both about police simply because it contains the word “police”. Despite not taking into account semantics and word order it is pretty cool to see it produce some topics the similar to how a human would.</p>

<p>There are also some hyper parameters which could be tuned to produce better results like: number of topics, n-grams during bag of words matrix, number of top words for each topic. Perhaps the results of this topic modeling could be improved a little more just by further tuning these hyper parameters.</p>

<p>I am really interested to see how a model which does take into account the order of words could be used for topic modeling. An example includes BERTopic. There are also many extensions to LDA such as Dynamic LDA, Hierarchical Dirichlet Process, and GuidedLDA. I may explore these other techniques for topic modeling in future posts.</p>]]></content><author><name></name></author><category term="topic-modeling," /><category term="reddit," /><category term="colab" /><summary type="html"><![CDATA[Supervised and Unsupervised Topic modeling on r/changemyview]]></summary></entry><entry><title type="html">Apple-Pineapple Classifier</title><link href="https://spknash.github.io/blog/2023/PPAP/" rel="alternate" type="text/html" title="Apple-Pineapple Classifier" /><published>2023-07-14T00:00:00+00:00</published><updated>2023-07-14T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2023/PPAP</id><content type="html" xml:base="https://spknash.github.io/blog/2023/PPAP/"><![CDATA[<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/PPAP-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/PPAP-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/PPAP-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/PPAP.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>In this post I’m going over how I made
<a href="https://huggingface.co/spaces/suhaaspk/PPAP">this</a> simple
hugging face space which can classify between different fruits when you
input an image to the app. The first step is training the model which
can classify apples and pineapples.</p>

<p>I trained on google colab. Below are the necesary imports and mounting
to my personal google drive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">Uqq</span> <span class="n">fastai</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">Uqq</span> <span class="n">bing_image_downloader</span>
<span class="kn">from</span> <span class="n">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">Uqq</span> <span class="n">gradio</span>
<span class="kn">import</span> <span class="n">gradio</span> <span class="k">as</span> <span class="n">gr</span>
<span class="kn">from</span> <span class="n">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="p">.</span><span class="nf">mount</span><span class="p">(</span><span class="sh">'</span><span class="s">/content/drive</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p>The plan is to use FastAI’s vision learner which is already trained to
recognize many things in images, and finetune this model using images of
apples and pineapples. The images to finetune are fetched using bing
search. 100 images of each fruit are used.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">bing_image_downloader</span> <span class="kn">import</span> <span class="n">downloader</span>

<span class="n">downloader</span><span class="p">.</span><span class="nf">download</span><span class="p">(</span><span class="sh">"</span><span class="s">apple fruit</span><span class="sh">"</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="n">output_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">dataset</span><span class="sh">'</span><span class="p">,</span> <span class="n">adult_filter_off</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">force_replace</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">downloader</span><span class="p">.</span><span class="nf">download</span><span class="p">(</span><span class="sh">"</span><span class="s">pineapple fruit</span><span class="sh">"</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="n">output_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">dataset</span><span class="sh">'</span><span class="p">,</span> <span class="n">adult_filter_off</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">force_replace</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
</code></pre></div></div>

<p>Below is a small sample of the images taken from bing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">path</span> <span class="o">=</span> <span class="nc">Path</span><span class="p">(</span><span class="sh">'</span><span class="s">dataset</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">ImageDataLoaders</span><span class="p">.</span><span class="nf">from_folder</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">,</span> <span class="n">valid_pct</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">item_tfms</span><span class="o">=</span><span class="nc">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">))</span>

<span class="c1"># Checking the data
</span><span class="n">dls</span><span class="p">.</span><span class="nf">show_batch</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cell-4-output-1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cell-4-output-1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cell-4-output-1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cell-4-output-1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Sample of the bing images to fine tune the model
</div>

<p>The next step is to finetune using the images and this is done using
fastai’s learner in the following lines. It runs for 3 epochs. The
training error continuously decreases as well as the validation error.
The validation error is only slightly larger than training error and the
distance between the two errors decreases for each epoch so I feel good
there is not much overfitting.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span> <span class="o">=</span> <span class="nf">vision_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet18</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">error_rate</span><span class="p">)</span>
<span class="n">learn</span><span class="p">.</span><span class="nf">fine_tune</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<p>After the model is done training, I export the model to my google drive
so I can upload to my hugging face spaces app.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span><span class="p">.</span><span class="nf">export</span><span class="p">(</span><span class="sh">'</span><span class="s">model.pkl</span><span class="sh">'</span><span class="p">)</span>
<span class="err">!</span><span class="n">mv</span> <span class="sh">'</span><span class="s">dataset/model.pkl</span><span class="sh">'</span> <span class="sh">'</span><span class="s">/content/drive/My Drive/Colab Projects/Apple-Pineapple</span><span class="sh">'</span>
<span class="n">learn</span> <span class="o">=</span> <span class="nf">load_learner</span><span class="p">(</span><span class="sh">'</span><span class="s">drive/My Drive/Colab Projects/Apple-Pineapple/model.pkl</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p>To create a hugging face space go
<a href="https://huggingface.co/spaces/suhaaspk/PPAP">hugging face</a>
and click create a new space. For this space I am going to use gradio.
After creating the space you can add files and edit the app with git. I
added the model.pkl which I had trained earlier to the space. And added
app.py which creates the app.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">gradio</span> <span class="k">as</span> <span class="n">gr</span>
<span class="kn">from</span> <span class="n">gradio</span> <span class="kn">import</span> <span class="n">components</span>
<span class="kn">from</span> <span class="n">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">learn</span> <span class="o">=</span> <span class="nf">load_learner</span><span class="p">(</span><span class="sh">'</span><span class="s">model.pkl</span><span class="sh">'</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">learn</span><span class="p">.</span><span class="n">dls</span><span class="p">.</span><span class="n">vocab</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">PILImage</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">img</span><span class="p">))</span>
    <span class="n">pred</span><span class="p">,</span><span class="n">pred_idx</span><span class="p">,</span><span class="n">probs</span> <span class="o">=</span> <span class="n">learn</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="nf">float</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))}</span>


<span class="n">iface</span> <span class="o">=</span> <span class="n">gr</span><span class="p">.</span><span class="nc">Interface</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">predict</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">components</span><span class="p">.</span><span class="nc">Image</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">components</span><span class="p">.</span><span class="nc">Label</span><span class="p">(</span><span class="n">num_top_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>

<span class="n">iface</span><span class="p">.</span><span class="nf">launch</span><span class="p">()</span>
</code></pre></div></div>

<p>I could add app.py with git but model.pkl could not be added with git
because the file was too large, so I added it manually on hf spaces.
After the adding these files the app was able to build and now runs on
hf spaces!</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/files-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/files-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/files-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/files.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    The files in hf spaces to make the app.
</div>

<p>The final product and in action:</p>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hf_app_final-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hf_app_final-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hf_app_final-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/hf_app_final.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>]]></content><author><name></name></author><category term="tutorial," /><category term="intro" /><summary type="html"><![CDATA[A simple hugging-face space which can be used to classify fruit.]]></summary></entry><entry><title type="html">First Post!</title><link href="https://spknash.github.io/blog/2023/first-post/" rel="alternate" type="text/html" title="First Post!" /><published>2023-07-02T00:00:00+00:00</published><updated>2023-07-02T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2023/first-post</id><content type="html" xml:base="https://spknash.github.io/blog/2023/first-post/"><![CDATA[<h2 id="motivation">Motivation</h2>

<p>Hello there! this is my first post. My motivation for writing here is to document my learning in the topics I am interested in (ML/DL, system design, math, programming, tech, etc.). I have always been very interested in how individuals continue to learn and improve their craft. I thought Jeremy Howards thoughts on how to learn are very insightful. He believes that practical application of a new concept is a more valuable first step than trying to understand all the foundational underpinnings. I tend to agree with that sentiment. Despite having so much information about how to learn, and how to improve I feel like there are still so many questions that are left unanswered. If you’re trying to learn more about Deep learning, what did you do each week? How did your view/understanding of a topic change as you went deeper into it? I am interested in these types of questions and by documenting my learning and improvement starting from the level of a beginner in many ways, I will leave behind a record of what I did, and how it helped me learn. This record will be really interesting for me to look back on, and hopefully it can help someone else too.</p>

<hr />]]></content><author><name></name></author><category term="intro" /><summary type="html"><![CDATA[An overview of things I will talk about here]]></summary></entry></feed>