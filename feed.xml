<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://spknash.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://spknash.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-03-06T20:06:02+00:00</updated><id>https://spknash.github.io/feed.xml</id><title type="html">Suhaas</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">2023 Media Recap</title><link href="https://spknash.github.io/blog/2024/media-recap/" rel="alternate" type="text/html" title="2023 Media Recap" /><published>2024-02-23T00:00:00+00:00</published><updated>2024-02-23T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2024/media-recap</id><content type="html" xml:base="https://spknash.github.io/blog/2024/media-recap/"><![CDATA[<p>Finished writing. Adding formating, and images</p>]]></content><author><name></name></author><category term="books," /><category term="fun" /><summary type="html"><![CDATA[A summary of all the books, podcasts, blogs, tv shows and movies I consumed in 2023 and what stood out]]></summary></entry><entry><title type="html">AI generated profile picture</title><link href="https://spknash.github.io/blog/2024/stable-diffusion-profilepic/" rel="alternate" type="text/html" title="AI generated profile picture" /><published>2024-02-16T00:00:00+00:00</published><updated>2024-02-16T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2024/stable-diffusion-profilepic</id><content type="html" xml:base="https://spknash.github.io/blog/2024/stable-diffusion-profilepic/"><![CDATA[<p>I want to create a profile picture for myself using stable diffusion. I’ve seen people online such as Abhishek Thakur and Max Howell use AI-generated profile pictures. The good ones that I’ve seen make the photo more interesting but preserve the subject’s core facial features. Below are some that I like.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/mahowellreall-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/mahowellreall-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/mahowellreall-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/mahowellreall.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/maxhowellAI-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/maxhowellAI-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/maxhowellAI-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/maxhowellAI.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Max Howell
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/AbhiREAL-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/AbhiREAL-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/AbhiREAL-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/AbhiREAL.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/AbhiAI-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/AbhiAI-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/AbhiAI-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/AbhiAI.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Abhishek Thakur
</div>

<p>To create my profile picture the first thing I did was do some research into what models are used to create these images. The most popular open-source models are the diffusion models released by <a href="https://stability.ai/">Stability AI</a> and <a href="https://app.runwayml.com/">runway ML</a>. The most recent of which is Stable Diffusion 3. Their older model Stable Diffusion XL is still being updated by the open-source community and is still considered the SOTA model by most.</p>

<h2 id="stable-diffusion">Stable Diffusion</h2>

<p>The first thing I tried out was vanilla stable diffusion text-to-image. The model I am using is Stable Diffusion XL (SDXL) I simply described my appearance to see what stable diffusion returned. Some results are below.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/zeroshot0-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/zeroshot0-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/zeroshot0-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/zeroshot0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/zeroshot2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/zeroshot2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/zeroshot2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/zeroshot2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/zeroshot3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/zeroshot3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/zeroshot3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/zeroshot3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: indian boy with curly hair, jawline, trimmed beard, smiling, dimples, pimple on cheek, in a forest, pixel art, colorful, vibrant, cheerful
</div>

<p>I obviously did not expect this method to give any output close to desirable because the model has no information about my appearance. Next, I tried using
the image-to-image pipeline of the model. The input image is an image of myself. Some results are below.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/im2im1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/im2im1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/im2im1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/im2im1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/im2im2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/im2im2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/im2im2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/im2im2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/im2im3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/im2im3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/im2im3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/im2im3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: smiling man, comic book style
</div>

<p>I expected these images to be better since the model has access to my face. However, the out-of-the-box image to image method is also not what I am looking for here.
The model seems to change many of my core facial features and it does not look like me at all. The model still does not retain information about the features of my face.
Since the out-of-box pipelines for SDXL do not give me what I want, I began researching how a model can learn a subject’s features. This research brought me to
Dreambooth.</p>

<h2 id="dreambooth">Dreambooth</h2>

<p>Dreambooth is a method to finetune a pretrained text-to-image model with a set of images of a subject along with captions including the subject’s unique identifier. The final model will
then be able to generate images including the subject when the unique identifier is used in the prompt. Below is the example used in the 2022 <a href="https://arxiv.org/abs/2208.12242">paper</a> which introduced Dreambooth.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/dreambooth-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/dreambooth-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/dreambooth-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/dreambooth.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
source: Google Research
</div>

<p>I first tried using the example included in the HuggingFace Diffusers library. I inputted my images along with their captions and began the training. Below are some of the results.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/bad1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/bad1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/bad1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/bad1.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/bad2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/bad2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/bad2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/bad2.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/bad3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/bad3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/bad3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/bad3.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of jrdnpl man, formal (added pencil sketch to last one)
</div>

<p>I was surprised by the low quality of these images, and I was initially unsure why these images were such low quality. I realized what the issue was when I used a different training script. I then used TheLastBen’s training script
and there were several things different about this training script. It encouraged creating a class images folder. The base class images are images of the class of the subject but not the subject itself. For this use case, I am the subject, so the class
would be a person. I generated class images using the prompt “portrait of a man, photorealistic”. Below are some of the results of this model.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/frizzy-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/frizzy-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/frizzy-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/frizzy.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of jrdnpl man, 20 year old man, smiling
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/jungle-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/jungle-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/jungle-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/jungle.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of jrdnpl man, 20 year old man, in jungle
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/indian-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/indian-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/indian-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/indian.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of jrdnpl man, 20 year old man, indian
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/punk-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/punk-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/punk-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/punk.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/punk5-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/punk5-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/punk5-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/punk5.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/sajh-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/sajh-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/sajh-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/sajh.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of jrdnpl man, 20 year old man, wearing sweatshirt
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/punk9-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/punk9-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/punk9-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/punk9.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/punk7-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/punk7-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/punk7-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/punk7.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of jrdnpl man, 20 year old man, wearing sweatshirt, bubblegum background, punk
</div>

<p>These are much better! These images clearly show the core facial features of my face, unlike the previous scripts. The base classification images seem to help the model quite a bit. TheLastBen’s scripts also have many comments to understand what each of the parameters does.
Increasing the number of epochs of the training improved the quality of the images significantly as well.</p>

<p>I wanted to see if I could get the model to output a portrait of me in different styles. Below are some examples of pencil sketches, anime style, and comic book style.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/image-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/image-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/image-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/image.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of jrdnpl man, 20 year old man, smiling, anime style
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/punk3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/punk3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/punk3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/punk3.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of jrdnpl man, 20 year old man, smiling, comic book style
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/another-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/another-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/another-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/another.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of jrdnpl man, 20 year old man, smiling, pencil sketch
</div>

<p>These are not what I was hoping for! It seems the model is unable to switch to a different style and maintain the subject’s core features. When I prompt for anime style it simply outputs a typical anime character that does not look like me at all. 
I thought a possible reason may be the base class images. The class images show the model what kind of images it should produce. I trained a new model using pencil sketch pictures as the base class images. The results are below.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/closeup-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/closeup-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/closeup-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/closeup.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/muscular-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/muscular-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/muscular-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/muscular.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of jrdnpl man, 20 year old man, wearing sweatshirt, bubblegum background, punk
</div>

<p>As you can see, these images are still not what I was hoping for. The subject does not look like me at all. Despite the model not working well for particular styles like “anime style” and “pencil sketch”, it does work very well for other styles like “punk”, “impressionist”, and “oil painting”. 
Some examples are below.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/dramatic2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/dramatic2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/dramatic2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/dramatic2.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/dramatic3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/dramatic3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/dramatic3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/dramatic3.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/dramatic5-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/dramatic5-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/dramatic5-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/dramatic5.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of jrdnpl man, 20 year old man, oil painting, dramatic
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/punk4-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/punk4-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/punk4-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/punk4.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of jrdnpl man, 20 year old man, wearing sweatshirt, animated, punk
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/impressionist1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/impressionist1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/impressionist1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/impressionist1.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/impressionist2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/impressionist2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/impressionist2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/impressionist2.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/impressionist4-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/impressionist4-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/impressionist4-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/impressionist4.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of jrdnpl man, 20 year old man, oil painting, impressionist
</div>

<p>I think these styles work because they are much closer to the styles present in the input images. All of the input images could be described as “photorealistic, hd”. To produce images in a specific style, it is possible to train stable diffusion on a style as well as a subject.
This could be promising and also very useful – If you see a style of animation you like you could potentially create your image in that style by training stable diffusion. Despite, some of these pictures turning out very well, there are some issues which are present in many of them.
In many photos, I am given elephant ears and my hair is usually very bushy. For a professional artist or someone who wants their image to be exactly a specific kind of way, it would be nice if you could take an image produced by stable diffusion and then modify it to your needs.
This is where a tool called ControlNet has a lot of potential.</p>

<h2 id="controlnet">ControlNet</h2>

<p>ControlNet is a method for controlling image diffusion models by providing the model with an image as a reference in the last few resolution steps. This method was originally introduced in a <a href="https://arxiv.org/abs/2302.05543">paper</a> published in February 2023. The most common use of ControlNet is with a Canny image. There are several other types of images you can feed into a ControlNet model
which are all very interesting like depth, segmentation, and human pose. But for my use case, I think Canny is the most appropriate.
A canny image is an image with only the edges turned on, all the other pixels are black. Below is an example of a canny image.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/Canny-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/Canny-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/Canny-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/Canny.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Source: Sofiane Sahir on medium
</div>

<p>The Canny image is fed into the model in the last few steps of resolution causing the output image to be very similar to the original image. By feeding in the Canny, the idea is that the model must conform to the core features of the original images, but has the freedom to create something original between the edges.
Below is an example where I use my image and then give the model the prompt “Mona Lisa”. The model will try to show me the Mona Lisa but it must conform to features of the canny image.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/ControlNetmona-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/ControlNetmona-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/ControlNetmona-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/ControlNetmona.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: Mona Lisa
</div>

<p>That worked decently well! Now I will try the thing I was interested in earlier. Try to make the portrait of myself but in an interesting animation style. Below are some initial results.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/ControlNetinit1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/ControlNetinit1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/ControlNetinit1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/ControlNetinit1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of a man, anime style
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/ControlNetinit2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/ControlNetinit2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/ControlNetinit2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/ControlNetinit2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of a man, anime style
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/ControlNetinit3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/ControlNetinit3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/ControlNetinit3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/ControlNetinit3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of a man, comic book style
</div>

<p>Those are ok, and certainly much better than the Dreambooth results. Since many of these seem underdeveloped and without color I will increase the number of resolution steps to see if that makes an improvement.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/comicbook1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/comicbook1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/comicbook1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/comicbook1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of a man, comic book style
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/Controlnet2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/Controlnet2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/Controlnet2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/Controlnet2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of a man, anime style
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/Controlnet3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/Controlnet3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/Controlnet3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/Controlnet3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of a man, anime style
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/Controlnet4-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/Controlnet4-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/Controlnet4-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/Controlnet4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Prompt: portrait of a man, comic book style
</div>

<p>Ok! These are much better and quite good. There are many other ControlNet techniques I could try using but I’ll stop here for now and explore ControlNet further at a later time. There is no question that ControlNet is a very powerful technique.</p>

<h2 id="automatic1111">AUTOMATIC1111</h2>

<p>At the beginning of this process, I was calling the inference of the model using the HuggingFace Diffusers library in Colab. Although this was serviceable, it caused some difficulties and inconveniences. I had to check what parameters I could put into the pipeline function
and had to manually change the parameters in a way that was not very user-friendly. Eventually, I learned of different user interfaces that can be used for image generation workflows. The most popular one is called AUTOMATIC1111 or A1111 and it is a web-based UI that 
makes producing images and altering parameters much easier. There are other similar open-source UIs available but A1111 is by far the most popular. I had not heard of it before diving into this topic, but this project was one of the biggest open-source successes in the last year
with over 500 different contributors.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Throughout this process of trying to create an interesting profile picture for myself, I became familiar with several very powerful tools and techniques. I used stable diffusion (SDXL) as the pretrained image generation model. I used Dreambooth to 
train stable diffusion on my face so I could use my face as a concept in new images. I also began using ControlNet to make modifications to generated images and have more control over the output. Simultaneously, I began using the A1111 user interface which is 
immensely useful in the image generation workflow. Based on my research and asking questions online, this is a very common stack of tools, and workflow to use among many AI art people. Of course, new techniques and tools are being added as we speak because the 
field is advancing so quickly. There is another popular technique called Segment Anything (SAM) from 2023 which is also very popular. Diffusion models for video generation and video editing are also advancing rapidly.</p>

<p>The advantage of all of the tools that I used is that they are open source. As long as you have access to a gpu you can use any of these techniques for free. A downside to these tools is their disjointedness and ease of use. It is certainly not straightforward how to proceed
if you are trying to create an AI image using only open-source software. There are closed-source solutions for AI art as well. The two major models which compete with Stable Diffusion are Midjourney and DALL-E. I have not tried either of these paid options and I do not know
if these platforms allow users to use Dreambooth and ControlNet in a user-friendly way. I would doubt it since both of these methods are still fairly new and there has not been enough time to integrate these methods into a smooth UI. It is also relevant to note that
many AI images that are customized or have specific requirements use open-source software because open-source methods allow for much more customization. They also allow you to use the newest image generation techniques such as ControlNet, and SAM without having to wait for a closed-source
platform to offer support. However, for general-purpose AI images – images that do not need to be in a specific style, or contain a specific subject – Midjourney and DALL-E can perform on par or better than Stable Diffusion. They are also much easier to use. For general use I would say they are all in the
same range of capability. Below are some images fromMidjourney v6, DALL-E 3, and Stable Diffusion 3 to compare.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/midjourneyv6-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/midjourneyv6-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/midjourneyv6-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/midjourneyv6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Modjourney v6 is known for the amount of detail, and realism of its images (Source: Ars Technica)
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/DallE3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/DallE3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/DallE3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/DallE3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
DallE3 showing nuclear war (Source: stable-diffusion-art.com)
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-pic/sd3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-pic/sd3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-pic/sd3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/profile-pic/sd3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
Stable Diffusion 3 came out this week and touts it capability to include text in the image. The prompt for this image was: "Epic anime artwork of a wizard atop a mountain at night casting a cosmic spell into the dark sky that says 'Stable Diffusion 3' made out of colorful energy" (Source: Stability AI)
</div>

<p>Another important thing to note is that creating these images was not completely free of charge for me. I do not have a GPU so I bought 100 GPU hours for $10 on Google Colab. I trained a total of 4 models and did inference many times. After my usage the last
couple weeks I am left with 65.4 GPU hours. So my total cost for this post was $3.46.</p>

<p>For future work, I am very interested in diving deeper into the architecture of Stable Diffusion models, as well as exploring further how Dreambooth and ControlNet work. ControlNet especially seems to be a very promising technique since it gives so much control to the user. 
I will certainly continue playing around with ControlNet as I learn more about diffusers and image generation.</p>]]></content><author><name></name></author><category term="HuggingFace," /><category term="diffusers," /><category term="stable-diffusion," /><category term="dreambooth," /><category term="controlnet" /><summary type="html"><![CDATA[My approach trying to create a AI-generated profile picture, overview of tools used]]></summary></entry><entry><title type="html">Financial Phrases Classification (part 2)</title><link href="https://spknash.github.io/blog/2024/company-sentiment-app/" rel="alternate" type="text/html" title="Financial Phrases Classification (part 2)" /><published>2024-02-09T00:00:00+00:00</published><updated>2024-02-09T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2024/company-sentiment-app</id><content type="html" xml:base="https://spknash.github.io/blog/2024/company-sentiment-app/"><![CDATA[<h2 id="the-app">The App</h2>

<p>I used the model I made trained in the previous post to make an app which shows the recent sentiment of a company on Twitter when you input its stock tag on twitter. Check it out <a href="https://huggingface.co/spaces/suhaaspk/Company-Sentiment">here!</a>. You can find the code <a href="https://huggingface.co/spaces/suhaaspk/Company-Sentiment/tree/main">here</a>. I may have to disconnect the app if I make more spaces in the future. If that happens, below is a screenshot for reference.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/financial-phrases/app-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/financial-phrases/app-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/financial-phrases/app-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/financial-phrases/app.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">

    
</div>

<p>I use the Twitter API to fetch a 1000 of the most recent tweets and use the sentiment classifying pipeline to classify each tweet.</p>

<h2 id="improvements">Improvements</h2>

<p>One way this app can be improved is: The app currently assesses the overall sentiment of the tweet and not the sentiment of the tweet with regards to stock in question. For example, the statement: “Google did good last quarter, but Paypal did bad” may be classified as neutral since it assesses the whole tweet even though it is clearly negative for PayPal. I still think a large enough sample size gives a pretty good heuristic of the current sentiment of a company on Twitter.</p>

<p>Next week, I may look to fix this issue and improve this app, or go onto other projects.</p>]]></content><author><name></name></author><category term="HuggingFace," /><category term="transformers," /><category term="spaces," /><category term="part2" /><summary type="html"><![CDATA[Showing the app I made]]></summary></entry><entry><title type="html">Financial Phrases Classification (part 1)</title><link href="https://spknash.github.io/blog/2024/financial-phrases-pt1/" rel="alternate" type="text/html" title="Financial Phrases Classification (part 1)" /><published>2024-02-02T00:00:00+00:00</published><updated>2024-02-02T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2024/financial-phrases-pt1</id><content type="html" xml:base="https://spknash.github.io/blog/2024/financial-phrases-pt1/"><![CDATA[<p>In last week’s post I said one of the challenges I had in understanding the code for weak-to-strong models was my lack of familiarity with PyTorch and the HuggingFace Transformers library. To address this I went through the <a href="https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt">introductory course</a> that HuggingFace includes in it’s docs, and then I applied the Transformers library to a mini project which I’ll show here.</p>

<h2 id="huggingface-course-overview">HuggingFace Course Overview</h2>

<p>The mini course in the HuggingFace Docs are very easy to follow. There are currently 9 sections and each section covers a different part of the transformers library. That is, every section except the first one which gives an overview of what transformers are and how they work. This was pretty useful for me because even though I have used language models frequently in the last 1 year, I don’t have a very deep understanding of exactly how they work. I think within the next couple weeks I will do a detailed read of the <em>Attention is All You Need</em> paper which first introduced the transformer model.</p>

<p>They go over a brief history of Transformer models. The first was GPT in June of 2018, which was shortly followed by BERT, GPT-2, BART, and GPT-3 in May of 2020. They then go over the architecture of a transformer.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/financial-phrases/transformer-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/financial-phrases/transformer-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/financial-phrases/transformer-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/financial-phrases/transformer.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
credit: Google AI
    
</div>

<p>But only high-level concepts and to get a full understanding I’ll have to dig deeper. The two main components of a transformer are the encoder and the decoder. The encoder uses a full attention layer grasp all of the information contained in the input. The decoder uses a partial attention layer – only considering words which have already been outputted by the transformer. The authors of <em>Attention is All You Need</em> created the model in this way because this model was intially proposed for language translation. The idea was that by using both an encoder and a decoder, the model will be able to understand the full statement in the first language, and then be able to generate words in the translated language using the full encoder knowledge and the decoder knowledge of words generated so far.</p>

<p>Some models are encoder only(auto-encoding), some are decoder only(auto-regressive), and some are both encoder and decoder(sequence to sequence). Encoder only models are good for tasks which require full understanding of input because it uses a full attention mask. These models are good at things like sentiment analysis, Q&amp;A, and sentence classification. BERT is a popular example of a encoder-only model. Decoder only models on the other hand are good at tasks which work best when the model is only aware of previously generated words. These models are good at text-generation. Popular examples are GPT and GPT-2. Sequence to Sequence models work by using the encoder once on the input text and then use the decoder repeatedly to generate more and more words to be appended to the response. These models are good for text-generation and language translation. BART and T5 are examples of sequence to sequence models.</p>

<p>That pretty much covers the first section of the course. The remaining sections cover the specific classes and uses cases of the transformers library. Instead of going into detail about each one of these sections, I will describe the mini-project I did. This project touches all the sections of the course so it will give a good overview of what is possible with the transformers library.</p>

<h2 id="finetuning-on-financial-phrases">Finetuning on Financial Phrases</h2>

<p>The mini project is as follows: To finetune a model that outputs the sentiment of a financial phrase. After creating this model I will create a hugging face space where users can input a name of a company on twitter, and the app uses the twitter API to gather the last 1000 mentions of the company and gives a summary of the sentiments of the 1000 tweets. In this part 1 post I am only going over the finetuning part.</p>

<p>Sentiment analysis we aim this model to do is classify a sentence as 0-negative, 1-neutral, or 2-postiive. The first step is to choose a model to finetune. Based on the fact that encoder only models are suitable for understanding the entire input which is what sentiment analysis should do, I choose DiistilBert. This model is encoder only, and it is also much fewer parameters than BERT with similar performance so it can be trained and evaluated much faster. The next step is choosing a suitable dataset. <a href="https://huggingface.co/datasets/financial_phrasebank">This dataset</a> has just what we are looking for – there is a sentence column with the financial phrase and then a label column with the human labelled sentiment. This dataset is found on Hugging Face datasets</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/financial-phrases/dataset-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/financial-phrases/dataset-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/financial-phrases/dataset-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/financial-phrases/dataset.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>Before we begin we need the following packages:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Importing the libraries needed
</span><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">transformers</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">DistilBertModel</span><span class="p">,</span> <span class="n">DistilBertTokenizer</span><span class="p">,</span> <span class="n">DistilBertForSequenceClassification</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">DatasetDict</span><span class="p">,</span> <span class="n">Dataset</span>

</code></pre></div></div>

<p>To finetune the model we need to download the dataset and then modify it so that it can be trained. the first modification is creating a training split, validation split, and test split. In the original state there is only a train split.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">datasets</span>

<span class="n">raw_datasets</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">financial_phrasebank</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">sentences_50agree</span><span class="sh">'</span><span class="p">)</span>
<span class="n">data_splits</span> <span class="o">=</span> <span class="nc">DatasetDict</span><span class="p">({})</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][:</span><span class="mi">3876</span><span class="p">])</span>
<span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">valid_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">3876</span><span class="p">:</span><span class="mi">3876</span><span class="o">+</span><span class="mi">485</span><span class="p">])</span>
<span class="n">valid_df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">valid_df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">3876</span><span class="o">+</span><span class="mi">485</span><span class="p">:])</span>
<span class="n">test_df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">training_data</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">.</span><span class="nf">from_pandas</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][:</span><span class="mi">3876</span><span class="p">]))</span>
<span class="n">validation_data</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">.</span><span class="nf">from_pandas</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">3876</span><span class="p">:</span><span class="mi">3876</span><span class="o">+</span><span class="mi">485</span><span class="p">]))</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">.</span><span class="nf">from_pandas</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">3876</span><span class="o">+</span><span class="mi">485</span><span class="p">:]))</span>

<span class="n">data_splits</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">.</span><span class="nf">from_pandas</span><span class="p">(</span><span class="n">train_df</span><span class="p">)</span>
<span class="n">data_splits</span><span class="p">[</span><span class="sh">'</span><span class="s">valid</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">.</span><span class="nf">from_pandas</span><span class="p">(</span><span class="n">valid_df</span><span class="p">)</span>
<span class="n">data_splits</span><span class="p">[</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">.</span><span class="nf">from_pandas</span><span class="p">(</span><span class="n">test_df</span><span class="p">)</span>


</code></pre></div></div>

<p>This code creates a new DatasetDict called <code class="language-plaintext highlighter-rouge">data_splits</code> which has a different split for training, validation, and testing. I used 80% of the dataset for training and 10% for validation and testing. The validation split is useful for determining whether the model is overfitting or underfitting. It can also be used for evaluation. The test split is used for evaluation. After creating the different splits of the database, the next step is to pre-process the data so that it can be fed into the pretrained model. This primarily involves turning the sentences into sequences of numbers, aka tokenizing. To do this we use the <code class="language-plaintext highlighter-rouge">DistillBertTokenizer</code> module from the transformers library. Trying it out on a smaple sentence:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">distilbert-base-cased</span><span class="sh">'</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello, this is a example sentence</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Hi im Bob</span><span class="sh">"</span><span class="p">)</span>
<span class="n">inputs</span>
</code></pre></div></div>
<p>returns</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'input_ids': [101, 1188, 1110, 1103, 1148, 5650, 119, 102, 1188, 1110, 1103, 1248, 1141, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre></div></div>

<p>It returns an attention mask as well as the tokenized sentence – this just allows the model to know which tokens to pay attention to. Since BERT is an auto-encoding model it will pay attention to all of the input.</p>

<p>Another crucial pre-processing step is padding. When inputs are fed into the model during fin-tuning, they are loaded in batches. In order for the gpu to compute the new weights of the model efficiently, all of the inputs must have the same size. This allows the gpu to use its parallel computing capabilities. Below is how I did the padding:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">DataCollatorWithPadding</span>

<span class="n">data_collator</span> <span class="o">=</span> <span class="nc">DataCollatorWithPadding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

</code></pre></div></div>

<p>This defines a function data_collator which is passed a parameter to the PyTorch Trainer API. The last step of finetuning is training and evaluation. We can use the evaluate library to pass in a parameter into the Trainer API which tells the Trainer API about which metrics to keep track of. I defined a metric which keeps track of training loss and validation loss, and accuracy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">evaluate</span>

<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">)</span>
<span class="n">metric</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">preds</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">predictions</span><span class="p">.</span><span class="n">label_ids</span><span class="p">)</span>
</code></pre></div></div>

<p>For training I used the PyTorch training API:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">checkpoint</span> <span class="o">=</span> <span class="sh">'</span><span class="s">distilbert-base-cased</span><span class="sh">'</span>

<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_preds</span><span class="p">):</span>
    <span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_preds</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metric</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="nc">TrainingArguments</span><span class="p">(</span><span class="sh">"</span><span class="s">test-trainer</span><span class="sh">"</span><span class="p">,</span> <span class="n">evaluation_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="nc">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="sh">"</span><span class="s">valid</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<p>and the results were as follows:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/financial-phrases/results-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/financial-phrases/results-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/financial-phrases/results-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/financial-phrases/results.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>The final accuracy on the test split is 80% which is relatively good considering that the labels were also not unanimously – meaning the labels were voted on by a group of people and the dataset includes labels with &gt;60% consensus among the voters. The part that is concerning is that the validation loss goes up on the last epoch despite the training loss going down. This is a common sign of over-fitting. Over-fitting can be solved by increasing the dataset or by using methods such as regularization. Since I am getting 80% accuracy on the test split which is pretty good I will continue to making the app with this model and I may modify the model later if I find that over-fitting is a significant problem.</p>

<h2 id="next-steps">Next Steps</h2>

<p>This mini-project shows many of the core functionalities of the transformers library: using the datasets hub, using tokenizers, using pretrained model checkpoints, using the evluate library. Now that I have a finetuned model that is scoring 80% on the test split, I will use this model inference to make a app on HuggingFace spaces which uses this inference on the last 1000 mentions of a name to summarize the sentiment.</p>

<p>Later on I also hope to dig deeper into the details of transformers and tokenizers. Another thing I was very curious about was how exactly DistilBERT was created? I am interested how a model with similar performance but much fewer parameters was achieved.</p>]]></content><author><name></name></author><category term="HuggingFace," /><category term="transformers," /><category term="part1" /><summary type="html"><![CDATA[Overview of HuggingFace NLP course and applied to financial phrases dataset]]></summary></entry><entry><title type="html">Weak to Strong with LLaMas</title><link href="https://spknash.github.io/blog/2024/llama-generalization/" rel="alternate" type="text/html" title="Weak to Strong with LLaMas" /><published>2024-01-26T00:00:00+00:00</published><updated>2024-01-26T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2024/llama-generalization</id><content type="html" xml:base="https://spknash.github.io/blog/2024/llama-generalization/"><![CDATA[<h2 id="recap">Recap</h2>

<p>In the previous post I went into detail about this OpenAI paper. A full summary of the experiment and results are found in that post but here is a quick recap. Superhuman AI alignment is somewhat analogous to weak-to-strong generalization. This is because is superhuman alignment, an inferior intelligence(humans) aims to supervise a superior intelligence(the AI). Since superhuman AI does not exist yet, a close analogy is weak-to-strong generalization in which a weak AI aims to supervise a stronger AI. In the paper, the researchers discuss an experiment where they test weak-to-strong generalization where GPT-2 supervises GPT-4 on 3 different tasks(NLP benchmarks, chess puzzles, GPT reward modeling). One of the weaknesses of the experiment which the researchers noted is that the saliency of the tasks in the stronger model is not clearly known. For example, if GPT-4’s pretraining dataset included the tasks, this would artificially inflate the perceived effectiveness of weak-to-strong generalization.</p>

<p>For this reason my goal here was to repeat the experiment with llama v1 supervising llama v2 on a task which is not salient in the strong model. The idea here is that llama v1 and llama v2 are opensource so it should be possible to choose a task which is not salient in llama. In addition to choosing opensource models in which there is more knowledge about the pretraining dataset, simply choosing a more complicated task reduces the likelihood of pretraining leakage.</p>

<h2 id="openai-repo">OpenAI repo</h2>

<p>The OpenAI weak-to-strong repo has code similar to what is used in the experiment the paper covers. The repo however uses the models GPT-2 at varying sizes and some other language models like QWEN instead of GPT-2 and GPT-4 which were used in the experiment discussed in the paper. I didn’t really understand why the code repo is so different from the code which must have been used for the actual experiment in the paper. It may be because GPT-4 needs API use and it would be an expensive experiment for an average person to run if the strong model used was GPT-4.</p>

<p>The first thing I did was test out the code in the repo using GPT-2 small as the weak model and GPT-2 medium as the strong model. This runs GPT-2 small on ground truth and GPT-2 medium on ground truth as the two benchmarks, and then the GPT-2 medium finetuned on GPT-2 small labels as the weak-to-strong model. I ran this experiment to see what I get and I get the following results:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/weak-strong-llamas/sciq-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/weak-strong-llamas/sciq-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/weak-strong-llamas/sciq-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/weak-strong-llamas/sciq.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
GPT-2 small, and GPT-2 medium on ground truth, and GPT-2 medium on weak labels
    
</div>

<p>Based on these results, it is clear that a very small PGR is achieved by the</p>

<p>Despite looking very different, they are pretty similar to the expected results given by the repo authors:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/weak-strong-llamas/amazon_polarity-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/weak-strong-llamas/amazon_polarity-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/weak-strong-llamas/amazon_polarity-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/weak-strong-llamas/amazon_polarity.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
credit: OpenAI
    
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/weak-strong-llamas/anthropic_hh-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/weak-strong-llamas/anthropic_hh-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/weak-strong-llamas/anthropic_hh-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/weak-strong-llamas/anthropic_hh.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
credit: OpenAI
    
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/weak-strong-llamas/boolq-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/weak-strong-llamas/boolq-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/weak-strong-llamas/boolq-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/weak-strong-llamas/boolq.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
credit: OpenAI
    
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/weak-strong-llamas/sciq%20expected-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/weak-strong-llamas/sciq%20expected-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/weak-strong-llamas/sciq%20expected-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/weak-strong-llamas/sciq%20expected.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
credit: OpenAI
    
</div>

<p>These results show in all the datasets the weak-to-strong generalization typically achieves a PGR &gt;= 0.2 . With the highest PGR being 0.8 and lowest PGR being negative. However in both of these outliers the distance between ground truth weak and ground truth strong was very small. These PGRs are aligned with the papers findings the weak-to-strong generalization has some promise but the current PGRs acheived are not close to adequate fro superalignment or any alignment.</p>

<p>I forked this repo and used the same code to run my experiment using llama v1 as the weak model, llama v2 as the strong model. The task I choose was chess puzzles again because it doesn’t seem to be included in the llama pretraining datasets according to here. Additionally, of the three tasks used in the original experiment, chess is the most complicated and least likely to be leaked from the pretraining.</p>

<h2 id="weak-to-strong-generalization-with-llamas">Weak to strong Generalization with LLaMas</h2>

<p>Similarly to the GPT-2 small and GPT-2 medium experiment, In the LLaMa experiment I trained v1 on ground truth and v2 on ground truth as the benchmarks. I then trained v2 on the weakly generated labels by v1 to produce the weak-to-strong model. Below are the results of this experiment:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/weak-strong-llamas/sciq-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/weak-strong-llamas/sciq-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/weak-strong-llamas/sciq-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/weak-strong-llamas/sciq.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
LLaMa v1, and LLaMa v2 on ground truth, and LLaMa v2 on weak labels
    
</div>

<p>With the task being chess puzzles we can see that v1 and v2 both do much poorly compared to GPT-2 and GPT-4 repectively. The distance between v1’s ground truth training and v2’s ground truth training is 0.2 which is less than the difference between GPT-2 and GPT-4. The weak-to-strong model is 0.015 above v1’s ground truth. This leads to a PGR of around 0.07. This PGR is lower than the PGR for chess puzzles of the weakly supervised GPT-4 model. This difference could be caused by multiple factors:</p>

<ul>
  <li>The performance gap between GPT-2 and GPT-4 is greater than v1 vs v2. This allows a weakly supervised GPT-4 to make more gains</li>
  <li>There could be pretraining leakage of the task in GPT which the authors acknowledge.</li>
  <li>The researchers used larger models which have more latent capabilities and this could have allowed the weak GPT to elicit more capability from the strong GPT. The LLaMa v1 and v2 I used are 3.5B parameters and 7B parameters respectively and the llama.cpp version as well.</li>
</ul>

<p>The challenge with deep learning is that it is difficult to impossible to scientifically show which of these factors cause this difference and how much. The lack of scientific understanding surrounding weak-to-strong generalization is also something the paper addresses as future work. One of the goals was to see if the pretraining leakage disanalogy revealed itself in this experiment, but it is unclear at this point because numerous other factors could have caused a lower PGR in the LLaMa weak-to-strong model.</p>

<h2 id="challenges">Challenges</h2>

<p>One major challenge throughout this experiment was working with colab. Running the experiments took a very long time and there was a risk of the runtime being disconnected in the middle of the run which forces you to start over. Testing just GPT2 and GPT2-medium took 2 hours and testing LLaMa v1 and v2, and the weak-to-strong model took around 3 hours. I may consider upgrading my colab to save runtimes or to train models faster, esepcially as I continue to do more deepl learning. I may also look into other cloud computing platforms like lambda labs or Nvidia.</p>

<p>Another challenge was my lack of familiarity with PyTorch and the HuggingFace Transformers library. It was difficult to understand the code that produced the experiment in the OpenAI repository because it was all using PyTorch and Transformers. I was able to gain a good enough understanding to use it and modify it just by looking at the comments and my general programming experience. I have been going through the HuggingFace Transformers introductory mini course and I think my next post will be an application of what I learned through that. I am also going through the stable diffusion series on FastAI and that has given me some very interesting projects to do as well.</p>

<p>I was not able to run an experiment on the same level as the researchers or even the truncated experiment done in the repo due to limitations in compute and time access to GPUs imposed by Colab. I think as I continue deep learning projects I will need to find a way to solve this common problem.</p>]]></content><author><name></name></author><category term="llama," /><category term="colab," /><category term="llm," /><category term="openai" /><summary type="html"><![CDATA[Weak to Strong Generalization experiment with LLaMa v1 and v2]]></summary></entry><entry><title type="html">Weak to Strong Generalization</title><link href="https://spknash.github.io/blog/2024/weak-to-strong/" rel="alternate" type="text/html" title="Weak to Strong Generalization" /><published>2024-01-19T00:00:00+00:00</published><updated>2024-01-19T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2024/weak-to-strong</id><content type="html" xml:base="https://spknash.github.io/blog/2024/weak-to-strong/"><![CDATA[<h1 id="weak-to-strong-generalization-openai">Weak-to-Strong Generalization (OpenAI)</h1>

<p>OpenAI recently came out with a paper titled <em>Weak-to-Strong Generalization: Eliciting Strong Capabilities using Weak Supervision</em>. The paper tackles the broad problem of super human alignment – how will humans supervise superhuman artificial intelligence. Since superintelligent AI does not exist yet, this alignment problem can not be tackled directly. The authors choose to tackle an analogous problem – can a weak model supervise a strong model(both of whichare sub-human intelligence)? I found this paper to be very interesting. It shows that weak-to-strong generalisation is tractable based on the experiments they ran, however a significant percentage of strong capabilities are not elicitied using weak supervision.</p>

<p>In this post I’ll summarize methods used, results, and conclusions of the paper. Then I’ll look ahead to experiments that I am doing which were inspired by this paper.</p>

<h2 id="aligning-superintelligence">Aligning Superintelligence</h2>

<p>Aligning Superintelligence is one of the grand challenges of AI. Modern day sota models use human supervision for alignment. This is fine because humans are still stronger than the current models. However when superintelligent AI exists, they will be stronger than humans and humans can only weakly supervise these models. Superintelligent AI does not exist yet, so a close analogous situation is weak-to-strong generalization – a weak model supervising a strong model. The authors aim to show that weak-to-strong generalazation and take the first steps towards super alignment.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/weak_strong-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/weak_strong-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/weak_strong-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/weak_strong.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
credit: OpenAI
    
</div>

<h2 id="methodology">Methodology</h2>

<p>GPT-2 is used as the weak supervisor, and GPT-4 is used as the strong student in the experiments</p>

<p>The experiment the authors conducted is structured in the following way. A weak supervisor is finetuned on the ground truth, and this model makes predictions on test data(Call the score of the weak model WM). A strong student is trained with weak supervision, ie the strong model is finetuned on labels produced by the weak model. This model then makes predictions on the test data(Call the score of this weakly trained strong model WTS). Finally, as a benchmark, the strong model is trained on the ground truth and labels the test data(Call the score of this model SM). The effectiveness of the weak-to-strong generalization is measured by the Performance Gap Recovered(PGR) which is the percent of the performance gap between the weak model and strong model covered by the weak to strong model. So the PGR = (WTS-WM)/(SM-WM). In other words if the PGR is 0% the weak to strong generalization was completely innefective and the weakly supervised strong model performed the same as the weak model. If PGR is 100% then it was very effective and the weak to strong generalization is just as good as the strong model.</p>

<p>Some advantages of this setup include:</p>

<ul>
  <li>Any weak and strong models acn be used in this experiment, as long as there are task and measurement methods to assess performance on tasks.</li>
  <li>Similarly, any set of tasks can be used</li>
  <li>The authors say success in weak-to-strong generalization will help with alignment of current models.</li>
</ul>

<p>There are also some limits of this setup. Specifically, there are disanalogies to the problem of humans supervising super intelligent AI.</p>

<ul>
  <li>The first is Imitation Saliency: The errors weak models make may be differenct than the errors humans make.</li>
  <li>The second is Pretraining Leakage: The pretraining data in the strong models used in these experiments contains supervision from humans. So in the experiment, the weakly trained strong models may be artificially producing the correct answer in some cases because of their human supervised pretraining. Howerver, super-intelligence may not be created using human supervision, eg self-supervised training. This would make it harder to elicit from super human models.</li>
</ul>

<h2 id="results">Results</h2>

<p>The tasks used in the experiment are the following:</p>

<ul>
  <li>Popular NLP benchmarks: all datasets converted to binary classification, these tasks include sentiment analysis, natural language inference and more</li>
  <li>Chess Puzzles: data set of puzzles which is a position on the chess board and the label is the best next move. This is the only generative task used in this experiment</li>
  <li>GPT reward modeling: a binary dataset of pairs of gpt responses and the label is the better completion result</li>
</ul>

<p>First the authors did plain weak-to-strong generalization with no additional mathods used and tested to get a benchmark. On the NLP benchmarks test, the PGR was between 20% and 50% with the PGR increasing as the model size of the strong model increases. On the chess puzzles task, the PGR is close to 0% for small weak model size and it increases to 40% for the largest weak model size. The PGR does not increase with an increase of strong model size. On the GPT reward modeling task, the PGR is always quite low never crossing 25% PGR. On all these tasks the PGR was greater than 0% but still very far from the ceiling set by the strong model trained on the ground truth. The highest PGR in any task was around 50%. These results show that weak-to-strong generalization is tractable but many improvements need to be made before it is a viable alignment technique.</p>

<p>The authors then show results from experiments using weak-to-strong generalization but with different improvements. The first is bootstrapping.</p>

<p>Bootstrapping is a method that has been talked about for super alignment before. The idea is to train a model slightly stronger than the weak model, then use this model to train a slightly stronger model than itself, and so on until you train the strong model you are trying to align. In this experiment, the authors used two intermediary models(I1 and I2) between GPT2 and GPT4. So GPT-2 trained I1 which trained I2 which trained GPT-4. Bootstrapping showed a significant improvement in the chess puzzles task, the largest improvement being around a 25% increase in the PGR. The other tasks, however, did not show much improvement.</p>

<p>The second method which caused gains for weak-to-strong supervision was the auxillary confidence loss term. One of the main things causing errors in the weak-to-strong training is the strong model will sometimes imitate the mistakes of the weak supervisor. Giving the strong model the ability to disagree with the weak models label using its pretraining knowledge would help improve the weak-to-strong generalization. That is what adding the auxillary confidence loss term to standard cross entropy does. During training, if the strong model disagrees with the weak models labels, this label will not affec the model weights very much. This method dramatically improves PGR expecially in large gaps between the size of weak and strong models.</p>

<h2 id="understanding-weak-to-strong-generalization">Understanding Weak-to-Strong Generalization</h2>

<p>The authors identify two major phenomena related to weak-to-strong generalization: Imitation of weak supervisor mistakes, and saliency of tasks in the strong model.</p>

<h3 id="weak-supervisor-imitation">Weak Supervisor Imitation</h3>

<p>The strong model repeating the weak models mistakes reduces the effectiveness of weak to strong generalization. One cause of mistake imitation is the strong model overfitting with the weak model. This can be reduced by using techniques commonly used to fix over-fitting like regularization or early stopping of training. Another reason for imitation is high supervisor-student agreement. From the experiment they saw that introducing confidence loss significantly decreased model agreement and the PGR increased as well.</p>

<h3 id="saliency-of-strong-model-representations">Saliency of Strong model representations</h3>

<p>Some of the PGR acheived by the strong model could be due to the inherent saliency of the tasks in the strong model. For example, if GPT-4 was trained using data similar to the popular NLP tasks it may be able to acheive better results than the weak model without any finetuning. The authors tested this by seeing how accurate the strong models were with zero-shot or few shot prompting. For smaller strong models the PGR acheived was minimal but for larger students the PGR was significantly higher and comparable to results of the weakly supervised model. However, weakly supervised models with confidence loss generally outperforms zero-shot and few-shot prompting even for large models.</p>

<p>If salient representations of the task are ineherent in the pretrained strong model, then weak-to-strong generalization could be improved by using unsupervised finetuning in order to bring out the salient abilities of the strong model. Unsupervised finetuning improves the PGR of the GPT reward modeling task from  less than 20% to 30-40%.</p>

<p>Finally, the authors discuss the possibility of using linear probing to improve weak-to-strong. They found that weak supervision increases the linearity of the labels so finetuning using weak labels and then linear probing increases the PGR of the weak-to-strong generalization.</p>

<h2 id="discussion">Discussion</h2>

<p>The first potential avenue for future work discussed is changes to the analogous setup. As mentioned before, there are two main disanalogies with this setup:</p>

<ul>
  <li>Imitation Saliency</li>
  <li>Pretraining Leakage</li>
</ul>

<p>The authors mention the following ways to improve the setup:</p>
<ul>
  <li>fixing disanalogies</li>
  <li>showing that the disanalogies are not severe</li>
  <li>generalizing tasks to more complicated tasks</li>
</ul>

<p>The next avenue future work is scalable methods for weak-to-strong generalization. In a desired weak-to-strong generalization:</p>

<ul>
  <li>the strong model should disagree with weak models errors</li>
  <li>the generalization should be salient to the strong model</li>
  <li>The gneralization should be consistent.</li>
</ul>

<p>The authors also believe there is a lot of potential for gains to be made in weak-to-strong generalization by using more ML techniques like confidence loss which dramatically improved PGR.</p>

<p>Finally, the authors say that super human alignment is a critical problem, and if we are to use super human AI for important things there should be a clear scientific understanding of when and why the alignment works. They mention the following questions among others:</p>

<ul>
  <li>Why is there a significant difference between PGR obtained for different tasks?</li>
  <li>What makes a task or concept easy or hard to elicit?</li>
  <li>How much to errors in weak labels affect the strong models behavior?</li>
</ul>

<h2 id="what-i-am-working-on">What I am working on</h2>

<p>The authors mention that the saliency of the strong model representation for the tasks used is not clearly known and could have effected the results. Specifically, there may be overlap between the GPT-4 pretraining data and the popular NLP tasks data which was used in the experiment. I plan to recreate the experiment but instead of using GPT-2 and GPT-4 as the weak and strong models I will use 2 opensource models llama v1 and llama v2. And I will test on data that is disjoint from the pretraining data of these models. The idea behind this is to choose a task which the strong model does not have pretraining data on, to show I will share all details and code in my next post.</p>]]></content><author><name></name></author><category term="AI," /><category term="Alignment," /><category term="openai" /><summary type="html"><![CDATA[Review of new paper on super aligment and weak to strong generalization]]></summary></entry><entry><title type="html">Echo Server cpp</title><link href="https://spknash.github.io/blog/2023/echo-server-cpp/" rel="alternate" type="text/html" title="Echo Server cpp" /><published>2023-09-09T00:00:00+00:00</published><updated>2023-09-09T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2023/echo-server-cpp</id><content type="html" xml:base="https://spknash.github.io/blog/2023/echo-server-cpp/"><![CDATA[<h1 id="intro-socket-programming-in-c">Intro Socket Programming in C</h1>

<p>Hello! It has been a while since my last post. I have been busy with job applications and switching jobs the last few weeks and haven’t set aside enough time for this. From now on I’ll make sure to post every week I’ll say Monday even if what I am working on is not in any kind of finished state I’ll just put my progress or talk about something completely different. Like a filler episode in anime.</p>

<p>This week I am getting started with C++ and neworking/socket programming. I want to learn C++ because I said on my resume and some job applications that I know it and I want to also improve my networking/socket programming skills. The end goal is to create a something more interesting like a packet sniffer (like libpcap) or a blockchain network but since I am just getting started this week I am getting used to socket programming in C++ and implementing an echo server and client</p>

<h2 id="echo-server">Echo Server</h2>

<p>One guide that was very helpful with getting familiar with socket programming in C was <a href="https://beej.us/guide/bgnet/html/split/slightly-advanced-techniques.html#blocking">Beej’s guide</a>. I knew the overall structure for a echo server. I would need:</p>
<ol>
  <li>Accept connection from client socket</li>
  <li>Have multi-threading capability to allow the server to handle multiple clients simultaneously</li>
  <li>Ability to read message from client and send message back</li>
  <li>Client side should have some kind of command line program to allow client to type message and receive echo</li>
</ol>

<p>Some parts of this are in the server side and some parts are on the client side. I will start by going throught the server side.</p>

<h2 id="server">Server</h2>
<p>First some variables I initialized at the beginning of the program that I will use throughout server.cpp:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">status</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">sockfd</span><span class="p">,</span> <span class="n">new_fd</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">port_num</span><span class="p">;</span>
<span class="k">struct</span> <span class="n">sigaction</span> <span class="n">sa</span><span class="p">;</span>
<span class="n">socklen_t</span> <span class="n">addr_size</span><span class="p">;</span>
<span class="k">struct</span> <span class="n">sockaddr_storage</span> <span class="n">their_addr</span><span class="p">;</span>
<span class="k">struct</span> <span class="n">addrinfo</span> <span class="n">hints</span><span class="p">;</span>
<span class="k">struct</span> <span class="n">addrinfo</span> <span class="o">*</span><span class="n">res</span><span class="p">,</span> <span class="o">*</span><span class="n">p</span><span class="p">;</span>
<span class="n">socklen_t</span> <span class="n">sin_size</span><span class="p">;</span>
<span class="kt">char</span> <span class="n">s</span><span class="p">[</span><span class="n">INET6_ADDRSTRLEN</span><span class="p">];</span>
<span class="kt">char</span> <span class="n">ipstr</span><span class="p">[</span><span class="n">INET6_ADDRSTRLEN</span><span class="p">];</span>
<span class="n">memset</span><span class="p">(</span><span class="o">&amp;</span><span class="n">hints</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="k">sizeof</span> <span class="n">hints</span><span class="p">);</span>
<span class="n">hints</span><span class="p">.</span><span class="n">ai_family</span> <span class="o">=</span> <span class="n">AF_UNSPEC</span><span class="p">;</span> <span class="c1">// AF_INET or AF_INET6 to force version</span>
<span class="n">hints</span><span class="p">.</span><span class="n">ai_socktype</span> <span class="o">=</span> <span class="n">SOCK_STREAM</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">yes</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</code></pre></div></div>

<p>The first step is to get the address info of the server and that is done with the function ‘getaddrinfo’:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">((</span><span class="n">status</span> <span class="o">=</span> <span class="n">getaddrinfo</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span> <span class="s">"3490"</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">hints</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">res</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">){</span>
        <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span> <span class="s">"getaddrinfo: %s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">gai_strerror</span><span class="p">(</span><span class="n">status</span><span class="p">));</span>
        <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
    <span class="p">};</span>
</code></pre></div></div>

<p>This populates <code class="language-plaintext highlighter-rouge">res</code> with a linked list of potential addresses that a client can bind to in the server. The next step is to iterate through the possible addresses and initialize a socket file descriptor using one of the valid addresses.</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span> <span class="n">p</span><span class="o">!=</span> <span class="nb">NULL</span><span class="p">;</span> <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_next</span><span class="p">){</span>
        <span class="k">if</span><span class="p">((</span><span class="n">sockfd</span> <span class="o">=</span> <span class="n">socket</span><span class="p">(</span><span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_family</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_socktype</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_protocol</span><span class="p">))</span><span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">){</span>
            <span class="n">perror</span><span class="p">(</span><span class="s">"server: socket"</span><span class="p">);</span>
            <span class="k">continue</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">setsockopt</span><span class="p">(</span><span class="n">sockfd</span><span class="p">,</span> <span class="n">SOL_SOCKET</span><span class="p">,</span> <span class="n">SO_REUSEADDR</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">yes</span><span class="p">,</span>
                <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">))</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">perror</span><span class="p">(</span><span class="s">"setsockopt"</span><span class="p">);</span>
            <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">bind</span><span class="p">(</span><span class="n">sockfd</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_addr</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_addrlen</span><span class="p">)</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">close</span><span class="p">(</span><span class="n">sockfd</span><span class="p">);</span>
            <span class="n">perror</span><span class="p">(</span><span class="s">"server: bind"</span><span class="p">);</span>
            <span class="k">continue</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="k">break</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div>

<p>The socket fild descriptor(<code class="language-plaintext highlighter-rouge">sockfd</code>) will be set to the first address that is valid and if there are no valid addresses the program will exit:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">freeaddrinfo</span><span class="p">(</span><span class="n">res</span><span class="p">);</span>
    <span class="k">if</span><span class="p">(</span><span class="n">p</span><span class="o">==</span><span class="nb">NULL</span><span class="p">){</span>
        <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span> <span class="s">"Failed to bind</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
</code></pre></div></div>

<p>The next step is for the socket to listen to incoming clients trying to connec to the server which is done through the <code class="language-plaintext highlighter-rouge">listen()</code> function:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span><span class="p">(</span><span class="n">listen</span><span class="p">(</span><span class="n">sockfd</span><span class="p">,</span> <span class="n">BACKLOG</span><span class="p">)</span><span class="o">==-</span><span class="mi">1</span><span class="p">){</span>
        <span class="n">perror</span><span class="p">(</span><span class="s">"listen"</span><span class="p">);</span>
        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">};</span>
    <span class="c1">// waiting for connection now</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"server waiting for connections ... </span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
</code></pre></div></div>

<p>This code concludes the setup of the server; it now is able to listen to incoming clients. Now the server must accept clients and send echos to each client simultaneously as long as the server is active. Since the following functionality happens for the duration of the lifetime of the server, the remaining code is within a while loop: <code class="language-plaintext highlighter-rouge">while(1) </code></p>

<p>The first step is to accept clients that connec to the server:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sin_size</span> <span class="o">=</span> <span class="k">sizeof</span> <span class="n">their_addr</span><span class="p">;</span>
<span class="n">new_fd</span> <span class="o">=</span> <span class="n">accept</span><span class="p">(</span><span class="n">sockfd</span><span class="p">,</span> <span class="p">(</span><span class="k">struct</span> <span class="n">sockaddr</span> <span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">their_addr</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">sin_size</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">new_fd</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">perror</span><span class="p">(</span><span class="s">"accept"</span><span class="p">);</span>
    <span class="k">continue</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The return value of the <code class="language-plaintext highlighter-rouge">accept</code> function is the filedescriptor of the client socket and can be used as a parameter in <code class="language-plaintext highlighter-rouge">send</code> and <code class="language-plaintext highlighter-rouge">recv</code> to send and receive data from the client socket. After the client is connected to the server, the server must spawn a child process to handle this client simultaneously with the parent process as well as handling requests from other clients. This is where we must use <code class="language-plaintext highlighter-rouge">fork()</code> to do this. I also send a message to the client explaining what this particular server does:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">!</span><span class="n">fork</span><span class="p">()</span> <span class="c1">// this is the child process</span>
<span class="n">close</span><span class="p">(</span><span class="n">sockfd</span><span class="p">);</span> <span class="c1">// child doesn't need the listener</span>
<span class="k">if</span> <span class="p">(</span><span class="n">send</span><span class="p">(</span><span class="n">new_fd</span><span class="p">,</span> <span class="s">"Hello! welcome to echo server. Type something and I'll type the exact same thing back. How exciting!!"</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">perror</span><span class="p">(</span><span class="s">"send"</span><span class="p">);</span>
</code></pre></div></div>

<p>Now that the client is connected and running in a child process, the server must do what it is intended to do: echo messages from the client. So whenever the server socket receives data from this client it will send a message with the same data back to the client.</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">while</span> <span class="p">(</span><span class="mi">1</span><span class="p">){</span>
    <span class="c1">//printf("trying to recv from client");</span>
    <span class="kt">int</span> <span class="n">numbytes</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="kt">char</span> <span class="o">*</span><span class="n">buf</span><span class="p">[</span><span class="mi">256</span><span class="p">];</span>
    <span class="n">numbytes</span> <span class="o">=</span> <span class="n">recv</span><span class="p">(</span><span class="n">new_fd</span><span class="p">,</span> <span class="n">buf</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">numbytes</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">){</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"sever: received '%s'</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">buf</span><span class="p">);</span>
    <span class="p">}</span>
    
    <span class="k">if</span> <span class="p">(</span><span class="n">numbytes</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">){</span>
        <span class="kt">int</span> <span class="n">bytes_sent</span><span class="p">;</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">bytes_sent</span> <span class="o">=</span> <span class="n">send</span><span class="p">(</span><span class="n">new_fd</span><span class="p">,</span> <span class="n">buf</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">perror</span><span class="p">(</span><span class="s">"send"</span><span class="p">);</span>
            <span class="n">printf</span><span class="p">(</span><span class="s">"exiting"</span><span class="p">);</span>
            <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This concludes the logic needed on the server side. In summary, we get the address info of the server, initialize a socket file descriptor using the address, bind the socket to a specific port, listen to incoming connection requests form clients, accept connection from client, start a child process for client and send echo messages. Now we can move on to the client side logic which is very similar.</p>
<h2 id="client">Client</h2>

<p>The client side will be similar to the server side but instead of listening for requests, now we are making a connection request and we need to have functionality allowing the user to connect to a particular server and make echo requests to the server. The plan is to have the client executable run like this: <code class="language-plaintext highlighter-rouge">./client ***server name***</code> and once connected to the server the user should be able to type a message and receive the echo.</p>

<p>First some variable intialized at the beginning of the script that will be used throughout client.cpp:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">status</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">sockfd</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">numbytes</span><span class="p">;</span>

<span class="kt">char</span> <span class="o">*</span><span class="n">buf</span><span class="p">[</span><span class="mi">100</span><span class="p">];</span>

<span class="k">struct</span> <span class="n">addrinfo</span> <span class="n">hints</span><span class="p">;</span>
<span class="k">struct</span> <span class="n">addrinfo</span> <span class="o">*</span><span class="n">res</span><span class="p">,</span> <span class="o">*</span><span class="n">p</span><span class="p">;</span>
<span class="n">socklen_t</span> <span class="n">sin_size</span><span class="p">;</span>

<span class="kt">char</span> <span class="n">s</span><span class="p">[</span><span class="n">INET6_ADDRSTRLEN</span><span class="p">];</span>
</code></pre></div></div>

<p>Make sure that the command line arguments are coming in correctly:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"usage: client hostname</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Now we get the address info of the server using the command line argument:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// getting addrinfo and binding to specific port</span>
<span class="k">if</span> <span class="p">((</span><span class="n">status</span> <span class="o">=</span> <span class="n">getaddrinfo</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s">"3490"</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">hints</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">res</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">){</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span> <span class="s">"getaddrinfo: %s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">gai_strerror</span><span class="p">(</span><span class="n">status</span><span class="p">));</span>
    <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">};</span>
</code></pre></div></div>

<p>The result is stored a linked list in res just as before. Just as before we iterate through the possible addresses and create a sock file descriptor using the first valid one and make a connection request:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span> <span class="n">p</span><span class="o">!=</span> <span class="nb">NULL</span><span class="p">;</span> <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_next</span><span class="p">){</span>
    <span class="k">if</span><span class="p">((</span><span class="n">sockfd</span> <span class="o">=</span> <span class="n">socket</span><span class="p">(</span><span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_family</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_socktype</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_protocol</span><span class="p">))</span><span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">){</span>
        <span class="n">perror</span><span class="p">(</span><span class="s">"client: socket"</span><span class="p">);</span>
        <span class="k">continue</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">connect</span><span class="p">(</span><span class="n">sockfd</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_addr</span><span class="p">,</span> <span class="n">p</span><span class="o">-&gt;</span><span class="n">ai_addrlen</span><span class="p">)</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">close</span><span class="p">(</span><span class="n">sockfd</span><span class="p">);</span>
        <span class="n">perror</span><span class="p">(</span><span class="s">"client: connect"</span><span class="p">);</span>
        <span class="k">continue</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">break</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>After the client is connected to the socket there will be a loop of the client receiving the message from the server then getting a prompt to make another echo request. If another echo request is made then another interation fo the loop happens. We will have a way for the client to break out of the loop and end the program if they type <code class="language-plaintext highlighter-rouge">exit</code> when they are prompted with another echo request.</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">while</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>

    <span class="k">if</span> <span class="p">((</span><span class="n">numbytes</span> <span class="o">=</span> <span class="n">recv</span><span class="p">(</span><span class="n">sockfd</span><span class="p">,</span> <span class="n">buf</span><span class="p">,</span> <span class="n">MAXDATASIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">perror</span><span class="p">(</span><span class="s">"recv"</span><span class="p">);</span>
        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">buf</span><span class="p">[</span><span class="n">numbytes</span><span class="p">]</span> <span class="o">=</span> <span class="sc">'\0'</span><span class="p">;</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">"client: received '%s' from server</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">buf</span><span class="p">);</span>
    
    <span class="kt">char</span> <span class="n">buffer</span><span class="p">[</span><span class="mi">256</span><span class="p">];</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">"Enter a message: "</span><span class="p">);</span>
    
    <span class="c1">// Read a line of input from the user</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">fgets</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">buffer</span><span class="p">),</span> <span class="n">stdin</span><span class="p">)</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"Error reading input.</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
        <span class="k">continue</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Remove the trailing newline character, if any</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">buffer</span><span class="p">[</span><span class="n">strlen</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="sc">'\n'</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">buffer</span><span class="p">[</span><span class="n">strlen</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="sc">'\0'</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Exit the loop if the user types "exit"</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">strcmp</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="s">"exit"</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">break</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="c1">//printf("%s\n", buffer);</span>

    <span class="c1">// send the message</span>
    <span class="kt">int</span> <span class="n">bytes_sent</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">bytes_sent</span> <span class="o">=</span> <span class="n">send</span><span class="p">(</span><span class="n">sockfd</span><span class="p">,</span> <span class="n">buffer</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">perror</span><span class="p">(</span><span class="s">"send"</span><span class="p">);</span>
        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This concludes the client side code.</p>

<p>Here is a quick demo of how it works</p>

<p>After <code class="language-plaintext highlighter-rouge">./server</code> is run we get this:</p>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/echo1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/echo1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/echo1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/echo1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>After the client connects to the server we see this on the server side:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/echo2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/echo2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/echo2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/echo2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>This is what the client sees when they connect to the client and enters a message to the server:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/echo3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/echo3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/echo3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/echo3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>And this is what is seen on the server side after the client sends a message to the server:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/echo4-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/echo4-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/echo4-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/echo4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>To conclude – this was just a very short introduction into socket programming in C/C++ and I plan to go deeper into network programming. I feel I got a good understanding for setting up a server client connection and plan to do a more complicated network program next time like a network packet sniffer, or simple blockchain network. I’ll be back here next week!</p>]]></content><author><name></name></author><category term="networking," /><category term="C++" /><summary type="html"><![CDATA[Intro to socket programming in C++]]></summary></entry><entry><title type="html">Webex llama bot</title><link href="https://spknash.github.io/blog/2023/Webex-llama-bot/" rel="alternate" type="text/html" title="Webex llama bot" /><published>2023-08-11T00:00:00+00:00</published><updated>2023-08-11T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2023/Webex-llama-bot</id><content type="html" xml:base="https://spknash.github.io/blog/2023/Webex-llama-bot/"><![CDATA[<h1 id="webex-chatbot-to-increase-office-interaction">Webex chatbot to increase office interaction</h1>

<p>This project started off as the project my team did for the intern hackathon at my summer internship. The topic of the hackathon was build something that improves remote and hybrid work. Our team decided that one of the biggest things lost during remote work is in person interaction and social interaction. It is very hard to become friends with someone when you don’t see them in person. Even harder to become friends when the only topic of conversation between you and them is a specific work task within the context of a work meeting. In person there are a lot more casual interactions in the hallway and especially during lunch that allow co-workers to become much closer to each other.</p>

<p>Our idea was to create a application which pairs two random employees in the company for a 30 minute chat each week. I won’t talk about the implementation of the full application but only about what I did. I worked on all parts related to Webex. We needed to use the Webex API to automatically schedule meetings between two people in the org. We also decided to incorporate a webex chatbot which could do things like provide background information about each participant in the meeting and also do things like offer a potential fun meeting topic if the participants don’t know what to talk about. As I was making the chatbot I realized its not a very good chatbot if it only responds to certain commands. Like below:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bot-help-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bot-help-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bot-help-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/bot-help.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>I wanted to use a Large language model to allow the bot to respond to messages more dynamically. The only issue is I don’t want to spend any money on this because this a purely for fun project and many llms require payment to either use their apis or require the use of cloud credits to host on a cloud platform. I arrived at an idea based on something I used to build the webex chatbot.</p>

<h2 id="webex-chatbot">Webex chatbot</h2>

<p>The webex chatbot was made using this template: <a href="https://github.com/WebexSamples/webex-bot-starter">github link</a>. I cloned this repo to my local and began to make changes. In order for the bot to work it needs to be able to communicate over http, so I used ngrok to expose the bot application to the internet.
###What is ngrok and how does it work?
When developing a web application, you typically run a local server that is only accessible on your personal computer. However, there might be instances where you need to share this local server with someone else on the internet, perhaps for testing, demonstrations, or external integrations like webhooks.</p>

<p>This is where ngrok comes in. By running a simple command in your command line interface, ngrok provides you with a public URL (HTTP/HTTPS) that forwards incoming requests to your local server. This URL can be shared with anyone, and they’ll be able to access your locally running application as if it were hosted online.</p>

<h3 id="general-use-cases">General Use Cases:</h3>
<p>Testing on Different Devices: If you want to test how your application appears on different devices, ngrok makes it easy by allowing those devices to access your local server through the public URL.</p>

<p>Collaborating with Team Members: If you’re working with team members who are not on the same local network, you can use ngrok to give them access to your development environment.</p>

<p>Webhook Development and Testing: Many third-party services use webhooks to communicate with your application. Ngrok allows these services to connect with your local server, simplifying the process of developing and testing webhooks.</p>

<p>Sharing a Demo with Clients: If you want to share a live demo of an application that’s still in development, ngrok enables you to do so without having to deploy it to a public server.</p>

<p>Temporary Hosting for Hackathons or Prototyping: Quick prototyping or participation in hackathons often requires temporary public access to your local development. Ngrok provides a swift solution for these scenarios.</p>

<p>In conclusion, ngrok is an essential tool for modern web development, offering a convenient way to share your local development environment with others. Its ease of use and wide range of applications make it an indispensable resource for developers seeking to streamline their workflow and collaboration efforts.</p>

<h2 id="trying-to-use-llamacpp">Trying to use llama.cpp</h2>
<p>So my webex bot at this point works well with limited structured commands but I wanted the catch-all case to give a best response using a large language model instead of not being able to handle a novel response. The first LLM I thought of using was llama because it seems to be the best performing open source langauge model at the moment but also because of llama.cpp which is a implementation of llama in C which is compact enough thata it can be run on Mac M1 or M2 chips. I forked llama.cpp and tried using it on local but the speed at which the response was very slow and it is also probably not great to run something so intensive on my Mac even if it was just for a hackathon/demo.</p>

<p>The next idea was to use llama.cpp on colab – and use ngrok to expose llama to the internet so it can be used essentially as an inference api for any application I want to build.</p>

<p>##Using llama.cpp and ngrok on Colab</p>

<p>Now I can use llama.cpp to create an app on colab and expose it to the internet to my webex bot can use it as a inference API to generate responses. To use llama.cpp on colab I need to use llama-cpp-python which is python bindings for llama.cpp. I also use langchains to do some minor prompt templating.</p>

<p>The imports:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.llms</span> <span class="kn">import</span> <span class="n">LlamaCpp</span>
<span class="kn">from</span> <span class="n">langchain</span> <span class="kn">import</span> <span class="n">PromptTemplate</span><span class="p">,</span> <span class="n">LLMChain</span>
<span class="kn">from</span> <span class="n">langchain.callbacks.manager</span> <span class="kn">import</span> <span class="n">CallbackManager</span>
<span class="kn">from</span> <span class="n">langchain.callbacks.streaming_stdout</span> <span class="kn">import</span> <span class="n">StreamingStdOutCallbackHandler</span>
</code></pre></div></div>

<p>The simple prompt template using langchains:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">{question}

Answer: Let</span><span class="sh">'</span><span class="s">s work this out in a step by step way to be sure we have the right answer.</span><span class="sh">"""</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">template</span><span class="p">,</span> <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div>

<p>This command declares a callback manager which allows the llm to produce tokens one by one chat-gpt style.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">callback_manager</span> <span class="o">=</span> <span class="nc">CallbackManager</span><span class="p">([</span><span class="nc">StreamingStdOutCallbackHandler</span><span class="p">()])</span>
</code></pre></div></div>

<p>This command gets the ggml file needed to use llama.cpp</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="p">.</span><span class="n">co</span><span class="o">/</span><span class="n">TheBloke</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">13</span><span class="n">B</span><span class="o">-</span><span class="n">chat</span><span class="o">-</span><span class="n">GGML</span><span class="o">/</span><span class="n">resolve</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">13</span><span class="n">b</span><span class="o">-</span><span class="n">chat</span><span class="p">.</span><span class="n">ggmlv3</span><span class="p">.</span><span class="n">q4_0</span><span class="p">.</span><span class="nb">bin</span>
</code></pre></div></div>

<p>This next block of code sets the model parameters and creates the inference function: llm</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_gpu_layers</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Change this value based on your model and your GPU VRAM pool.
</span><span class="n">n_batch</span> <span class="o">=</span> <span class="mi">512</span>  <span class="c1"># Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
</span>
<span class="c1"># Make sure the model path is correct for your system!
</span><span class="n">llm</span> <span class="o">=</span> <span class="nc">LlamaCpp</span><span class="p">(</span>
    <span class="n">model_path</span><span class="o">=</span><span class="sh">"</span><span class="s">llama-2-13b-chat.ggmlv3.q4_0.bin</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">n_gpu_layers</span><span class="o">=</span><span class="n">n_gpu_layers</span><span class="p">,</span>
    <span class="n">n_batch</span><span class="o">=</span><span class="n">n_batch</span><span class="p">,</span>
    <span class="n">callback_manager</span><span class="o">=</span><span class="n">callback_manager</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Now that we have the inference function we can create a app and expose to the internet using ngrok. First I test llm to see how fast it is.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prompt</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
Are there hills in Peru?
</span><span class="sh">"""</span>
<span class="nf">llm</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Yes, Peru is home to a variety of landscapes including hills, mountains,
 plains, and coastal areas. The Andes Mountains run through Peru, giving rise
 to many high peaks and rolling hills. These geographical features create
 diverse ecosystems throughout the country, from the high-altitude grasslands
 of the Andean Plateau to the arid coastal plains.
</code></pre></div></div>

<p>It runs much faster than on local and it is a necesary improvement because it was way too slow on local. Now onto the app.  Initially I had trouble using ngrok, this was because I hadn’t authenticated my ngrok token and to do that I needed to run these commands:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="nb">bin</span><span class="p">.</span><span class="n">equinox</span><span class="p">.</span><span class="n">io</span><span class="o">/</span><span class="n">c</span><span class="o">/</span><span class="mi">4</span><span class="n">VmDzA7iaHb</span><span class="o">/</span><span class="n">ngrok</span><span class="o">-</span><span class="n">stable</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">amd64</span><span class="p">.</span><span class="nb">zip</span>
<span class="err">!</span><span class="n">unzip</span> <span class="n">ngrok</span><span class="o">-</span><span class="n">stable</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">amd64</span><span class="p">.</span><span class="nb">zip</span>
<span class="err">!</span><span class="p">.</span><span class="o">/</span><span class="n">ngrok</span> <span class="n">authtoken</span> <span class="o">*</span><span class="n">my</span><span class="o">-</span><span class="n">token</span><span class="o">*</span>
</code></pre></div></div>

<p>I made a flask app and which on a POST request to /generate, retrieves the prompt and generates the response, and packages the response in json and sends it back.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">flask</span> <span class="kn">import</span> <span class="n">Flask</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">jsonify</span>
<span class="kn">from</span> <span class="n">flask_ngrok</span> <span class="kn">import</span> <span class="n">run_with_ngrok</span>
<span class="kn">from</span> <span class="n">flask_cors</span> <span class="kn">import</span> <span class="n">CORS</span>

<span class="n">app</span> <span class="o">=</span> <span class="nc">Flask</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>
<span class="nc">CORS</span><span class="p">(</span><span class="n">app</span><span class="p">)</span>  <span class="c1"># Enable CORS for all routes
</span><span class="nf">run_with_ngrok</span><span class="p">(</span><span class="n">app</span><span class="p">)</span>  <span class="c1"># Start ngrok when app is run
</span>
<span class="c1"># Assuming llm is already defined and loaded elsewhere in your code
# and you can get a response by calling llm(prompt)
</span><span class="nd">@app.route</span><span class="p">(</span><span class="sh">'</span><span class="s">/</span><span class="sh">'</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">home</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">home page</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="sh">'</span><span class="s">Hello, World!</span><span class="sh">'</span>
<span class="nd">@app.route</span><span class="p">(</span><span class="sh">'</span><span class="s">/generate</span><span class="sh">'</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">POST</span><span class="sh">'</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">generate</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">generating response ...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="n">json</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">prompt</span><span class="sh">'</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Received prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># Print the received prompt
</span>    <span class="n">response</span> <span class="o">=</span> <span class="nf">llm</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Generated response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># Print the generated response
</span>
    <span class="k">return</span> <span class="nf">jsonify</span><span class="p">({</span><span class="sh">'</span><span class="s">response</span><span class="sh">'</span><span class="p">:</span> <span class="n">response</span><span class="p">})</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">app</span><span class="p">.</span><span class="nf">run</span><span class="p">()</span>
</code></pre></div></div>

<p>Lets say this app runs on abc.ngrok-free.app. Now since this app is exposed to the internet I can make a POST request to this app in the app hosting the Webex bot.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">import</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">sys</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Retrieve the prompt from command-line arguments
</span><span class="n">url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">abc.ngrok-free.app/generate</span><span class="sh">"</span>
<span class="n">payload</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">Content-Type</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">application/json</span><span class="sh">'</span><span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">payload</span><span class="p">),</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">)</span>

<span class="k">if</span> <span class="n">response</span><span class="p">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Failed to make request. Status code: </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">status_code</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>This script communicates with the main javascript file which produces the bot responses</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">const</span> <span class="p">{</span> <span class="k">exec</span> <span class="p">}</span> <span class="o">=</span> <span class="nf">require</span><span class="p">(</span><span class="sh">'</span><span class="s">child_process</span><span class="sh">'</span><span class="p">);</span>

    <span class="n">const</span> <span class="n">prompt</span> <span class="o">=</span> <span class="n">trigger</span><span class="p">.</span><span class="n">text</span><span class="p">;</span>
    <span class="n">const</span> <span class="n">scriptPath</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./make_post.py</span><span class="sh">'</span><span class="p">;</span> <span class="o">//</span> <span class="n">Make</span> <span class="n">sure</span> <span class="n">to</span> <span class="n">provide</span> <span class="n">the</span> <span class="n">correct</span> <span class="n">path</span> <span class="n">to</span> <span class="n">the</span> <span class="n">script</span>

    <span class="nf">exec</span><span class="p">(</span><span class="sb">`python ${scriptPath} "${prompt}"`</span><span class="p">,</span> <span class="p">(</span><span class="n">error</span><span class="p">,</span> <span class="n">stdout</span><span class="p">,</span> <span class="n">stderr</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
      <span class="nf">if </span><span class="p">(</span><span class="n">error</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">console</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span><span class="sb">`An error occurred: ${error}`</span><span class="p">);</span>
        <span class="k">return</span><span class="p">;</span>
      <span class="p">}</span>
      <span class="n">const</span> <span class="n">res</span> <span class="o">=</span> <span class="n">stdout</span><span class="p">.</span><span class="nf">trim</span><span class="p">();</span>
      <span class="n">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="sb">`Response from Python: ${res.response}`</span><span class="p">);</span>

      <span class="n">bot</span><span class="p">.</span><span class="nf">say</span><span class="p">(</span><span class="sb">`Sorry, I don't know how to respond to "${trigger.text}" but llama might`</span><span class="p">)</span>
        <span class="p">.</span><span class="nf">then</span><span class="p">(()</span> <span class="o">=&gt;</span> <span class="n">bot</span><span class="p">.</span><span class="nf">say</span><span class="p">(</span><span class="sh">"</span><span class="s">markdown</span><span class="sh">"</span><span class="p">,</span> <span class="n">res</span><span class="p">))</span>
        <span class="o">//</span> <span class="p">.</span><span class="nf">then</span><span class="p">(()</span> <span class="o">=&gt;</span> <span class="nf">sendHelp</span><span class="p">(</span><span class="n">bot</span><span class="p">))</span>
        <span class="p">.</span><span class="nf">catch</span><span class="p">((</span><span class="n">e</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="n">console</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span><span class="sb">`Problem in the unexepected command hander: ${e.message}`</span><span class="p">));</span>
    <span class="p">});</span>
</code></pre></div></div>

<p>And lets see if that works:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bot-llama-response-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bot-llama-response-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bot-llama-response-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/bot-llama-response.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>Yes it does!</p>
<h2 id="final-thoughts">Final thoughts</h2>
<p>So this project started off as a fun way to get more interaction in a remote workplace, but I think the most important thing I found here is how to create my own inference api using a very high quality LLM in llama v2 using ngrok and colab. I think soon this may not even be an issue, people are finding ways to compact these opensource language models so that they can be run on local computers and even micro controllers like Raspberry Pis. I am excited to see where that goes.</p>

<p>This exposed me to some interesting concepts like ngrok for application development and the trend towards models being made light weight to run on devices is also very interesting – tiny ML as they call it. And I’ll probably go deeper into these topics later.</p>]]></content><author><name></name></author><category term="chat-bot," /><category term="llama," /><category term="networking," /><category term="colab" /><summary type="html"><![CDATA[Webex chatbot using llama.cpp and ngrok]]></summary></entry><entry><title type="html">Topic Modeling in Reddit</title><link href="https://spknash.github.io/blog/2023/Reddit-Topic-Modeling/" rel="alternate" type="text/html" title="Topic Modeling in Reddit" /><published>2023-07-28T00:00:00+00:00</published><updated>2023-07-28T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2023/Reddit-Topic-Modeling</id><content type="html" xml:base="https://spknash.github.io/blog/2023/Reddit-Topic-Modeling/"><![CDATA[<h1 id="most-common-topic-in-rchangemyview">Most Common Topic in r/changemyview</h1>

<p>In this post I am tackling the problem: What is the most talked about issue/topic on r/changemyview right now?</p>

<p>r/changemyview is a a subreddit where users post an opinion they have about a certain issue or topic and people post replies which try to pursuade the original poster to change their view. The original poster can then award delta points to the reply which pursuades them to change their view, or comes close by presenting a very good argument. Due to moderation and community rules, every single post has this same format. A opinion, and then replies which try to pursuade away from that opinion, and the replies which do an exceptional job receive “delta points”. Due to the structure and high quality of posts in this subreddit, r/changemyview is an ideal source of data for machine learning projects. Below is picture of what a r/changemyview post looks like.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/r:changemyview-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/r:changemyview-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/r:changemyview-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/r:changemyview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>For the problem I am trying to solve we only need the post titles and not the full scope of the data provided in the sub. The post title contains an opinion on a topic/issue and this is enough to determine the what topic the post is addressing. The first step to solve this problem no matter the strategy chosen is to fetch post titles over the timeframe you are interested in. Below is code for fetching titles from the top 50 post titles since last year as a sample. <code class="language-plaintext highlighter-rouge">reddit</code> is a read/write reddit instance that has been initialized with my reddit api credentials.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">submission</span> <span class="ow">in</span> <span class="n">reddit</span><span class="p">.</span><span class="nf">subreddit</span><span class="p">(</span><span class="sh">"</span><span class="s">changemyview</span><span class="sh">"</span><span class="p">).</span><span class="nf">top</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
      <span class="nf">print</span><span class="p">(</span><span class="n">submission</span><span class="p">.</span><span class="n">title</span><span class="p">)</span>
</code></pre></div></div>

<p>From here the question becomes how do we use this collection of post titles to determine the most common topic. I try a few different methods.
##Manual Labels &amp; Zero-shot Classification</p>

<p>The idea behind this first method is to manually provide several potential topics which we think are popular and use zero-shot classification to classify each post title. Either zero shot classification or trained classifier could be used but I thought zero shot classification(specifically the hugging face pipeline) might work reasonably well especially when the categories are very diverse. After manually scanning through some of the r/changemyview titles, here are the categories I came up with:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">candidate_labels</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">education</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">taxes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Trump</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">healthcare</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Religion</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">elections</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">race</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">LGBTQ</span><span class="sh">"</span><span class="p">]</span>
</code></pre></div></div>

<p>After that I used zero-shot classification in hugging-face pipelines to classify each title and here were the results of 5 posts to see if it was working as well as I wanted it to:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CMV: Mike Bloomberg's campaign is proof that the ultra wealthy in the US can afford a higher tax rate with no ill effect on them
[0.8761507272720337, 0.039841409772634506, 0.028353260830044746, 0.025303443893790245, 0.010511195287108421, 0.007160380948334932, 0.006719440221786499, 0.005960128735750914]
CMV: Kanye West is a shill for president Trump and running to syphon off young voters from voting for Biden.
[0.3772038221359253, 0.30558013916015625, 0.2326604723930359, 0.038126397877931595, 0.012778099626302719, 0.012653850950300694, 0.011820374056696892, 0.009176867082715034]
CMV: Most Americans who oppose a national healthcare system would quickly change their tune once they benefited from it.
[0.8781680464744568, 0.03057781793177128, 0.028276970610022545, 0.018674472346901894, 0.012455514632165432, 0.012006503529846668, 0.010698405094444752, 0.009142286144196987]
CMV: Donald Trump has not made a single lasting positive impact on the USA during his term as president.
[0.9717963933944702, 0.008201655931770802, 0.006410819478332996, 0.0034016254357993603, 0.0033515936229377985, 0.0030098608694970608, 0.0021523970644921064, 0.0016757362755015492]
CMV: being a conservative is the least Christ-like political view
[0.24276088178157806, 0.15054336190223694, 0.12963169813156128, 0.1104813814163208, 0.10187441110610962, 0.09882443398237228, 0.08833231031894684, 0.07755151391029358]
</code></pre></div></div>

<p>A quick look at how zero-shot classification is doing in these few examples reveals that it is not working that well. The topic “CMV: Donald Trump has not made a single lasting positive impact on the USA during his term as president.” is classified as 0.97 towards the topic <code class="language-plaintext highlighter-rouge">education</code> even though this topic has the word “Trump” in it and <code class="language-plaintext highlighter-rouge">Trump</code> is one of the categories. That is concerning but could be because the model doesn’t have embeddings for the word “Trump” as in president Trump instead of just the dictionary word. The same thing happens for “CMV: Most Americans who oppose a national healthcare system would quickly change their tune once they benefited from it.”. The classifier chooses <code class="language-plaintext highlighter-rouge">education</code> with 0.87 probability even though <code class="language-plaintext highlighter-rouge">healthcare</code> exists as an option.</p>

<p>Clearly, this strategy of manual labels and zero-shot classification is not effective. The next strategy I could explore is still within supervised topic modeling, but instead of zero-shot classification we use a classifier trained specifically for this classification task. The big problem with this method is there is no obvious way to get labels for each post title in the training dataset – manually would take very long. So instead of going in this route I will explore unsupervised topic modeling because there is no clear path forward within supervised modeling, and unsupervised modeling seems more interesting anyways because we can let the model identify distinctions between topics. So I will look at a Bag of Words model first.</p>

<h2 id="bag-of-wordslda">Bag of Words/LDA</h2>

<p>I’ll walk through how I used the bag of words method and LDA to perform unsupervised topic classification. Afterwards I’ll provide a brief overview of how these methods work and the motivation behind it.</p>

<p>The first step is to create a bag of words matrix, which will contain  For each document(each post title in this case) the frequency of each word in the dictionary is recorded in a row vector. The idea behind the bag of words matrix is to capture where words are repeating to provide information about which documents are covering the similar topics. For example if document 1 and document 3 both contain high frequency of the word “cat” they likely cover similar topics. This type of analysis clearly does not depend on many words which appear very frequently in the english language such as “as”, “to”, or “the”. These words are called stop words. Additionally, in this type of analysis there is no real difference between root words and extended words such as “happy” and “happiest”. Therefore, all words should be shortened to their shortest stem. Below is the function used to pre-process every post title. It tokenizes, deletes stop words, and shortens to smallest stem. This implementation uses the nltk library which is common for text preprocessing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">process_document</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
  <span class="n">tokens</span> <span class="o">=</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

  <span class="c1"># Removing Stop words
</span>  <span class="n">stop_words</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">stopwords</span><span class="p">.</span><span class="nf">words</span><span class="p">(</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">))</span>
  <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>

  <span class="c1"># Stemming
</span>  <span class="n">stemmer</span> <span class="o">=</span> <span class="nc">PorterStemmer</span><span class="p">()</span>
  <span class="n">stemmed_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="p">.</span><span class="nf">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">stemmed_tokens</span>
</code></pre></div></div>

<p>Below is an example to show exactly what the preprocessing is doing:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">process_document</span><span class="p">(</span><span class="sh">"</span><span class="s">hello, my name is Suhaas and I really like frisbee and tennis. What sports do you like?</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['hello',
 ',',
 'name',
 'suhaa',
 'i',
 'realli',
 'like',
 'frisbe',
 'tenni',
 '.',
 'what',
 'sport',
 'like',
 '?']
</code></pre></div></div>

<p>After preprocessing of the text is complete the bag of words matrix can be made.</p>

<p>The row vector of every document in the corpus are stacked on top of each other to form the Bag of words matrix. The row vector contains the frequency of each word. Below is a simple example. Say the corpus is</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>corpus = ["the dog is wet, the dog is angry",
  "the cat is upset and very hungry",
  "who let the dogs out? They are making a mess",
  "I can't believe the cat drank the milk"]
</code></pre></div></div>
<p>Then the bag of words matrix will be:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   angri  believ  ca  cat  dog  drank  hungri  let  make  mess  milk  upset  \
0      1       0   0    0    2      0       0    0     0     0     0      0   
1      0       0   0    1    0      0       1    0     0     0     0      1   
2      0       0   0    0    1      0       0    1     1     1     0      0   
3      0       1   1    1    0      1       0    0     0     0     1      0   

   wet  
0    1  
1    0  
2    0  
3    0
</code></pre></div></div>

<p>This matrix is pretty useful but can be made even more meaningful if rare words are given a greater weight and prevalence in the model. For example, in r/changemyview topics the word “abortion” is very rare. But when it does appear it is almost certainly the topic of the entire post. There are many words like this: words which are rare in the english language but determine the topic of the document when they do appear. This is where TF-IDF comes in. TF-IDF is a method to alter the bag of words matrix to increase the weight on these important words which are rare in the corpus. TF-IDF is composed of two parts:</p>

<p>Term Frequency (TF): This is simply the frequency of a word in a document. It’s based on the idea that the importance of a word is proportional to its frequency. However, some words like ‘the’, ‘is’, and ‘and’ appear frequently in all sorts of contexts, so high frequency doesn’t always mean high importance. That’s where the second part of TF-IDF comes in.</p>

<p>Inverse Document Frequency (IDF): This reduces the weight of words that are common in the corpus. IDF is calculated as the logarithm of the total number of documents in the corpus divided by the number of documents containing the term. Thus, it increases for rare words and decreases for common words.</p>

<p>The overall TF-IDF score for a word in a document is the product of its TF and IDF scores.</p>

<p>Here’s the mathematical formula for TF-IDF:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TF-IDF(t, d, D) = TF(t, d) * IDF(t, D)
</code></pre></div></div>

<p>where:</p>

<p><code class="language-plaintext highlighter-rouge">t</code> is the term or word
<code class="language-plaintext highlighter-rouge">d</code> is the document
<code class="language-plaintext highlighter-rouge">D</code> is the corpus
<code class="language-plaintext highlighter-rouge">TF(t, d)</code> is the term frequency of t in d (usually normalized by dividing by the total number of words in d)
<code class="language-plaintext highlighter-rouge">IDF(t, D)</code> is the inverse document frequency of t in D, calculated as <code class="language-plaintext highlighter-rouge">log(N / df(t))</code>, where N is the total number of documents and df(t) is the number of documents that contain t (to prevent division by zero if a word is not in the corpus, it’s common to add 1 to the denominator)</p>

<p>This calculation increases the weight of words which are rare in the corpus but frequent in a particular document. Such a word is probably very relevant to the topic of that document. Below is a implementation of creating a bag of words matrix using TF-IDF(this implementation uses the sci-kit library):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">BAG_matrix</span><span class="p">(</span><span class="n">corpus</span><span class="p">):</span>
  <span class="c1"># Apply preprocessing to each document in the corpus
</span>  <span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="nf">process_document</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>

  <span class="c1"># Initialize CountVectorizer
</span>  <span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

  <span class="c1"># Tokenize and build vocab
</span>  <span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

  <span class="c1"># Now, we initialize the TfidfTransformer and transform our count-matrix to tf-idf representation
</span>  <span class="n">transformer</span> <span class="o">=</span> <span class="nc">TfidfTransformer</span><span class="p">(</span><span class="n">smooth_idf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">use_idf</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="n">tfidf</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

  <span class="c1"># Output the shape of X
</span>  <span class="nf">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

  <span class="c1"># To get feature names
</span>  <span class="n">feature_names</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">()</span>

  <span class="c1"># To view the matrix as a DataFrame
</span>  <span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
  <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">tfidf</span><span class="p">.</span><span class="nf">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_names</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tfidf</span><span class="p">,</span> <span class="n">vectorizer</span>
</code></pre></div></div>

<p>And below is the matrix of the same corpus from earlier but using tf-idf now:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     angri    believ        ca       cat       dog     drank    hungri  \
0  0.47212  0.000000  0.000000  0.000000  0.744450  0.000000  0.000000   
1  0.00000  0.000000  0.000000  0.486934  0.000000  0.000000  0.617614   
2  0.00000  0.000000  0.000000  0.000000  0.414289  0.000000  0.000000   
3  0.00000  0.465162  0.465162  0.366739  0.000000  0.465162  0.000000   

        let      make      mess      milk     upset      wet  
0  0.000000  0.000000  0.000000  0.000000  0.000000  0.47212  
1  0.000000  0.000000  0.000000  0.000000  0.617614  0.00000  
2  0.525473  0.525473  0.525473  0.000000  0.000000  0.00000  
3  0.000000  0.000000  0.000000  0.465162  0.000000  0.00000  
</code></pre></div></div>

<p>The next step is to use this tf-idf matrix to construct topics. This can be done using Latent Dirichlet Allocation.
We will be using the LDA method in sci-kit but here is an overview of how LDA works:</p>

<p>Initialize LDA: First, initialize the LDA model. One key parameter to set here is the number of topics you want the model to identify. This is a hyperparameter that might need to be tuned to get the best results.</p>

<p>Assign topics to words: LDA starts by randomly assigning each word in each document to one of the K topics (where K is the number of topics you decided on). This random assignment already gives you both topic representations of all the documents and word distributions of all the topics (albeit not very good ones because it is random).</p>

<p>Iteratively update topic assignments: Then, LDA iteratively updates the topic assignments for each word in each document, based on two criteria:</p>

<p>a. How prevalent is the topic in the document? The more often the topic occurs in the document, the more likely it is that the word belongs to this topic.</p>

<p>b. How prevalent is the word across topics? If a word is already often assigned to a topic, it’s likely that it will be assigned to this topic again.</p>

<p>Each iteration of this step is done using a method called Gibbs Sampling, which is a type of Markov Chain Monte Carlo (MCMC) algorithm.</p>

<p>The process continues until the model’s estimates of the topics stabilize, or after a set number of iterations. Once finished, you’ll evaluate the topics that the model has learned. After this iterative process, LDA will represent each document in the corpus as a mixture of different topics(learns a distribution of topics for each document), and represents each topic as a set of top words(learns a distribution of words for each topic). More information about LDA can be found in the paper by Blie, Jordan, and Ng <a href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">here.</a></p>

<p>Below is an implementattion using the bag of words matrix from earlier and an LDA method from sci-kit. It produces <code class="language-plaintext highlighter-rouge">num_topics</code> and prints the top 5 words for each topic to allow us to see what each topic is really about.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">LDA_topics</span><span class="p">(</span><span class="n">tfidf</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">,</span> <span class="n">num_topics</span><span class="p">):</span>
  <span class="c1"># Initialize LDA
</span>  <span class="c1"># n_components specifies the number of topics
</span>  <span class="n">lda</span> <span class="o">=</span> <span class="nc">LatentDirichletAllocation</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">num_topics</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

  <span class="c1"># Fit LDA to BoW data
</span>  <span class="n">lda</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">tfidf</span><span class="p">)</span>

  <span class="c1"># For each topic, print the top 10 most representative words
</span>  <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">topic</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">lda</span><span class="p">.</span><span class="n">components_</span><span class="p">):</span>
      <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Top 5 words for Topic #</span><span class="si">{</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
      <span class="nf">print</span><span class="p">(</span>
          <span class="p">[</span><span class="n">vectorizer</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">()[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">topic</span><span class="p">.</span><span class="nf">argsort</span><span class="p">()[</span><span class="o">-</span><span class="mi">5</span><span class="p">:]])</span>
      <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>This function takes the tfidf matrix and vectorizer as inputs from the bag of words function – The function below creates the tf-idf matrix and topics using LDA.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_topics</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">num_topics</span><span class="p">):</span>
  <span class="n">tfidf</span><span class="p">,</span> <span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">BAG_matrix</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
  <span class="nc">LDA_topics</span><span class="p">(</span><span class="n">tfidf</span><span class="p">,</span><span class="n">vectorizer</span><span class="p">,</span><span class="n">num_topics</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we are almost ready to use this function to generate topics from r/changemyview posts! The last thing left to do is to produce the corpus which will be a list post title from the subreddit. The code below creates the corpus:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">reddit_corpus</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">subreddit</span> <span class="o">=</span> <span class="n">reddit</span><span class="p">.</span><span class="nf">subreddit</span><span class="p">(</span><span class="sh">"</span><span class="s">changemyview</span><span class="sh">"</span><span class="p">)</span>
<span class="n">after</span> <span class="o">=</span> <span class="bp">None</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">after</span><span class="p">:</span>
        <span class="n">new_posts</span> <span class="o">=</span> <span class="n">subreddit</span><span class="p">.</span><span class="nf">top</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">after</span><span class="sh">'</span><span class="p">:</span> <span class="n">after</span><span class="p">})</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">new_posts</span> <span class="o">=</span> <span class="n">subreddit</span><span class="p">.</span><span class="nf">top</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

    <span class="n">last_post</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">post</span> <span class="ow">in</span> <span class="n">new_posts</span><span class="p">:</span>
        <span class="n">reddit_corpus</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">post</span><span class="p">.</span><span class="n">title</span><span class="p">[</span><span class="mi">4</span><span class="p">:])</span>
        <span class="n">last_post</span> <span class="o">=</span> <span class="n">post</span>

    <span class="n">after</span> <span class="o">=</span> <span class="n">last_post</span>  <span class="c1"># ID of the last post
</span>
<span class="k">for</span> <span class="n">submission</span> <span class="ow">in</span> <span class="n">reddit</span><span class="p">.</span><span class="nf">subreddit</span><span class="p">(</span><span class="sh">"</span><span class="s">changemyview</span><span class="sh">"</span><span class="p">).</span><span class="nf">top</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
      <span class="c1">#print(submission.title[4:])
</span>      <span class="n">reddit_corpus</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">submission</span><span class="p">.</span><span class="n">title</span><span class="p">[</span><span class="mi">4</span><span class="p">:])</span>
</code></pre></div></div>

<p>For each post I append <code class="language-plaintext highlighter-rouge">post.title[4:]</code> instead of the full post because each post starts with “CMV:” and we don’t want CMV to be one of the words which define a topic since it is present in every title. Additionally, the reddit API only allows 60 queries per minute and on 500 posts per query so I had to use the <code class="language-plaintext highlighter-rouge">params={'after': after})</code> parameter to start the next query where the previous one left off.</p>

<p>Now lets try out how the LDA topic generation did.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">make_topics</span><span class="p">(</span><span class="n">reddit_corpus</span><span class="p">,</span><span class="mi">15</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Top 5 words for Topic #1
['work', 'licens', 'conserv', 'the', 'religion']


Top 5 words for Topic #2
['charact', 'the', 'race', 'us', 'racist']


Top 5 words for Topic #3
['noth', 'there', 'sex', 'peopl', 'gender']


Top 5 words for Topic #4
['tri', 'cover', 'parent', 'world', 'sub']


Top 5 words for Topic #5
['joke', 'eat', 'women', 'citizenship', 'the']


Top 5 words for Topic #6
['offens', 'help', 'place', 'polic', 'the']


Top 5 words for Topic #7
['anim', 'much', 'small', 'commun', 'peopl']


Top 5 words for Topic #8
['child', 'flag', 'the', 'peopl', 'chang']


Top 5 words for Topic #9
['parti', 'make', 'peopl', 'vote', 'we']


Top 5 words for Topic #10
['wealth', 'donald', 'if', 'bodi', 'posit']


Top 5 words for Topic #11
['realiti', 'answer', 'appropri', 'cultur', 'thing']


Top 5 words for Topic #12
['hire', 'incom', 'job', 'includ', 'social']


Top 5 words for Topic #13
['consid', 'there', 'health', 'it', 'peopl']


Top 5 words for Topic #14
['homeless', 'there', 'it', 'see', 'wrong']


Top 5 words for Topic #15
['like', 'need', 'cultur', 'us', 'get']
</code></pre></div></div>

<p>Ok! so definetly an improvement from zero-shot learning, but still room for improvement as well. Some topics have top words that are very connected and make sense with how humans would define topics in this corpus. For example topic 3</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'charact', 'the', 'race', 'us', 'racist'
</code></pre></div></div>
<p>seems to be about race and culture.</p>

<p>Topic 12 is</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['hire', 'incom', 'job', 'includ', 'social']
</code></pre></div></div>
<p>This topic seems to be about employment and jobs.</p>

<p>And this topic:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['noth', 'there', 'sex', 'peopl', 'gender']
</code></pre></div></div>
<p>seems to be about gender and sexuality issues. On that same note there are some topics which don’t have a common theme like these ones:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['homeless', 'there', 'it', 'see', 'wrong']
['joke', 'eat', 'women', 'citizenship', 'the']
</code></pre></div></div>
<p>This is to be expected and highlights the limitations of LDA. Because it is a unsupervised topic model, the topics may not be formed in the same way a human would. The algorithms may form a topic based on some similarities or frequency of a particular word which we don’t see as important. Additionally, LDA does not take into account the order and semantics fo the words. For example, LDA would think “this post is about police” and “this post is not about police” are both about police simply because it contains the word “police”. Despite not taking into account semantics and word order it is pretty cool to see it produce some topics the similar to how a human would.</p>

<p>There are also some hyper parameters which could be tuned to produce better results like: number of topics, n-grams during bag of words matrix, number of top words for each topic. Perhaps the results of this topic modeling could be improved a little more just by further tuning these hyper parameters.</p>

<p>I am really interested to see how a model which does take into account the order of words could be used for topic modeling. An example includes BERTopic. There are also many extensions to LDA such as Dynamic LDA, Hierarchical Dirichlet Process, and GuidedLDA. I may explore these other techniques for topic modeling in future posts.</p>]]></content><author><name></name></author><category term="topic-modeling," /><category term="reddit," /><category term="colab" /><summary type="html"><![CDATA[Supervised and Unsupervised Topic modeling on r/changemyview]]></summary></entry><entry><title type="html">Apple-Pineapple Classifier</title><link href="https://spknash.github.io/blog/2023/PPAP/" rel="alternate" type="text/html" title="Apple-Pineapple Classifier" /><published>2023-07-14T00:00:00+00:00</published><updated>2023-07-14T00:00:00+00:00</updated><id>https://spknash.github.io/blog/2023/PPAP</id><content type="html" xml:base="https://spknash.github.io/blog/2023/PPAP/"><![CDATA[<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/PPAP-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/PPAP-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/PPAP-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/PPAP.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>In this post I’m going over how I made
<a href="https://huggingface.co/spaces/suhaaspk/PPAP">this</a> simple
hugging face space which can classify between different fruits when you
input an image to the app. The first step is training the model which
can classify apples and pineapples.</p>

<p>I trained on google colab. Below are the necesary imports and mounting
to my personal google drive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">Uqq</span> <span class="n">fastai</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">Uqq</span> <span class="n">bing_image_downloader</span>
<span class="kn">from</span> <span class="n">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">Uqq</span> <span class="n">gradio</span>
<span class="kn">import</span> <span class="n">gradio</span> <span class="k">as</span> <span class="n">gr</span>
<span class="kn">from</span> <span class="n">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="p">.</span><span class="nf">mount</span><span class="p">(</span><span class="sh">'</span><span class="s">/content/drive</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p>The plan is to use FastAI’s vision learner which is already trained to
recognize many things in images, and finetune this model using images of
apples and pineapples. The images to finetune are fetched using bing
search. 100 images of each fruit are used.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">bing_image_downloader</span> <span class="kn">import</span> <span class="n">downloader</span>

<span class="n">downloader</span><span class="p">.</span><span class="nf">download</span><span class="p">(</span><span class="sh">"</span><span class="s">apple fruit</span><span class="sh">"</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="n">output_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">dataset</span><span class="sh">'</span><span class="p">,</span> <span class="n">adult_filter_off</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">force_replace</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">downloader</span><span class="p">.</span><span class="nf">download</span><span class="p">(</span><span class="sh">"</span><span class="s">pineapple fruit</span><span class="sh">"</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="n">output_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">dataset</span><span class="sh">'</span><span class="p">,</span> <span class="n">adult_filter_off</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">force_replace</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
</code></pre></div></div>

<p>Below is a small sample of the images taken from bing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">path</span> <span class="o">=</span> <span class="nc">Path</span><span class="p">(</span><span class="sh">'</span><span class="s">dataset</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">ImageDataLoaders</span><span class="p">.</span><span class="nf">from_folder</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">,</span> <span class="n">valid_pct</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">item_tfms</span><span class="o">=</span><span class="nc">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">))</span>

<span class="c1"># Checking the data
</span><span class="n">dls</span><span class="p">.</span><span class="nf">show_batch</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cell-4-output-1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cell-4-output-1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cell-4-output-1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cell-4-output-1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Sample of the bing images to fine tune the model
</div>

<p>The next step is to finetune using the images and this is done using
fastai’s learner in the following lines. It runs for 3 epochs. The
training error continuously decreases as well as the validation error.
The validation error is only slightly larger than training error and the
distance between the two errors decreases for each epoch so I feel good
there is not much overfitting.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span> <span class="o">=</span> <span class="nf">vision_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet18</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">error_rate</span><span class="p">)</span>
<span class="n">learn</span><span class="p">.</span><span class="nf">fine_tune</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<p>After the model is done training, I export the model to my google drive
so I can upload to my hugging face spaces app.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span><span class="p">.</span><span class="nf">export</span><span class="p">(</span><span class="sh">'</span><span class="s">model.pkl</span><span class="sh">'</span><span class="p">)</span>
<span class="err">!</span><span class="n">mv</span> <span class="sh">'</span><span class="s">dataset/model.pkl</span><span class="sh">'</span> <span class="sh">'</span><span class="s">/content/drive/My Drive/Colab Projects/Apple-Pineapple</span><span class="sh">'</span>
<span class="n">learn</span> <span class="o">=</span> <span class="nf">load_learner</span><span class="p">(</span><span class="sh">'</span><span class="s">drive/My Drive/Colab Projects/Apple-Pineapple/model.pkl</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p>To create a hugging face space go
<a href="https://huggingface.co/spaces/suhaaspk/PPAP">hugging face</a>
and click create a new space. For this space I am going to use gradio.
After creating the space you can add files and edit the app with git. I
added the model.pkl which I had trained earlier to the space. And added
app.py which creates the app.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">gradio</span> <span class="k">as</span> <span class="n">gr</span>
<span class="kn">from</span> <span class="n">gradio</span> <span class="kn">import</span> <span class="n">components</span>
<span class="kn">from</span> <span class="n">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">learn</span> <span class="o">=</span> <span class="nf">load_learner</span><span class="p">(</span><span class="sh">'</span><span class="s">model.pkl</span><span class="sh">'</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">learn</span><span class="p">.</span><span class="n">dls</span><span class="p">.</span><span class="n">vocab</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">PILImage</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">img</span><span class="p">))</span>
    <span class="n">pred</span><span class="p">,</span><span class="n">pred_idx</span><span class="p">,</span><span class="n">probs</span> <span class="o">=</span> <span class="n">learn</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="nf">float</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))}</span>


<span class="n">iface</span> <span class="o">=</span> <span class="n">gr</span><span class="p">.</span><span class="nc">Interface</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">predict</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">components</span><span class="p">.</span><span class="nc">Image</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">components</span><span class="p">.</span><span class="nc">Label</span><span class="p">(</span><span class="n">num_top_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>

<span class="n">iface</span><span class="p">.</span><span class="nf">launch</span><span class="p">()</span>
</code></pre></div></div>

<p>I could add app.py with git but model.pkl could not be added with git
because the file was too large, so I added it manually on hf spaces.
After the adding these files the app was able to build and now runs on
hf spaces!</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/files-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/files-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/files-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/files.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    The files in hf spaces to make the app.
</div>

<p>The final product and in action:</p>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hf_app_final-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hf_app_final-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hf_app_final-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/hf_app_final.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>]]></content><author><name></name></author><category term="tutorial," /><category term="intro" /><summary type="html"><![CDATA[A simple hugging-face space which can be used to classify fruit.]]></summary></entry></feed>