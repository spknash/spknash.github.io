<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Webex llama bot | Suhaas</title>
    <meta name="author" content="Suhaas  ">
    <meta name="description" content="Webex chatbot using llama.cpp and ngrok">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://spknash.github.io/blog/2023/Webex-llama-bot/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Suhaas</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">Repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Webex llama bot</h1>
    <p class="post-meta">August 11, 2023</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
        ·  
        <a href="/blog/tag/chat-bot">
          <i class="fas fa-hashtag fa-sm"></i> chat-bot,</a>  
          <a href="/blog/tag/llama">
          <i class="fas fa-hashtag fa-sm"></i> llama,</a>  
          <a href="/blog/tag/networking">
          <i class="fas fa-hashtag fa-sm"></i> networking,</a>  
          <a href="/blog/tag/colab">
          <i class="fas fa-hashtag fa-sm"></i> colab</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <h1 id="webex-chatbot-to-increase-office-interaction">Webex chatbot to increase office interaction</h1>

<p>This project started off as the project my team did for the intern hackathon at my summer internship. The topic of the hackathon was build something that improves remote and hybrid work. Our team decided that one of the biggest things lost during remote work is in person interaction and social interaction. It is very hard to become friends with someone when you don’t see them in person. Even harder to become friends when the only topic of conversation between you and them is a specific work task within the context of a work meeting. In person there are a lot more casual interactions in the hallway and especially during lunch that allow co-workers to become much closer to each other.</p>

<p>Our idea was to create a application which pairs two random employees in the company for a 30 minute chat each week. I won’t talk about the implementation of the full application but only about what I did. I worked on all parts related to Webex. We needed to use the Webex API to automatically schedule meetings between two people in the org. We also decided to incorporate a webex chatbot which could do things like provide background information about each participant in the meeting and also do things like offer a potential fun meeting topic if the participants don’t know what to talk about. As I was making the chatbot I realized its not a very good chatbot if it only responds to certain commands. Like below:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bot-help-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bot-help-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bot-help-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/bot-help.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>I wanted to use a Large language model to allow the bot to respond to messages more dynamically. The only issue is I don’t want to spend any money on this because this a purely for fun project and many llms require payment to either use their apis or require the use of cloud credits to host on a cloud platform. I arrived at an idea based on something I used to build the webex chatbot.</p>

<h2 id="webex-chatbot">Webex chatbot</h2>

<p>The webex chatbot was made using this template: <a href="https://github.com/WebexSamples/webex-bot-starter" rel="external nofollow noopener" target="_blank">github link</a>. I cloned this repo to my local and began to make changes. In order for the bot to work it needs to be able to communicate over http, so I used ngrok to expose the bot application to the internet.
###What is ngrok and how does it work?
When developing a web application, you typically run a local server that is only accessible on your personal computer. However, there might be instances where you need to share this local server with someone else on the internet, perhaps for testing, demonstrations, or external integrations like webhooks.</p>

<p>This is where ngrok comes in. By running a simple command in your command line interface, ngrok provides you with a public URL (HTTP/HTTPS) that forwards incoming requests to your local server. This URL can be shared with anyone, and they’ll be able to access your locally running application as if it were hosted online.</p>

<h3 id="general-use-cases">General Use Cases:</h3>
<p>Testing on Different Devices: If you want to test how your application appears on different devices, ngrok makes it easy by allowing those devices to access your local server through the public URL.</p>

<p>Collaborating with Team Members: If you’re working with team members who are not on the same local network, you can use ngrok to give them access to your development environment.</p>

<p>Webhook Development and Testing: Many third-party services use webhooks to communicate with your application. Ngrok allows these services to connect with your local server, simplifying the process of developing and testing webhooks.</p>

<p>Sharing a Demo with Clients: If you want to share a live demo of an application that’s still in development, ngrok enables you to do so without having to deploy it to a public server.</p>

<p>Temporary Hosting for Hackathons or Prototyping: Quick prototyping or participation in hackathons often requires temporary public access to your local development. Ngrok provides a swift solution for these scenarios.</p>

<p>In conclusion, ngrok is an essential tool for modern web development, offering a convenient way to share your local development environment with others. Its ease of use and wide range of applications make it an indispensable resource for developers seeking to streamline their workflow and collaboration efforts.</p>

<h2 id="trying-to-use-llamacpp">Trying to use llama.cpp</h2>
<p>So my webex bot at this point works well with limited structured commands but I wanted the catch-all case to give a best response using a large language model instead of not being able to handle a novel response. The first LLM I thought of using was llama because it seems to be the best performing open source langauge model at the moment but also because of llama.cpp which is a implementation of llama in C which is compact enough thata it can be run on Mac M1 or M2 chips. I forked llama.cpp and tried using it on local but the speed at which the response was very slow and it is also probably not great to run something so intensive on my Mac even if it was just for a hackathon/demo.</p>

<p>The next idea was to use llama.cpp on colab – and use ngrok to expose llama to the internet so it can be used essentially as an inference api for any application I want to build.</p>

<p>##Using llama.cpp and ngrok on Colab</p>

<p>Now I can use llama.cpp to create an app on colab and expose it to the internet to my webex bot can use it as a inference API to generate responses. To use llama.cpp on colab I need to use llama-cpp-python which is python bindings for llama.cpp. I also use langchains to do some minor prompt templating.</p>

<p>The imports:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.llms</span> <span class="kn">import</span> <span class="n">LlamaCpp</span>
<span class="kn">from</span> <span class="n">langchain</span> <span class="kn">import</span> <span class="n">PromptTemplate</span><span class="p">,</span> <span class="n">LLMChain</span>
<span class="kn">from</span> <span class="n">langchain.callbacks.manager</span> <span class="kn">import</span> <span class="n">CallbackManager</span>
<span class="kn">from</span> <span class="n">langchain.callbacks.streaming_stdout</span> <span class="kn">import</span> <span class="n">StreamingStdOutCallbackHandler</span>
</code></pre></div></div>

<p>The simple prompt template using langchains:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">{question}

Answer: Let</span><span class="sh">'</span><span class="s">s work this out in a step by step way to be sure we have the right answer.</span><span class="sh">"""</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">template</span><span class="p">,</span> <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div>

<p>This command declares a callback manager which allows the llm to produce tokens one by one chat-gpt style.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">callback_manager</span> <span class="o">=</span> <span class="nc">CallbackManager</span><span class="p">([</span><span class="nc">StreamingStdOutCallbackHandler</span><span class="p">()])</span>
</code></pre></div></div>

<p>This command gets the ggml file needed to use llama.cpp</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="p">.</span><span class="n">co</span><span class="o">/</span><span class="n">TheBloke</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">13</span><span class="n">B</span><span class="o">-</span><span class="n">chat</span><span class="o">-</span><span class="n">GGML</span><span class="o">/</span><span class="n">resolve</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">13</span><span class="n">b</span><span class="o">-</span><span class="n">chat</span><span class="p">.</span><span class="n">ggmlv3</span><span class="p">.</span><span class="n">q4_0</span><span class="p">.</span><span class="nb">bin</span>
</code></pre></div></div>

<p>This next block of code sets the model parameters and creates the inference function: llm</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_gpu_layers</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Change this value based on your model and your GPU VRAM pool.
</span><span class="n">n_batch</span> <span class="o">=</span> <span class="mi">512</span>  <span class="c1"># Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
</span>
<span class="c1"># Make sure the model path is correct for your system!
</span><span class="n">llm</span> <span class="o">=</span> <span class="nc">LlamaCpp</span><span class="p">(</span>
    <span class="n">model_path</span><span class="o">=</span><span class="sh">"</span><span class="s">llama-2-13b-chat.ggmlv3.q4_0.bin</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">n_gpu_layers</span><span class="o">=</span><span class="n">n_gpu_layers</span><span class="p">,</span>
    <span class="n">n_batch</span><span class="o">=</span><span class="n">n_batch</span><span class="p">,</span>
    <span class="n">callback_manager</span><span class="o">=</span><span class="n">callback_manager</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Now that we have the inference function we can create a app and expose to the internet using ngrok. First I test llm to see how fast it is.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prompt</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
Are there hills in Peru?
</span><span class="sh">"""</span>
<span class="nf">llm</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Yes, Peru is home to a variety of landscapes including hills, mountains,
 plains, and coastal areas. The Andes Mountains run through Peru, giving rise
 to many high peaks and rolling hills. These geographical features create
 diverse ecosystems throughout the country, from the high-altitude grasslands
 of the Andean Plateau to the arid coastal plains.
</code></pre></div></div>

<p>It runs much faster than on local and it is a necesary improvement because it was way too slow on local. Now onto the app.  Initially I had trouble using ngrok, this was because I hadn’t authenticated my ngrok token and to do that I needed to run these commands:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="nb">bin</span><span class="p">.</span><span class="n">equinox</span><span class="p">.</span><span class="n">io</span><span class="o">/</span><span class="n">c</span><span class="o">/</span><span class="mi">4</span><span class="n">VmDzA7iaHb</span><span class="o">/</span><span class="n">ngrok</span><span class="o">-</span><span class="n">stable</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">amd64</span><span class="p">.</span><span class="nb">zip</span>
<span class="err">!</span><span class="n">unzip</span> <span class="n">ngrok</span><span class="o">-</span><span class="n">stable</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">amd64</span><span class="p">.</span><span class="nb">zip</span>
<span class="err">!</span><span class="p">.</span><span class="o">/</span><span class="n">ngrok</span> <span class="n">authtoken</span> <span class="o">*</span><span class="n">my</span><span class="o">-</span><span class="n">token</span><span class="o">*</span>
</code></pre></div></div>

<p>I made a flask app and which on a POST request to /generate, retrieves the prompt and generates the response, and packages the response in json and sends it back.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">flask</span> <span class="kn">import</span> <span class="n">Flask</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">jsonify</span>
<span class="kn">from</span> <span class="n">flask_ngrok</span> <span class="kn">import</span> <span class="n">run_with_ngrok</span>
<span class="kn">from</span> <span class="n">flask_cors</span> <span class="kn">import</span> <span class="n">CORS</span>

<span class="n">app</span> <span class="o">=</span> <span class="nc">Flask</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>
<span class="nc">CORS</span><span class="p">(</span><span class="n">app</span><span class="p">)</span>  <span class="c1"># Enable CORS for all routes
</span><span class="nf">run_with_ngrok</span><span class="p">(</span><span class="n">app</span><span class="p">)</span>  <span class="c1"># Start ngrok when app is run
</span>
<span class="c1"># Assuming llm is already defined and loaded elsewhere in your code
# and you can get a response by calling llm(prompt)
</span><span class="nd">@app.route</span><span class="p">(</span><span class="sh">'</span><span class="s">/</span><span class="sh">'</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">home</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">home page</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="sh">'</span><span class="s">Hello, World!</span><span class="sh">'</span>
<span class="nd">@app.route</span><span class="p">(</span><span class="sh">'</span><span class="s">/generate</span><span class="sh">'</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">POST</span><span class="sh">'</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">generate</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">generating response ...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="n">json</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">prompt</span><span class="sh">'</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Received prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># Print the received prompt
</span>    <span class="n">response</span> <span class="o">=</span> <span class="nf">llm</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Generated response: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># Print the generated response
</span>
    <span class="k">return</span> <span class="nf">jsonify</span><span class="p">({</span><span class="sh">'</span><span class="s">response</span><span class="sh">'</span><span class="p">:</span> <span class="n">response</span><span class="p">})</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">app</span><span class="p">.</span><span class="nf">run</span><span class="p">()</span>
</code></pre></div></div>

<p>Lets say this app runs on abc.ngrok-free.app. Now since this app is exposed to the internet I can make a POST request to this app in the app hosting the Webex bot.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">import</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">sys</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Retrieve the prompt from command-line arguments
</span><span class="n">url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">abc.ngrok-free.app/generate</span><span class="sh">"</span>
<span class="n">payload</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">Content-Type</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">application/json</span><span class="sh">'</span><span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">payload</span><span class="p">),</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">)</span>

<span class="k">if</span> <span class="n">response</span><span class="p">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Failed to make request. Status code: </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">status_code</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>This script communicates with the main javascript file which produces the bot responses</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">const</span> <span class="p">{</span> <span class="k">exec</span> <span class="p">}</span> <span class="o">=</span> <span class="nf">require</span><span class="p">(</span><span class="sh">'</span><span class="s">child_process</span><span class="sh">'</span><span class="p">);</span>

    <span class="n">const</span> <span class="n">prompt</span> <span class="o">=</span> <span class="n">trigger</span><span class="p">.</span><span class="n">text</span><span class="p">;</span>
    <span class="n">const</span> <span class="n">scriptPath</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./make_post.py</span><span class="sh">'</span><span class="p">;</span> <span class="o">//</span> <span class="n">Make</span> <span class="n">sure</span> <span class="n">to</span> <span class="n">provide</span> <span class="n">the</span> <span class="n">correct</span> <span class="n">path</span> <span class="n">to</span> <span class="n">the</span> <span class="n">script</span>

    <span class="nf">exec</span><span class="p">(</span><span class="sb">`python ${scriptPath} "${prompt}"`</span><span class="p">,</span> <span class="p">(</span><span class="n">error</span><span class="p">,</span> <span class="n">stdout</span><span class="p">,</span> <span class="n">stderr</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
      <span class="nf">if </span><span class="p">(</span><span class="n">error</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">console</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span><span class="sb">`An error occurred: ${error}`</span><span class="p">);</span>
        <span class="k">return</span><span class="p">;</span>
      <span class="p">}</span>
      <span class="n">const</span> <span class="n">res</span> <span class="o">=</span> <span class="n">stdout</span><span class="p">.</span><span class="nf">trim</span><span class="p">();</span>
      <span class="n">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="sb">`Response from Python: ${res.response}`</span><span class="p">);</span>

      <span class="n">bot</span><span class="p">.</span><span class="nf">say</span><span class="p">(</span><span class="sb">`Sorry, I don't know how to respond to "${trigger.text}" but llama might`</span><span class="p">)</span>
        <span class="p">.</span><span class="nf">then</span><span class="p">(()</span> <span class="o">=&gt;</span> <span class="n">bot</span><span class="p">.</span><span class="nf">say</span><span class="p">(</span><span class="sh">"</span><span class="s">markdown</span><span class="sh">"</span><span class="p">,</span> <span class="n">res</span><span class="p">))</span>
        <span class="o">//</span> <span class="p">.</span><span class="nf">then</span><span class="p">(()</span> <span class="o">=&gt;</span> <span class="nf">sendHelp</span><span class="p">(</span><span class="n">bot</span><span class="p">))</span>
        <span class="p">.</span><span class="nf">catch</span><span class="p">((</span><span class="n">e</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="n">console</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span><span class="sb">`Problem in the unexepected command hander: ${e.message}`</span><span class="p">));</span>
    <span class="p">});</span>
</code></pre></div></div>

<p>And lets see if that works:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bot-llama-response-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bot-llama-response-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bot-llama-response-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/bot-llama-response.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>Yes it does!</p>
<h2 id="final-thoughts">Final thoughts</h2>
<p>So this project started off as a fun way to get more interaction in a remote workplace, but I think the most important thing I found here is how to create my own inference api using a very high quality LLM in llama v2 using ngrok and colab. I think soon this may not even be an issue, people are finding ways to compact these opensource language models so that they can be run on local computers and even micro controllers like Raspberry Pis. I am excited to see where that goes.</p>

<p>This exposed me to some interesting concepts like ngrok for application development and the trend towards models being made light weight to run on devices is also very interesting – tiny ML as they call it. And I’ll probably go deeper into these topics later.</p>

    </div>
  </article>
</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Suhaas  . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
