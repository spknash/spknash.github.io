<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Topic Modeling in Reddit | Suhaas P Katikaneni</title>
    <meta name="author" content="Suhaas P Katikaneni">
    <meta name="description" content="Supervised and Unsupervised Topic modeling on r/changemyview">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://spknash.github.io/blog/2023/Reddit-Topic-Modeling/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Suhaas </span>P Katikaneni</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">Repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Topic Modeling in Reddit</h1>
    <p class="post-meta">July 28, 2023</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
        ·  
        <a href="/blog/tag/intro">
          <i class="fas fa-hashtag fa-sm"></i> intro</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <h1 id="most-common-topic-in-rchangemyview">Most Common Topic in r/changemyview</h1>

<p>In this post I am tackling the problem: What is the most talked about issue/topic on r/changemyview right now?</p>

<p>r/changemyview is a a subreddit where users post an opinion they have about a certain issue or topic and people post replies which try to pursuade the original poster to change their view. The original poster can then award delta points to the reply which pursuades them to change their view, or comes close by presenting a very good argument. Due to moderation and community rules, every single post has this same format. A opinion, and then replies which try to pursuade away from that opinion, and the replies which do an exceptional job receive “delta points”. Due to the structure and high quality of posts in this subreddit, r/changemyview is an ideal source of data for machine learning projects. Below is picture of what a r/changemyview post looks like.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/r:changemyview-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/r:changemyview-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/r:changemyview-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/r:changemyview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    
</div>

<p>For the problem I am trying to solve we only need the post titles and not the full scope of the data provided in the sub. The post title contains an opinion on a topic/issue and this is enough to determine the what topic the post is addressing. The first step to solve this problem no matter the strategy chosen is to fetch post titles over the timeframe you are interested in. Below is code for fetching titles from the top 50 post titles since last year as a sample. <code class="language-plaintext highlighter-rouge">reddit</code> is a read/write reddit instance that has been initialized with my reddit api credentials.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">submission</span> <span class="ow">in</span> <span class="n">reddit</span><span class="p">.</span><span class="nf">subreddit</span><span class="p">(</span><span class="sh">"</span><span class="s">changemyview</span><span class="sh">"</span><span class="p">).</span><span class="nf">top</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
      <span class="nf">print</span><span class="p">(</span><span class="n">submission</span><span class="p">.</span><span class="n">title</span><span class="p">)</span>
</code></pre></div></div>

<p>From here the question becomes how do we use this collection of post titles to determine the most common topic. I try a few different methods.
##Manual Labels &amp; Zero-shot Classification</p>

<p>The idea behind this first method is to manually provide several potential topics which we think are popular and use zero-shot classification to classify each post title. Either zero shot classification or trained classifier could be used but I thought zero shot classification(specifically the hugging face pipeline) might work reasonably well especially when the categories are very diverse. After manually scanning through some of the r/changemyview titles, here are the categories I came up with:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">candidate_labels</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">education</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">taxes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Trump</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">healthcare</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Religion</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">elections</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">race</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">LGBTQ</span><span class="sh">"</span><span class="p">]</span>
</code></pre></div></div>

<p>After that I used zero-shot classification in hugging-face pipelines to classify each title and here were the results of 5 posts to see if it was working as well as I wanted it to:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CMV: Mike Bloomberg's campaign is proof that the ultra wealthy in the US can afford a higher tax rate with no ill effect on them
[0.8761507272720337, 0.039841409772634506, 0.028353260830044746, 0.025303443893790245, 0.010511195287108421, 0.007160380948334932, 0.006719440221786499, 0.005960128735750914]
CMV: Kanye West is a shill for president Trump and running to syphon off young voters from voting for Biden.
[0.3772038221359253, 0.30558013916015625, 0.2326604723930359, 0.038126397877931595, 0.012778099626302719, 0.012653850950300694, 0.011820374056696892, 0.009176867082715034]
CMV: Most Americans who oppose a national healthcare system would quickly change their tune once they benefited from it.
[0.8781680464744568, 0.03057781793177128, 0.028276970610022545, 0.018674472346901894, 0.012455514632165432, 0.012006503529846668, 0.010698405094444752, 0.009142286144196987]
CMV: Donald Trump has not made a single lasting positive impact on the USA during his term as president.
[0.9717963933944702, 0.008201655931770802, 0.006410819478332996, 0.0034016254357993603, 0.0033515936229377985, 0.0030098608694970608, 0.0021523970644921064, 0.0016757362755015492]
CMV: being a conservative is the least Christ-like political view
[0.24276088178157806, 0.15054336190223694, 0.12963169813156128, 0.1104813814163208, 0.10187441110610962, 0.09882443398237228, 0.08833231031894684, 0.07755151391029358]
</code></pre></div></div>

<p>A quick look at how zero-shot classification is doing in these few examples reveals that it is not working that well. The topic “CMV: Donald Trump has not made a single lasting positive impact on the USA during his term as president.” is classified as 0.97 towards the topic <code class="language-plaintext highlighter-rouge">education</code> even though this topic has the word “Trump” in it and <code class="language-plaintext highlighter-rouge">Trump</code> is one of the categories. That is concerning but could be because the model doesn’t have embeddings for the word “Trump” as in president Trump instead of just the dictionary word. The same thing happens for “CMV: Most Americans who oppose a national healthcare system would quickly change their tune once they benefited from it.”. The classifier chooses <code class="language-plaintext highlighter-rouge">education</code> with 0.87 probability even though <code class="language-plaintext highlighter-rouge">healthcare</code> exists as an option.</p>

<p>Clearly, this strategy of manual labels and zero-shot classification is not effective. The next strategy I could explore is still within supervised topic modeling, but instead of zero-shot classification we use a classifier trained specifically for this classification task. The big problem with this method is there is no obvious way to get labels for each post title in the training dataset – manually would take very long. So instead of going in this route I will explore unsupervised topic modeling because there is no clear path forward within supervised modeling, and unsupervised modeling seems more interesting anyways because we can let the model identify distinctions between topics. So I will look at a Bag of Words model first.</p>

<h2 id="bag-of-wordslda">Bag of Words/LDA</h2>

<p>I’ll walk through how I used the bag of words method and LDA to perform unsupervised topic classification. Afterwards I’ll provide a brief overview of how these methods work and the motivation behind it.</p>

<p>The first step is to create a bag of words matrix, which will contain  For each document(each post title in this case) the frequency of each word in the dictionary is recorded in a row vector. The idea behind the bag of words matrix is to capture where words are repeating to provide information about which documents are covering the similar topics. For example if document 1 and document 3 both contain high frequency of the word “cat” they likely cover similar topics. This type of analysis clearly does not depend on many words which appear very frequently in the english language such as “as”, “to”, or “the”. These words are called stop words. Additionally, in this type of analysis there is no real difference between root words and extended words such as “happy” and “happiest”. Therefore, all words should be shortened to their shortest stem. Below is the function used to pre-process every post title. It tokenizes, deletes stop words, and shortens to smallest stem. This implementation uses the nltk library which is common for text preprocessing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">process_document</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
  <span class="n">tokens</span> <span class="o">=</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

  <span class="c1"># Removing Stop words
</span>  <span class="n">stop_words</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">stopwords</span><span class="p">.</span><span class="nf">words</span><span class="p">(</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">))</span>
  <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>

  <span class="c1"># Stemming
</span>  <span class="n">stemmer</span> <span class="o">=</span> <span class="nc">PorterStemmer</span><span class="p">()</span>
  <span class="n">stemmed_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="p">.</span><span class="nf">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">stemmed_tokens</span>
</code></pre></div></div>

<p>Below is an example to show exactly what the preprocessing is doing:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">process_document</span><span class="p">(</span><span class="sh">"</span><span class="s">hello, my name is Suhaas and I really like frisbee and tennis. What sports do you like?</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['hello',
 ',',
 'name',
 'suhaa',
 'i',
 'realli',
 'like',
 'frisbe',
 'tenni',
 '.',
 'what',
 'sport',
 'like',
 '?']
</code></pre></div></div>

<p>After preprocessing of the text is complete the bag of words matrix can be made.</p>

<p>The row vector of every document in the corpus are stacked on top of each other to form the Bag of words matrix. The row vector contains the frequency of each word. Below is a simple example. Say the corpus is</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>corpus = ["the dog is wet, the dog is angry",
  "the cat is upset and very hungry",
  "who let the dogs out? They are making a mess",
  "I can't believe the cat drank the milk"]
</code></pre></div></div>
<p>Then the bag of words matrix will be:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   angri  believ  ca  cat  dog  drank  hungri  let  make  mess  milk  upset  \
0      1       0   0    0    2      0       0    0     0     0     0      0   
1      0       0   0    1    0      0       1    0     0     0     0      1   
2      0       0   0    0    1      0       0    1     1     1     0      0   
3      0       1   1    1    0      1       0    0     0     0     1      0   

   wet  
0    1  
1    0  
2    0  
3    0
</code></pre></div></div>

<p>This matrix is pretty useful but can be made even more meaningful if rare words are given a greater weight and prevalence in the model. For example, in r/changemyview topics the word “abortion” is very rare. But when it does appear it is almost certainly the topic of the entire post. There are many words like this: words which are rare in the english language but determine the topic of the document when they do appear. This is where TF-IDF comes in. TF-IDF is a method to alter the bag of words matrix to increase the weight on these important words which are rare in the corpus. TF-IDF is composed of two parts:</p>

<p>Term Frequency (TF): This is simply the frequency of a word in a document. It’s based on the idea that the importance of a word is proportional to its frequency. However, some words like ‘the’, ‘is’, and ‘and’ appear frequently in all sorts of contexts, so high frequency doesn’t always mean high importance. That’s where the second part of TF-IDF comes in.</p>

<p>Inverse Document Frequency (IDF): This reduces the weight of words that are common in the corpus. IDF is calculated as the logarithm of the total number of documents in the corpus divided by the number of documents containing the term. Thus, it increases for rare words and decreases for common words.</p>

<p>The overall TF-IDF score for a word in a document is the product of its TF and IDF scores.</p>

<p>Here’s the mathematical formula for TF-IDF:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TF-IDF(t, d, D) = TF(t, d) * IDF(t, D)
</code></pre></div></div>

<p>where:</p>

<p><code class="language-plaintext highlighter-rouge">t</code> is the term or word
<code class="language-plaintext highlighter-rouge">d</code> is the document
<code class="language-plaintext highlighter-rouge">D</code> is the corpus
<code class="language-plaintext highlighter-rouge">TF(t, d)</code> is the term frequency of t in d (usually normalized by dividing by the total number of words in d)
<code class="language-plaintext highlighter-rouge">IDF(t, D)</code> is the inverse document frequency of t in D, calculated as <code class="language-plaintext highlighter-rouge">log(N / df(t))</code>, where N is the total number of documents and df(t) is the number of documents that contain t (to prevent division by zero if a word is not in the corpus, it’s common to add 1 to the denominator)</p>

<p>This calculation increases the weight of words which are rare in the corpus but frequent in a particular document. Such a word is probably very relevant to the topic of that document. Below is a implementation of creating a bag of words matrix using TF-IDF(this implementation uses the sci-kit library):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">BAG_matrix</span><span class="p">(</span><span class="n">corpus</span><span class="p">):</span>
  <span class="c1"># Apply preprocessing to each document in the corpus
</span>  <span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="nf">process_document</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>

  <span class="c1"># Initialize CountVectorizer
</span>  <span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

  <span class="c1"># Tokenize and build vocab
</span>  <span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

  <span class="c1"># Now, we initialize the TfidfTransformer and transform our count-matrix to tf-idf representation
</span>  <span class="n">transformer</span> <span class="o">=</span> <span class="nc">TfidfTransformer</span><span class="p">(</span><span class="n">smooth_idf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">use_idf</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="n">tfidf</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

  <span class="c1"># Output the shape of X
</span>  <span class="nf">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

  <span class="c1"># To get feature names
</span>  <span class="n">feature_names</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">()</span>

  <span class="c1"># To view the matrix as a DataFrame
</span>  <span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
  <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">tfidf</span><span class="p">.</span><span class="nf">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_names</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tfidf</span><span class="p">,</span> <span class="n">vectorizer</span>
</code></pre></div></div>

<p>And below is the matrix of the same corpus from earlier but using tf-idf now:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     angri    believ        ca       cat       dog     drank    hungri  \
0  0.47212  0.000000  0.000000  0.000000  0.744450  0.000000  0.000000   
1  0.00000  0.000000  0.000000  0.486934  0.000000  0.000000  0.617614   
2  0.00000  0.000000  0.000000  0.000000  0.414289  0.000000  0.000000   
3  0.00000  0.465162  0.465162  0.366739  0.000000  0.465162  0.000000   

        let      make      mess      milk     upset      wet  
0  0.000000  0.000000  0.000000  0.000000  0.000000  0.47212  
1  0.000000  0.000000  0.000000  0.000000  0.617614  0.00000  
2  0.525473  0.525473  0.525473  0.000000  0.000000  0.00000  
3  0.000000  0.000000  0.000000  0.465162  0.000000  0.00000  
</code></pre></div></div>

<p>The next step is to use this tf-idf matrix to construct topics. This can be done using Latent Dirichlet Allocation.
We will be using the LDA method in sci-kit but here is an overview of how LDA works:</p>

<p>Initialize LDA: First, initialize the LDA model. One key parameter to set here is the number of topics you want the model to identify. This is a hyperparameter that might need to be tuned to get the best results.</p>

<p>Assign topics to words: LDA starts by randomly assigning each word in each document to one of the K topics (where K is the number of topics you decided on). This random assignment already gives you both topic representations of all the documents and word distributions of all the topics (albeit not very good ones because it is random).</p>

<p>Iteratively update topic assignments: Then, LDA iteratively updates the topic assignments for each word in each document, based on two criteria:</p>

<p>a. How prevalent is the topic in the document? The more often the topic occurs in the document, the more likely it is that the word belongs to this topic.</p>

<p>b. How prevalent is the word across topics? If a word is already often assigned to a topic, it’s likely that it will be assigned to this topic again.</p>

<p>Each iteration of this step is done using a method called Gibbs Sampling, which is a type of Markov Chain Monte Carlo (MCMC) algorithm.</p>

<p>The process continues until the model’s estimates of the topics stabilize, or after a set number of iterations. Once finished, you’ll evaluate the topics that the model has learned. After this iterative process, LDA will represent each document in the corpus as a mixture of different topics(learns a distribution of topics for each document), and represents each topic as a set of top words(learns a distribution of words for each topic). More information about LDA can be found in the paper by Blie, Jordan, and Ng <a href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf" rel="external nofollow noopener" target="_blank">here.</a></p>

<p>Below is an implementattion using the bag of words matrix from earlier and an LDA method from sci-kit. It produces <code class="language-plaintext highlighter-rouge">num_topics</code> and prints the top 5 words for each topic to allow us to see what each topic is really about.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">LDA_topics</span><span class="p">(</span><span class="n">tfidf</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">,</span> <span class="n">num_topics</span><span class="p">):</span>
  <span class="c1"># Initialize LDA
</span>  <span class="c1"># n_components specifies the number of topics
</span>  <span class="n">lda</span> <span class="o">=</span> <span class="nc">LatentDirichletAllocation</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">num_topics</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

  <span class="c1"># Fit LDA to BoW data
</span>  <span class="n">lda</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">tfidf</span><span class="p">)</span>

  <span class="c1"># For each topic, print the top 10 most representative words
</span>  <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">topic</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">lda</span><span class="p">.</span><span class="n">components_</span><span class="p">):</span>
      <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Top 5 words for Topic #</span><span class="si">{</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
      <span class="nf">print</span><span class="p">(</span>
          <span class="p">[</span><span class="n">vectorizer</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">()[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">topic</span><span class="p">.</span><span class="nf">argsort</span><span class="p">()[</span><span class="o">-</span><span class="mi">5</span><span class="p">:]])</span>
      <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>This function takes the tfidf matrix and vectorizer as inputs from the bag of words function – The function below creates the tf-idf matrix and topics using LDA.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_topics</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">num_topics</span><span class="p">):</span>
  <span class="n">tfidf</span><span class="p">,</span> <span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">BAG_matrix</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
  <span class="nc">LDA_topics</span><span class="p">(</span><span class="n">tfidf</span><span class="p">,</span><span class="n">vectorizer</span><span class="p">,</span><span class="n">num_topics</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we are almost ready to use this function to generate topics from r/changemyview posts! The last thing left to do is to produce the corpus which will be a list post title from the subreddit. The code below creates the corpus:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">reddit_corpus</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">subreddit</span> <span class="o">=</span> <span class="n">reddit</span><span class="p">.</span><span class="nf">subreddit</span><span class="p">(</span><span class="sh">"</span><span class="s">changemyview</span><span class="sh">"</span><span class="p">)</span>
<span class="n">after</span> <span class="o">=</span> <span class="bp">None</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">after</span><span class="p">:</span>
        <span class="n">new_posts</span> <span class="o">=</span> <span class="n">subreddit</span><span class="p">.</span><span class="nf">top</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">after</span><span class="sh">'</span><span class="p">:</span> <span class="n">after</span><span class="p">})</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">new_posts</span> <span class="o">=</span> <span class="n">subreddit</span><span class="p">.</span><span class="nf">top</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

    <span class="n">last_post</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">post</span> <span class="ow">in</span> <span class="n">new_posts</span><span class="p">:</span>
        <span class="n">reddit_corpus</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">post</span><span class="p">.</span><span class="n">title</span><span class="p">[</span><span class="mi">4</span><span class="p">:])</span>
        <span class="n">last_post</span> <span class="o">=</span> <span class="n">post</span>

    <span class="n">after</span> <span class="o">=</span> <span class="n">last_post</span>  <span class="c1"># ID of the last post
</span>
<span class="k">for</span> <span class="n">submission</span> <span class="ow">in</span> <span class="n">reddit</span><span class="p">.</span><span class="nf">subreddit</span><span class="p">(</span><span class="sh">"</span><span class="s">changemyview</span><span class="sh">"</span><span class="p">).</span><span class="nf">top</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
      <span class="c1">#print(submission.title[4:])
</span>      <span class="n">reddit_corpus</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">submission</span><span class="p">.</span><span class="n">title</span><span class="p">[</span><span class="mi">4</span><span class="p">:])</span>
</code></pre></div></div>

<p>For each post I append <code class="language-plaintext highlighter-rouge">post.title[4:]</code> instead of the full post because each post starts with “CMV:” and we don’t want CMV to be one of the words which define a topic since it is present in every title. Additionally, the reddit API only allows 60 queries per minute and on 500 posts per query so I had to use the <code class="language-plaintext highlighter-rouge">params={'after': after})</code> parameter to start the next query where the previous one left off.</p>

<p>Now lets try out how the LDA topic generation did.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">make_topics</span><span class="p">(</span><span class="n">reddit_corpus</span><span class="p">,</span><span class="mi">15</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Top 5 words for Topic #1
['work', 'licens', 'conserv', 'the', 'religion']


Top 5 words for Topic #2
['charact', 'the', 'race', 'us', 'racist']


Top 5 words for Topic #3
['noth', 'there', 'sex', 'peopl', 'gender']


Top 5 words for Topic #4
['tri', 'cover', 'parent', 'world', 'sub']


Top 5 words for Topic #5
['joke', 'eat', 'women', 'citizenship', 'the']


Top 5 words for Topic #6
['offens', 'help', 'place', 'polic', 'the']


Top 5 words for Topic #7
['anim', 'much', 'small', 'commun', 'peopl']


Top 5 words for Topic #8
['child', 'flag', 'the', 'peopl', 'chang']


Top 5 words for Topic #9
['parti', 'make', 'peopl', 'vote', 'we']


Top 5 words for Topic #10
['wealth', 'donald', 'if', 'bodi', 'posit']


Top 5 words for Topic #11
['realiti', 'answer', 'appropri', 'cultur', 'thing']


Top 5 words for Topic #12
['hire', 'incom', 'job', 'includ', 'social']


Top 5 words for Topic #13
['consid', 'there', 'health', 'it', 'peopl']


Top 5 words for Topic #14
['homeless', 'there', 'it', 'see', 'wrong']


Top 5 words for Topic #15
['like', 'need', 'cultur', 'us', 'get']
</code></pre></div></div>

<p>Ok! so definetly an improvement from zero-shot learning, but still room for improvement as well. Some topics have top words that are very connected and make sense with how humans would define topics in this corpus. For example topic 3</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'charact', 'the', 'race', 'us', 'racist'
</code></pre></div></div>
<p>seems to be about race and culture.</p>

<p>Topic 12 is</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['hire', 'incom', 'job', 'includ', 'social']
</code></pre></div></div>
<p>This topic seems to be about employment and jobs.</p>

<p>And this topic:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['noth', 'there', 'sex', 'peopl', 'gender']
</code></pre></div></div>
<p>seems to be about gender and sexuality issues. On that same note there are some topics which don’t have a common theme like these ones:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['homeless', 'there', 'it', 'see', 'wrong']
['joke', 'eat', 'women', 'citizenship', 'the']
</code></pre></div></div>
<p>This is to be expected and highlights the limitations of LDA. Because it is a unsupervised topic model, the topics may not be formed in the same way a human would. The algorithms may form a topic based on some similarities or frequency of a particular word which we don’t see as important. Additionally, LDA does not take into account the order and semantics fo the words. For example, LDA would think “this post is about police” and “this post is not about police” are both about police simply because it contains the word “police”. Despite not taking into account semantics and word order it is pretty cool to see it produce some topics the similar to how a human would.</p>

<p>There are also some hyper parameters which could be tuned to produce better results like: number of topics, n-grams during bag of words matrix, number of top words for each topic. Perhaps the results of this topic modeling could be improved a little more just by further tuning these hyper parameters.</p>

<p>I am really interested to see how a model which does take into account the order of words could be used for topic modeling. An example includes BERTopic. There are also many extensions to LDA such as Dynamic LDA, Hierarchical Dirichlet Process, and GuidedLDA. I may explore these other techniques for topic modeling in future posts.</p>


    </div>
  </article>
</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Suhaas P Katikaneni. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
